<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="DNN," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="本文主要根据Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch的内容来进行试验。
#准备工作
根据原文“This code is written in Lua and requires Torch. Additionally, you need to">
<meta property="og:type" content="article">
<meta property="og:title" content="char-rnn-chinese">
<meta property="og:url" content="http://frankchen.xyz/2016/05/26/2016-05-26-char-rnn-chinese/index.html">
<meta property="og:site_name" content="不正经数据科学家">
<meta property="og:description" content="本文主要根据Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch的内容来进行试验。
#准备工作
根据原文“This code is written in Lua and requires Torch. Additionally, you need to">
<meta property="og:image" content="http://frankchen.xyz/images/2016/05/Screenshot from 2016-05-26 19-51-17.png">
<meta property="og:updated_time" content="2016-11-11T13:50:33.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="char-rnn-chinese">
<meta name="twitter:description" content="本文主要根据Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch的内容来进行试验。
#准备工作
根据原文“This code is written in Lua and requires Torch. Additionally, you need to">
<meta name="twitter:image" content="http://frankchen.xyz/images/2016/05/Screenshot from 2016-05-26 19-51-17.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"right","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://frankchen.xyz/2016/05/26/2016-05-26-char-rnn-chinese/"/>





  <title> char-rnn-chinese | 不正经数据科学家 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-right page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">不正经数据科学家</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Enjoy everything fun and challenging</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://frankchen.xyz/2016/05/26/2016-05-26-char-rnn-chinese/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="江南消夏">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不正经数据科学家">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                char-rnn-chinese
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-05-26T15:37:32+08:00">
                2016-05-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tutorial/" itemprop="url" rel="index">
                    <span itemprop="name">tutorial</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2016/05/26/2016-05-26-char-rnn-chinese/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/05/26/2016-05-26-char-rnn-chinese/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2016/05/26/2016-05-26-char-rnn-chinese/" class="leancloud_visitors" data-flag-title="char-rnn-chinese">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文主要根据<a href="https://github.com/zhangzibin/char-rnn-chinese" target="_blank" rel="external">Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch</a>的内容来进行试验。</p>
<p>#准备工作</p>
<p>根据原文“This code is written in Lua and requires Torch. Additionally, you need to install the nngraph and optim packages using LuaRocks”，安装以下依赖。<br><a id="more"></a></p>
<p>##安装Torch</p>
<p>使用如下的命令安装Torch</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cd ~/</div><div class="line">curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash</div><div class="line">git clone https://github.com/torch/distro.git ~/torch --recursive</div><div class="line">cd ~/torch; ./install.sh</div></pre></td></tr></table></figure>
<p>再用如下命令更新：<br><code>source ~/.bashrc</code></p>
<p>出现如下画面，代表Torch已经装好！</p>
<p><img src="/images/2016/05/Screenshot from 2016-05-26 19-51-17.png" alt=""></p>
<p>##安装lua<br><code>sudo apt-get install lua5.2</code></p>
<p>##安装其他依赖</p>
<p>使用<code>LuaRocks</code>来安装<code>nngraph</code>和<code>optim</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">luarocks install nngraph</div><div class="line">luarocks install optim</div></pre></td></tr></table></figure>
<p>首先安装<a href="https://luarocks.org/" target="_blank" rel="external"><code>LuaRocks</code></a><br>安装时在<code>config</code>部分遇到问题，参考<a href="https://segmentfault.com/a/1190000003920034" target="_blank" rel="external">安装Luarocks</a>和<a href="http://www.2cto.com/os/201506/412629.html" target="_blank" rel="external">linux下lua开发环境安装</a><br>这时可能遇到安装了<code>lua</code>但是却提示无法找到<code>lua.h</code>可能是因为还需要安装<code>liblua5.1-0-dev</code>的缘故。<br><del>使用<code>apt-get</code>安装luarocks后在安装<code>nngraph</code>时报错，需要解决</del></p>
<p>==其实使用<code>torch</code>内自带的<code>luarocks</code>安装即可==：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ~/torch/install/bin/luarocks install</div></pre></td></tr></table></figure></p>
<p>因为本机只有英特尔核显，所以只打算用CPU计算，就不安装<code>CUDA</code>了。</p>
<p>#开始实验</p>
<h2 id="karpathy的example实验-cpu版本"><a href="#karpathy的example实验-cpu版本" class="headerlink" title="karpathy的example实验-cpu版本"></a>karpathy的example实验-cpu版本</h2><p>###training过程</p>
<p>使用<code>th train.lua --help</code>查看一下各参数的作用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">Options</div><div class="line">  -data_dir                  data directory. Should contain the file input.txt with input data [data/tinyshakespeare] 训练语料</div><div class="line">  -min_freq                  min frequent of character [0]</div><div class="line">  -rnn_size                  size of LSTM internal state [128]</div><div class="line">  -num_layers                number of layers in the LSTM [2]</div><div class="line">  -model                     for now only lstm is supported. keep fixed [lstm]</div><div class="line">  -learning_rate             learning rate [0.002]</div><div class="line">  -learning_rate_decay       learning rate decay [0.97]</div><div class="line">  -learning_rate_decay_after in number of epochs, when to start decaying the learning rate [10]</div><div class="line">  -decay_rate                decay rate for rmsprop [0.95]</div><div class="line">  -dropout                   dropout for regularization, used after each RNN hidden layer. 0 = no dropout [0]</div><div class="line">  -seq_length                number of timesteps to unroll for [50]</div><div class="line">  -batch_size                number of sequences to train on in parallel [50]</div><div class="line">  -max_epochs                number of full passes through the training data [50]</div><div class="line">  -grad_clip                 clip gradients at this value [5]</div><div class="line">  -train_frac                fraction of data that goes into train set [0.95]</div><div class="line">  -val_frac                  fraction of data that goes into validation set [0.05]</div><div class="line">  -init_from                 initialize network parameters from checkpoint at this path []</div><div class="line">  -seed                      torch manual random number generator seed [123]</div><div class="line">  -print_every               how many steps/minibatches between printing out the loss [1]</div><div class="line">  -eval_val_every            every how many iterations should we evaluate on validation data? [2000]</div><div class="line">  -checkpoint_dir            output directory where checkpoints get written [cv]</div><div class="line">  -savefile                  filename to autosave the checkpont to. Will be inside checkpoint_dir/ [lstm]</div><div class="line">  -accurate_gpu_timing       set this flag to 1 to get precise timings when using GPU. Might make code bit slower but reports accurate timings. [0]</div><div class="line">  -gpuid                     which gpu to use. -1 = use CPU [0]</div><div class="line">  -opencl                    use OpenCL (instead of CUDA) [0]</div><div class="line">  -use_ss                    whether use scheduled sampling during training [1]</div><div class="line">  -start_ss                  start amount of truth data to be given to the model when using ss [1]</div><div class="line">  -decay_ss                  ss amount decay rate of each epoch [0.005]</div><div class="line">  -min_ss                    minimum amount of truth data to be given to the model when using ss [0.9]</div></pre></td></tr></table></figure></p>
<p>按照Github上的说明进行实验，使用原文件夹里的语料，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">th train.lua -data_dir data/tinyshakespeare/shakespeare_input.txt -gpuid -1</div></pre></td></tr></table></figure></p>
<p>报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"> th train.lua -data_dir data/tinyshakespeare/shakespeare_input.txt -gpuid -1</div><div class="line">vocab.t7 and data.t7 do not exist. Running preprocessing...</div><div class="line">one-time setup: preprocessing input text file data/tinyshakespeare/shakespeare_input.txt/input.txt...</div><div class="line">loading text file...</div><div class="line">/home/frank/torch/install/bin/luajit: cannot open &lt;data/tinyshakespeare/shakespeare_input.txt/input.txt&gt; in mode r  at /home/frank/torch/pkg/torch/lib/TH/THDiskFile.c:649</div><div class="line">stack traceback:</div><div class="line">	[C]: at 0x7f9c42473540</div><div class="line">	[C]: in function &apos;DiskFile&apos;</div><div class="line">	./util/CharSplitLMMinibatchLoader.lua:201: in function &apos;text_to_tensor&apos;</div><div class="line">	./util/CharSplitLMMinibatchLoader.lua:38: in function &apos;create&apos;</div><div class="line">	train.lua:118: in main chunk</div><div class="line">	[C]: in function &apos;dofile&apos;</div><div class="line">	...rank/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk</div><div class="line">	[C]: at 0x00405d70</div></pre></td></tr></table></figure></p>
<p>这里出现了问题，因为本文是中国作者按照原<a href="https://github.com/karpathy/char-rnn" target="_blank" rel="external">karpathy的char-rnn
</a>改写的，我认为或许使用karpathy作者的原版本教程可能会更加方便一些。于是使用<em>As a sanity check</em>，运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">th train.lua -gpuid -1</div></pre></td></tr></table></figure></p>
<p>这指的是使用CPU并不指定任何参数来训练example。</p>
<p>15:42开始训练<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"> th train.lua -gpuid -1</div><div class="line">loading data files...</div><div class="line">cutting off end of data so that the batches/sequences divide evenly</div><div class="line">reshaping tensor...</div><div class="line">data load done. Number of data batches in train: 423, val: 23, test: 0</div><div class="line">vocab size: 65</div><div class="line">creating an LSTM with 2 layers</div><div class="line">setting forget gate biases to 1 in LSTM layer 1</div><div class="line">setting forget gate biases to 1 in LSTM layer 2</div><div class="line">number of parameters in the model: 240321</div><div class="line">cloning rnn</div><div class="line">cloning criterion</div><div class="line">1/21150 (epoch 0.002), train_loss = 4.19803724, grad/param norm = 5.1721e-01, time/batch = 2.3129s</div><div class="line">2/21150 (epoch 0.005), train_loss = 3.93712133, grad/param norm = 1.4679e+00, time/batch = 2.3114s</div><div class="line">3/21150 (epoch 0.007), train_loss = 3.43764434, grad/param norm = 9.5800e-01, time/batch = 2.3022s</div><div class="line">4/21150 (epoch 0.009), train_loss = 3.41313742, grad/param norm = 7.5143e-01, time/batch = 2.5311s</div><div class="line">5/21150 (epoch 0.012), train_loss = 3.33707270, grad/param norm = 6.9269e-01, time/batch = 2.4913s</div></pre></td></tr></table></figure></p>
<p>到第300次迭代后，<code>time/batch</code>稳定在2.3s左右，也就是说，使用GPU训练这个1Mb的example，需要约14小时！<br>次日08:24训练完毕<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">21148/21150 (epoch 49.995), train_loss = 1.53254314, grad/param norm = 5.9157e-02, time/batch = 2.8658s</div><div class="line">21149/21150 (epoch 49.998), train_loss = 1.50882624, grad/param norm = 5.7123e-02, time/batch = 2.8737s</div><div class="line">decayed learning rate by a factor 0.97 to 0.00057368183755432</div><div class="line">evaluating loss over split index 2</div><div class="line">saving checkpoint to cv/lm_lstm_epoch50.00_1.3568.t7</div><div class="line">21150/21150 (epoch 50.000), train_loss = 1.46142484, grad/param norm = 5.9032e-02, time/batch = 2.8834s</div></pre></td></tr></table></figure></p>
<p>###Sample过程<br>查看<code>help</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">th sample.lua --help</div><div class="line">Usage: /home/frank/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th [options] &lt;model&gt;</div><div class="line"></div><div class="line">Sample from a character-level language model</div><div class="line"></div><div class="line">Options</div><div class="line">  &lt;model&gt;      model checkpoint to use for sampling</div><div class="line">  -seed        random number generator&apos;s seed [123]</div><div class="line">  -sample       0 to use max at each timestep, 1 to sample at each timestep [1]</div><div class="line">  -primetext   used as a prompt to &quot;seed&quot; the state of the LSTM using a given sequence, before we sample. []</div><div class="line">  -length      max number of characters to sample [2000] 采样字符大小，最大2000</div><div class="line">  -temperature temperature of sampling [1]</div><div class="line">  -gpuid       which gpu to use. -1 = use CPU [0] 和训练时设置应该保持一致</div><div class="line">  -verbose     set to 0 to ONLY print the sampled text, no diagnostics [1]</div><div class="line">  -stop        stop sampling when detected [</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">]</div></pre></td></tr></table></figure></p>
<p>先试运行一下<br><code>th sample.lua cv/lm_lstm_epoch50.00_1.3568.t7 -gpuid -1</code><br>生成了如下语句：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line">giff,</div><div class="line">Some sweet amends, aasher, had therein, not he had wot on</div><div class="line">man&apos;s friends, for her own blow: for my men&apos;s</div><div class="line">ackingly knight, it cannot hear upon yield.</div><div class="line"></div><div class="line">GLOUCESTER:</div><div class="line">How now again?</div><div class="line">i</div><div class="line">March, that&apos;s my arm determitness,</div><div class="line">The temper of the vrowal; ere from the grove</div><div class="line">Tut wilh &apos;goom&apos;d Carulea.</div><div class="line">&apos;Mwailich, tne shy had lost in When Ied the way</div><div class="line">The bower to a late his body, grim on you:</div><div class="line">His opicious shames a booy, infairs,&apos;</div><div class="line">From her, I tell you, ay, as we mean him</div><div class="line">Dear &apos;tis a giving o&apos; thur back&apos;s empass&apos;d,</div><div class="line">That nrost, I&apos;ll havk him cume thee for &apos;t.</div><div class="line"></div><div class="line">LAONTES:</div><div class="line">&apos;Twi&apos;l love thy sronowing.</div><div class="line"></div><div class="line">VALYRAN:</div><div class="line">Beord mocdoch him for thy follight</div><div class="line">snn</div><div class="line">hours,</div><div class="line">But thank yours lodkes, my good journeding,</div><div class="line">His jealousisposour thee are both abomish</div><div class="line">That noom that&apos;s easembelland. Camest, sir, more</div><div class="line">kia; one, in this highty be the un</div><div class="line">Since of a gournor on thy friendshall swow</div><div class="line">Some painon; and I, and lord, the  at the kins</div><div class="line">Wise rit hable surliments. Shd, believh gone.</div><div class="line"></div><div class="line">voisted tleace:</div><div class="line">Tock him what all you di turn up to celent</div><div class="line">To my sistinge. Frranch, good night, your child, so fatus;</div><div class="line">Aor he shall be my trueking:</div><div class="line">Come on my quarrel of the way:</div><div class="line">Methinks the letters; for this ctome-steers</div><div class="line">Tad mousd my smodered pouncy to</div><div class="line">haw up another sense tlays underttry</div><div class="line">Tut bonscuration fair all purpose,</div><div class="line">then be vesegt me: do not, yet rustle cannot,</div><div class="line">But for thy mustered a dust, let me</div><div class="line">Tncerfact me tresmer of his father:</div><div class="line">therefore by hanging,</div><div class="line">ANd</div><div class="line">Ays, my lord: you do here in coumisant.</div><div class="line"></div><div class="line">LORD:</div><div class="line">How lond the brown!</div><div class="line">So majp me; bonch, smmily  lovely blotters,</div><div class="line">When Ie my hoeaty threat and virlume these things,</div><div class="line">Make fasting garlands dfar the sack&apos;d my servictught</div><div class="line">Not knows the crowns: one air, Aumerle,</div><div class="line">Ere wear not so nour Bidagle? What Aphark is fury</div><div class="line">Tld meens them, faireyou consides to no more</div><div class="line">Ihis wantond frown and pollitueser&apos;d city.</div><div class="line">Can should put him more recounders to impudesnt poison on</div><div class="line">thet hour from hunt to Rame, supp to bere</div><div class="line">Flowerd and his friend is une dewn ao pirt,</div><div class="line">You know by join&apos;d guilty, whathout we e.</div><div class="line"></div><div class="line">ANd</div><div class="line">Ays, my lord: you do here in coumisant.</div><div class="line"></div><div class="line">LORD:</div><div class="line">How lond the brown!</div><div class="line">So majp me; bonch, smmily  lovely blotters,</div><div class="line">When Ie my hoeaty threat and virlume these things,</div><div class="line">Make fasting garlands dfar the sack&apos;d my servictught</div><div class="line">Not knows the crowns: one air, Aumerle,</div><div class="line">Ere wear not so nour Bidagle? What Aphark is fury</div><div class="line">Tld meens them, faire</div></pre></td></tr></table></figure></p>
<h2 id="karpathy的example实验-gpu版本"><a href="#karpathy的example实验-gpu版本" class="headerlink" title="karpathy的example实验-gpu版本"></a>karpathy的example实验-gpu版本</h2><p>使用和cpu版本相同的指令，只是<code>th train.lua -gpuid 0</code><br>得到的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">th sample.lua cv/lm_lstm_epoch50.00_1.3622.t7 -gpuid 0</div></pre></td></tr></table></figure></p>
<p>sample为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line">--------------------------</div><div class="line">ge</div><div class="line">I prithee; nor in the day of all report?</div><div class="line">Nou shall be you that lances stoson quarrel:</div><div class="line">We mits most stone, &apos;aim, upeed his forticent</div><div class="line">Ahen I was, yet for her, but of lovels Clarencel</div><div class="line">And past her got and father there, one weep</div><div class="line">In mine  wroth nightlys, smileed. Cantleye An York,</div><div class="line">A druls marriage, that though Service hated me</div><div class="line">Their duen of iflock, we are here in berieved</div><div class="line">and more than tanise them with suterept</div><div class="line">He rt in some pickle.</div><div class="line"></div><div class="line">SePSERTER:</div><div class="line">Or I have safe all thou depustere andthe before him.</div><div class="line"></div><div class="line">FRIAR LAURENCE:</div><div class="line">What art my government, cosbude, and hence; if Onfrawn? provest tor my duty?</div><div class="line"></div><div class="line">CATESBY:</div><div class="line"></div><div class="line">KARIANA:</div><div class="line">My love noe is are  with t herman and his,</div><div class="line">It should she well deeauring our consent:</div><div class="line">They hang me bointed on the king, let so two</div><div class="line">Nature by my sighsing pleasing &apos;jabe</div><div class="line">That leaven and grue, at her Richard&apos;s blood.</div><div class="line">More ends it likipenortnive, nor each of him</div><div class="line">ic.</div><div class="line"></div><div class="line">SLY:</div><div class="line">How dachors, Richmend, henr dack but like it?</div><div class="line">Be long, anon since your kingdom and us,</div><div class="line">And we that aver el</div><div class="line">aunter, my eee to toucurt tomends.</div><div class="line">It this her great fawn&apos;s birds,, sir! you&apos;er head.</div><div class="line"></div><div class="line">PAULINA:</div><div class="line">Upternalt cost of his hands for their tricks my father,</div><div class="line">Who ts it most seunt to live te and she were all.</div><div class="line">-kill</div><div class="line">O to thy son os shall not on your childrs,</div><div class="line">one next, for she did formly consixent</div><div class="line">Above, my life, and wew me worthy deeming tvenge!</div><div class="line">My mustere be exploience, aot come n leave where ahe knees in.</div><div class="line">dear, thus wild up tilt on the county, hath be one.</div><div class="line">See this sword of thee with the deepito man,</div><div class="line">For sunier ene first sears. Where&apos;s turn on to be.</div><div class="line">Unctious blunlest terrocate doves</div><div class="line">Trades Marcius aines of hlends</div><div class="line">My&apos;s learth an old--ay.</div><div class="line"></div><div class="line">LEONTES:</div><div class="line">Marcius?</div><div class="line"></div><div class="line">PRONVO:</div><div class="line">You would no gue.</div><div class="line"></div><div class="line">VOLUMNIA:</div><div class="line">Oovine s fetch to tight, thou must but loods.</div><div class="line"></div><div class="line">HASTINGS:</div><div class="line">And was with her, nor yonder to be sworn,</div><div class="line">What are you allady that I should have purpose.</div><div class="line">What men  revenge is a well patient</div><div class="line">And who seth sxoleng to knowled ed to myself;</div><div class="line">And married me in the joy:</div><div class="line">So reve I made to find me speak,</div><div class="line">how he been tou.</div><div class="line"></div><div class="line">PETCA:</div></pre></td></tr></table></figure>
<p>##《水浒传》语料实验</p>
<p>###cpu版本</p>
<p>把下载好的《水浒传》改名为<code>input.txt</code><br>使用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">th train.lua -data_dir data/mydata/ -gpuid -1</div></pre></td></tr></table></figure></p>
<p>训练，可以看到很明显，速度很慢<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">th train.lua -data_dir data/mydata/ -gpuid -1</div><div class="line">vocab.t7 and data.t7 do not exist. Running preprocessing...</div><div class="line">one-time setup: preprocessing input text file data/mydata/input.txt...</div><div class="line">loading text file...</div><div class="line">creating vocabulary mapping...</div><div class="line">putting data into tensor, it takes a lot of time...</div><div class="line">saving data/mydata/vocab.t7</div><div class="line">saving data/mydata/data.t7</div><div class="line">loading data files...</div><div class="line">cutting off end of data so that the batches/sequences divide evenly</div><div class="line">reshaping tensor...</div><div class="line">data load done. Number of data batches in train: 345, val: 19, test: 0</div><div class="line">vocab size: 4129</div><div class="line">creating an LSTM with 2 layers</div><div class="line">setting forget gate biases to 1 in LSTM layer 1</div><div class="line">setting forget gate biases to 1 in LSTM layer 2</div><div class="line">number of parameters in the model: 2845345</div><div class="line">cloning rnn</div><div class="line">cloning criterion</div><div class="line">1/17250 (epoch 0.003), train_loss = 8.32795887, grad/param norm = 9.6310e-02, time/batch = 28.8711s</div><div class="line">2/17250 (epoch 0.006), train_loss = 8.06433859, grad/param norm = 4.2826e-01, time/batch = 25.2184s</div><div class="line">3/17250 (epoch 0.009), train_loss = 7.28941094, grad/param norm = 3.9537e-01, time/batch = 25.2195s</div><div class="line">4/17250 (epoch 0.012), train_loss = 6.85331761, grad/param norm = 4.0576e-01, time/batch = 25.2011s</div><div class="line">5/17250 (epoch 0.014), train_loss = 6.69327439, grad/param norm = 3.8309e-01, time/batch = 24.9642s</div><div class="line">6/17250 (epoch 0.017), train_loss = 6.50776019, grad/param norm = 3.1042e-01, time/batch = 24.9203s</div></pre></td></tr></table></figure></p>
<p>预计需要120小时训练时间！但是，这都是未经过处理的语料，后续使用处理过的余料（如去掉低频词语等）再来训练应该会更快。因为时间太长，所以这个实验被放弃了。</p>
<p>###GPU版本的未处理语料实验<br>首先对未处理语料做训练：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">th train.lua -data_dir data/mydata/ -gpuid 0</div></pre></td></tr></table></figure></p>
<p>begin at 10:20<br>可以看到<code>time/batch</code>稳定在0.08s左右，也就是半小时就可以训练完成！GPU比cpu在科学计算上面实在是太强大了。<br>训练完毕，使用：<br><code>th sample.lua cv/lm_lstm_epoch50.00_3.9309.t7</code></p>
<blockquote>
<p>者，却得出来的物细物，没面少管生渔人。后来的知府时是乱色早晚，拾了几个。李逵惊得忙忙轻梳药穿在大牢里，摆在延安家处，推慰九节的。当下径到居中饮酒，牌门头，戴宗又焦躁。只见屏风背后转出一个小风大来，暗暗听得道：“反细放俺!兄弟拿着，趁这为害天明地清，我休要推道别事的都要做伴当拆投到会耳，便有进漏？”时迁舞起树下探人，的了夹搭，都拽了拽开，胸皮虽是好了六分惊得，是他麻。吐女棒放火了，走不向前，及宋江那道个留守他做个辩察的，先自去州里请明地烧了钱用，但有过京回家，听得状子好!这高老袋内却是出张招安，又都是他的欺负民，如何是计信?必须要和郎相会。且。”趁早起楼去了。两个连夜时候，仓治时，年二十八执迷者多要都要去行叶，早是亲家洒家，径挨到府前来。灯烛纸敌官，方才脱漏。亦被乱窝中有人等好人知蔡京道：“那个人也是他甚么?因此教大军打劫俺那干干金珠的。”母子那妇人来到大王尚安着，相交酒追惹三清酒搦战。不过半夜之事，早饭相烦，心里出对知府说：“官家初时在时，欲要市上时，兀自和我觅家劫犯了他，如何不赶我里来?我大小心定得，不分便了。”叉军转回，已做些小头，要打抵敌他!且把身，带了七个人时，都抢出家，但见：　　<br>    壮中醪浑纷领，腰细轻露。阔尺三层挺刀，鱼厚夸敌庆孙。高人有八句诗单道身心强似莽?；小付敲柴的，真多呼圣殿之主贞锋欣乐词。头邬闻丹腊良夫，耍达缘矮岁龙；四虎间，寒暮难以偷黄；牢记仁诸作像，显宝一根红佛。微。善得山迎能指挥，直救清天马星。　　<br>    话说上阵法， 师皇帝展得有?宫殿，雨翠云也上田地里。幸观非乃帝重了，宋江心如有誓，同宋受迷。却诗名唤，只传说开门，因此是贼人心腹事务，到宋江纠合生灵害，在忠渐存母亲来宋江以心却才，惊得义既灵垂德，对公孙胜为然无智真道好法，正为：须游六十为聚义，好像原林密寨郎山神保。　　<br>    当日宋庄客帐前，与晁保、公二位头领，众头领发起作法。　　<br>    石裂更兼地分都拨人汉，且不杀得蒙恩干人结义，下山只是锦袋百把，们有父亲孔宾，同商量。宋江又道：“自是好生，莫非也只是是有哥哥下了。”吴用笑道：“兄弟，不到山寨，吴用命作商量，将军不与他长犬马，力休曾平他：一话难以安身，宋江一力不东昌下几日，谁想大哥哥教小晁盖哥哥会合当的事。我们人投随天军来，又有伤损；若不连环甲关，着李横其不似火体，车藏御上尽挂玉水；军卒许多，无无不难之际?他但**，可等兵，可以斩遣。”众军健都管入庄，要把鲜血迸成，赶起来，背后解水边，唤车军跨城疮只等，原来正是之福，后往，来不见三个使汉。因弟清风船救应。路，至是路买酒，又拨五七百名、罩、白、孔亮，正将费珍、薛霸，尽是钱十二十军，其余的人在彼，欲得众兵险道地广花荣抵敌人住。这一队节度使士都军，被两个军猛，呐声喊，都抢过城里，并无腰迎敌，被贼兵赶上，时，却被花荣战箭射来。童太人、杨志正是南安江韩杨龙、穆弘、李逵、索超正定敌。孙军纵马。琼清马挺着枪，入来，尽被史进和贼人杀死贼兵，擒做霹雳。邬梨因成让风，连鼓上马，将股斧，却飞入阵，大小张清见了见乱军阵前卒法败坑回阵，宋江旭前只是：　　<br>    主人问姓，五应风万。侯海道正：“恶平可逃奔：。时们村野阴血，呼往天兵消波。正被杨志聚领渡江，望宋江攻还山庵；拔寨教活林冲、公万一通，并添下山南二王庆名事虚权，再被小人在戴上探山泊路，几路去报，不敢准备。不知这个人说起是百庄小黑凌州，已曾见了，对别无缘。”吴用道道：“便队军马解到此时，必是殡隘为百谷岭。原师悬流水军头二头领，结识江湖上好汉姓石，名给鬼，便乃五家庄二多情。我去这里地路，望会便行。”廊进雷车把人来不止，李应拈着诏书，自此付话。　　<br>    且说山客渡过了三只路，教穆弘扮做伴当，扮做阎婆者，带拢是臭镇一个没赌什门，分顺了同行，自去寻闭了的。原自去被人运烧将下去了。宋江等远远，一路进兵。十来县不在大路途来，又怕了到得闲意的张社长，听得监押一声，货钱便是。任原陈达在中，不知处打那华州，特使他来掳去太安军肆，只待下山。戴宗告随张小人，蔡九知府不得，连夜回话，同张招讨干办、众部吉。于路，忽报探知样悔，景珍全过，领回商议，“军师赵枢密喜绯金带，身上悬面草板，护道国师，服，神色不通。是奇诈将丘留的人，准备起船走径来借粮，业不同何遇一深困马灵夫，便因密的月色渐砂来完，斋。小温皇威，被宣刚引军来，武松彼朵并顾大嫂，赴了逃去，自逃命探了。被那几人娅?在古靖军吟涂炭，，态纪士，接应喊道，漏转身来，复有神诗，燕世曰：“寡人仰云监斩辽王康公外交法，何”奈阵圣怒须性重。铁挨填丹靴，万边狄行鉴。见。田户观看草畔，红日影豪困催急绩。宋玉游战，听听了大喜。话说宿太师诏奏道：“宿元帅差有敕入请罗真人，密封官军等八员高名，封当同达宋先锋。”日收选润之主，奏为圣旨，特着州殿府探知。太尉宿太尉回到内，启转马，众军方可亦成开大事，放起出来，更兼小一个唤做</p>
</blockquote>
<p>##使用增强RNN网络训练</p>
<p>如下，使用512个隐藏节点的3层RNN网络训练模型</p>
<p><code>th train.lua -data_dir data/shuihuzhuan/ -gpuid 0  -rnn_size 512 -num_layers 3</code><br><code>th sample.lua cv/lm_lstm_epoch50.00_5.4830.t7 -sample 0 -temperature 0.8 -verbose 0 -length 500</code></p>
<blockquote>
<p>弟兄两个，也得个信名之人。”那个也是个道理。童家四更，被张顺斩的粉碎，以下人人家拿去了。一面叫酒保打两桶酒来。小二哥叫道：“师父，你不是我来也！”那小牢子道：“我也不曾你，你便叫我上来。”　　 石秀道：“你且说他三宫百里吃酒了来，你便抢入去，你便先来看，却被这畜生说不得了。”那妇人道：“你真人要打这里话?你却不认你，你便叫我儿来寻。”李逵道：“你敢作吃的，便揪我做脚！”赤条条地寻谁，只得骂道：“爹娘，你且休了，我自不信，砍我头便打那老娘。”那妇人道：“也好。”便把袖儿丢下去了。那妇人也把刀带在一边，却似小窗????胡乱道：“好拳脚！”急叫开了店看。”王庆听了，连声叫道：“阿也!你不要吃！”把手一指，提倒上岸来，把朴刀倚在被里。就把篙子门内，倒做五六斤了，将把木鱼来摆下桶桶。少时，张顺吃了一回。两个回到店前，再出来赏赐，解了戒刀，包了水出去，到四更，把船渡入去，便叫艄公下楼，买了些鱼吃，把些酒肉吃了，酒保做些桶汤、盘酒、些肉。下来穿瓶与酒。一瓶儿酒肉，买些肉吃，只见店主人把包裹插下，那妇人也吃得饱了，口里说道：“娘子，老身等这几个泼，不要吃酒钱。”店小二道：“好酒好肉要打，我吃便饱</p>
</blockquote>
<p>很明显，此时sample的样本语句更加通顺，错误很少，从品味小说的角度来讲，增强了的RNN训练得到的模型更加完美了。 可以看到，<strong>增加了节点数和隐藏层的RNN具有更强的学习能力。
</strong></p>
<h2 id="对比训练过程模型表现力"><a href="#对比训练过程模型表现力" class="headerlink" title="对比训练过程模型表现力"></a>对比训练过程模型表现力</h2><p>与此同时我们可以对比一下，训练开始阶段与训练结束时的模型表现力的差异：</p>
<p><code>th sample.lua cv/06-01-shz_sp/lm_lstm_epoch5.80_4.0451.t7 -sample 0 -temperature 0.8 -verbose 0 -length 500</code></p>
<p>训练刚刚进行到5.8（为50时完成）得到的是：</p>
<blockquote>
<p>只见一个人从来，一个人，都来做一个。那人道：“你这厮们，我自去寻你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不</p>
</blockquote>
<p>此时语料具有较多的重复，模型还没有良好的收敛。</p>
<p>继续观察，当进行到四分之一左右时：</p>
<blockquote>
<p>那汉子听了，便问道：“你这厮不是歹人，如何不来?我们不曾有这般的，如何不来?你的那里去了？”那汉道：“我们不曾说谎。”那汉道：“我不曾说谎。”那汉道：“我不曾说谎。”那汉道：“我不曾说谎。”那汉道：“我不曾说谎。”那汉道：“既是恁地，我们自去。”那汉道：“既是恁地，我们自去。”那汉道：“既然恁地，我们自去买碗酒吃。”那汉道：“既是恁地，我们自去。”那汉道：“既然恁地，我们自去买碗酒吃。”那汉道：“既是恁地，我们自去。”那挑酒的汉子道：“我们自有计较，我们自有些钱，与你些银两，却去商议。”董超道：“我们不曾有这般的事，如何不来?你们自去取路，我和你如何不去?若还了他时，便是要去的。”李逵道：“你们不曾说的，你便是个老儿，如何不来?我们自去取他，你便不曾与他厮见。”那妇人道：“你的，你不省得。”那妇人道：“你的，不要胡说!我们自有钱帛，便要去便了。”那妇人道：“既是恁地，我们自去买些银子与我。”那妇人道：“我们自有钱与你，你便不曾去了。”那妇人道：“既是恁地，我们便去。”那妇人道：“既是恁地，我们便去。”那妇人道：“你的，不要胡说!我如何肯去这里？”那妇人道：“便是这般使棒，不曾得得他。</p>
</blockquote>
<p>再看看训练继续进行到一半左右时候的表现力：</p>
<blockquote>
<p>张都监为副将急体己人，不敢不依，只得随行众军，掌行送行。只留下降，尽皆欢喜，以此忠义。在府行军中，有使枪棒卖药的，将王庆领军到来，并不必说。当下宋江传令，教中军计策，与卢俊义等商议：“今日折了两阵，俺们自去了。”宋江道：“军师之言甚善！”当下即日便传将令，教军士点营，斩动军马。将及初五更战后，攻打常州，催趱军兵，一齐进发。 寨中，只听得高声叫道：“萧让等救兵！”宋江看那军将，尽数放起，对幽州一个大汉，乃古头上大叫道：“水洼草寇，怎敢轻慢！”只见里面关胜、呼延灼、关胜等探有一人，只见唐斌从人骑马，直到宋江寨前，喝请宋先锋。宋江听了大喜，传令令军士且去寨中坐地，备说宋先锋军马，攻打北京。吴用道：“且教两路军马，攻打北门。”宋江便令吴用、朱武商议：“今日可去，只是国师吴用，坐一件事，我等随顺到此，可用两处夹攻，那厮必然有人来。”宋江道：“军师言之极当。”便唤军士计策。”宋江道：“军师言之极当。”吴用道：“小生直作妙计，即且闻见。”宋江道：“先生之言，是不得这般忧疑。”宋江道：“既然如此，与你四位豪杰，不堪员外大王。”宋江道：“贤弟，你休要疑心，我便去请来。”吴用道：“不须你两个与我箭，只</p>
</blockquote>
<p>此时低级的重复没有了，但是可以看到，“宋江道”反复出现，而且说的内容类似，可以得到模型已在进一步完善之中。</p>
<p>这是模型接近训练完成的时候的sample：</p>
<blockquote>
<p>张青、孙二娘、顾大嫂、孙二娘，并四个好汉，引着一千余人，吹造大小船只，都投水路。不多时，只见松树背后转出一个小小人来，簇拥着两个人，各提着朴刀，背后有人，叫一声：“捉下！”　　 那汉子把船只一招，扶着一干人，把那碗饭打伤，打的粉碎，把头头割在一边，口里放火。那人见了是惊得呆了，又不来吃了一惊，扑地只顾走。却待再走，再去脱人避凉。李逵却亦不肯拦他，只得走了。可怜救他两个性命，那里敢?别人。前日被捉死了性命，杀了人，逃走在江州，被害人陷害，方得正中了。今日幸得相见，如何使得?便得是个知县过来的，也喜得及。他便是本人的人，须是高太尉的人，却不知是那里人。”任原道：“这个便是我的儿么？”王婆道：“便是前日那官司亲亲叔孝，为何到此？”那婆子答道：“老身只道不妨，只怕小人自有措置。老身看了，便忘了回去。”老都管道：“这个容易。老身先把银酒去了。”老儿道：“你们自不要吃酒。”那婆子也笑起来道：“这个便是我的老小人家。”那婆子道：“便是老身也不怕你，休要胡主干娘，只怕你疑心。”那妇人道：“不干了。你的女儿，老娘儿只做买些衣服来送与你。”王婆道：“娘子，你要知这几个字？”那婆子道：“有甚么哭处？”</p>
</blockquote>
<p>模型进一步完善，接近完成训练。</p>
<p>##《全唐诗》实验</p>
<h3 id="下载下来的全唐诗-txt是gbk编码的，首先需要转换为utf-8编码："><a href="#下载下来的全唐诗-txt是gbk编码的，首先需要转换为utf-8编码：" class="headerlink" title="下载下来的全唐诗.txt是gbk编码的，首先需要转换为utf-8编码："></a>下载下来的<code>全唐诗.txt</code>是<code>gbk</code>编码的，首先需要转换为<code>utf-8</code>编码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    s = open(<span class="string">"全唐诗.txt"</span>)</div><div class="line">    r = s.read()</div><div class="line">    r_uni = r.decode(<span class="string">'gb2312'</span>,<span class="string">'ignore'</span>)</div><div class="line">    r_en = r_uni.encode(<span class="string">'utf-8'</span>,<span class="string">'ignore'</span>)</div><div class="line">    fp=open(<span class="string">"全唐诗转换.txt"</span>,<span class="string">"w"</span>)</div><div class="line">    fp.write(r_en)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span> :</div><div class="line">    main()</div></pre></td></tr></table></figure>
<p>这样就得到了<code>utf-8</code>编码的文件。</p>
<p>开始想直接把这个文件丢给rnn训练，后来想一想，“全唐诗”包括了众多诗人的杰作，也许训练出来的模型sample不到什么特点鲜明的诗句，于是我首先想对诗仙李白的诗做一个实验。把得到的全唐诗文件中李白的诗切分出来作为一个文件。<br>开始训练：<br><code>th train.lua -data_dir data/tangshi/ -gpuid 0</code><br>得到模型后sample：</p>
<p><code>th sample.lua cv/lm_lstm_epoch50.00_5.3967.t7</code></p>
<blockquote>
<p>耶翳妃在。流明湖草，岂舞高散纷。小剑宫底寒，石思怀士。<br>归来相烟叹，又余未老此。此在见携节，气帝皇彩川。<br>尝君凌天鸟，羞从臂山中。何旋俱所偃，造愧翻遗耻。<br>悠挥李明云，坐月得相迟。君作成景鸡，暮日清膺发。<br>胡步垂洞松，三年延未莱。贤迢坐橐山，嗤楼谋疏川。<br>诗君九安菲，别去写日流。五时谢及洒，相魄三有玄。</p>
<p>卷177_21 【长道送秀士寄之十南国古松游明，》妓此蛾书此下见逃之始崔至六以吟酒宁】李白</p>
<p>仙阳壶我王，谣荣日茫过。幸生天行寒，半持见九忧。<br>早毂此楼宰歌，鹗税金归名。宾托昼宇闻，高汶堕臣泉。<br>峻悟湍素都，凤生鸟远才。虎傥亦成一，蹭据锁炳垣。<br>回谷闻叹波，摧人翼昆衣。丑德皆复贵，何袂自见宝。<br>黄然奉傍及，戾酒悲溪情。何悟下罗灭，壁令还济然。</p>
<p> 卷177_16 【送沙门饯元佛之嵩使归少丞寺辅晔年亭山云】李白</p>
<p>行道一狂日，陆杯欲我园。相寄属相者，不歌流成书。<br>幽景神兰马，今云乃相知。相能拂此门，娟笑无生魂。<br>肠后扫天所，罗瑟心世桥。</p>
<p> 卷169_11 【金松二炎师】李白</p>
<p>丽劝发何凤，含东药宛锦。且忍咏尺鳌，梦杯池月<br>。<br>罗鸟思归人，兼我无风歇。太色东草树，壮早游罗息。<br>别来青神极，谒长不陵寒。君忧东海鹤，吹欲得彩好。<br>梦君来太情，秦子忆延薪。闻干凌楼息，松水接廉才。<br>且识辞犹之，众断罗里中。</p>
<p>水羊远重，引飒高新策。目布愁霜亲，，随火赤云道。<br>解毂四上牛，以且汶云年。宁迎清巴寒，种欺清名风。<br>海坐去不意，思丹酩期然。闻钓曾帆景，一弄暗长雪。<br>且亦留我辉，杀讼韵中楼。</p>
<p>  卷174_7 【赠崔司州十三青寺黔洞姑圣毛闲华忘兼塔宅石】李白</p>
<p>窜笑敬鹤见梦走去留僧。地筑从尺人，泪树平众烂。<br>窈箸有吉氲，久聪欲洲真。朝春离相兽，飞此何垣发。<br>绿日偶可言，我言经精存。君卒限汉水，绿月相成失。<br>恋虏一登事，但乘涂应星。</p>
<p> 卷177_18 【咏夜别（人作帝阳之为昆者）】李白</p>
<p>笑乃度将明，蛾洞蕊山发。花坐新合露镜日边归溪。<br>长面云蓬立，自颜谢鸳分。灭用欲得碑，不云当天舟。<br>一来但不霄，乘我涉路来。怀子广陵远，夕人不元阙。<br>笑天亦望好，驱令适谁手。思此有门上，举与满丝君。<br>醉年归敬山，松藏谢未笑。却思笑古兴，凭凌泪高尺。</p>
<p>钱文1淮作3<br> 　<br>卷一两十七三北欢宁三元四慈平难名】李白</p>
<p>水乘黄楼镜君客绿1起，山门天阙已忍春。不喜出庭作，乎<br>晖酒苔。麒惊美。绮门，我见秦心遂天雪，只行天花流。<br>相随夔山肉烟喧，武斗吹齐而公还。以开玉去戟如雨，<br>欲席上醉鹊里洪。世闻燕弦对士去，曳不笑丘瞳中嗟。<br>国昔逢眼青山鸟，扫水长丘双泉空。</p>
<p>  卷165_8 【玉崔将言刺判池，一还精孰日君）】李白</p>
<p>军莫荆壮悲，揽迈宋钟客。高当愤酒酒，渴臣达庭边。<br>沙箸佳花诏，映河讵窗中。为交上梧空，空在猛杯闻。<br>惜缅何烦柯，屈笑献神然。万情上者溪，相以陶毫逸。</p>
<p>更闻无袂，小必笑伤穷。</p>
<p>  卷169_12 【高陵黄入蜀，寄纪侍御二首】李白</p>
<p>爱来游山子，北照有华宅。岩歧难碑李，独承崔滹鼙。<br>巨君亦不处，常阖之此酒。思君穷莫术，却成愁庄隈。征六此与玄，，羞逐但应君。</p>
<p>  卷189_26 【登溪马归歌，归石怀道怀山】】。<br>长腾明花屏爱心春，寄心入良素断。</p>
<p>  卷188_7 【送侍御从尔史崔崖赴天】白山年粲赤浑，书别诗游泛风】李白</p>
<p>祖国一官食，二藏系冠鹇。无砚号盆水，浮发王青吟。<br>登水惜不母，殷古启与游。萧觉留见去，待泪复相思。<br>绿风吹中信，江山知洞宫。宾阔未孤里，空可向江峤。<br>西舟青溟云，一浪摇苗彩。流镜沙青辉，夕落得寒洲。<br>妾头海弦弓，而多何皇笑。欢辰虽欢心，此鸣暗清飞。<br>广松春人草，为啸李田。。思君鸾可得，萧论以天风。<br>今家高去路，酌藏无云功。为时壮罢邑，陈不见长名。<br>闻啸不可在，玉服汉寒情。愿昆隐山水，更将谢敬离。</p>
<p>大说词隐开，夜子庐鹤行。他烛不知尽，逸君徒风草。<br>把柯南幽山，超血彩森兵。吊窈赤微鸟，娇若相踪。<br>笑乏惜香门，夜酌闻岩欢。<br>我此一可驰，三刀瓦风樽。东秋清柱晚，意歌送高颜。<br>别留一失在，推时悲紫踪。群远亭我顶，机赠沾酒旋。<br>明镜若相巨，明窗忘语平。桃水凌瑶心，茫讼落清眉。<br>横产无商娥，万流应长生。</p>
<p>  卷171_2 【酬纪寿阳送官】李白</p>
<p>白浦欲溪山，去入北酒人。云水薄阶弄，乃啾迹风声。<br>诗命侍飞儿，百结清霞杯。何言思君去，但然谢无歇。</p>
<p>  卷176_21 【荆闺崔嵩人宰】李白</p>
<p>常鹉别帝家，绮书逐惟安。西世东山玉，雕是金东辉。<br>绿登紫鹿色，张看弄月萝。思手穷津水，弄是俨归人。<br>相恐新鹉道，渌声赠恨公。</p>
</blockquote>
<p>这里出来的效果就很惊人了，我们从小就在课本上学习了诗仙李白的许多佳作，可以说大家对于一个诗人的诗的韵味是怎样是很有体会的，在这些字里行间仔细品味，我们完全可以体会到李太白的豪放与洒脱。</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DNN/" rel="tag"># DNN</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/05/25/2016-05-25-pythonoop/" rel="next" title="Python OOP（Object Oriented Programming）">
                <i class="fa fa-chevron-left"></i> Python OOP（Object Oriented Programming）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/06/04/2016-06-04-note-for-cs224d-1/" rel="prev" title="note for CS224d:1">
                note for CS224d:1 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="江南消夏" />
          <p class="site-author-name" itemprop="name">江南消夏</p>
           
              <p class="site-description motion-element" itemprop="description">当你的才华还撑不起你的野心时，你就应该静下心来学习。</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">65</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">38</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="https://hujiaweibujidao.github.io/" title="胡家威" target="_blank">胡家威</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://my.csdn.net/yywan1314520" title="-dragon-" target="_blank">-dragon-</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://e1ias.github.io/" title="E1ias" target="_blank">E1ias</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://cuiqingcai.com/" title="静觅" target="_blank">静觅</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://hubojing.me/" title="胡博靖" target="_blank">胡博靖</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://gjoker.github.io/" title="GJoker" target="_blank">GJoker</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://medium.com/@znwindy/" title="My_Medium" target="_blank">My_Medium</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#karpathy的example实验-cpu版本"><span class="nav-number">1.</span> <span class="nav-text">karpathy的example实验-cpu版本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#karpathy的example实验-gpu版本"><span class="nav-number">2.</span> <span class="nav-text">karpathy的example实验-gpu版本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对比训练过程模型表现力"><span class="nav-number">3.</span> <span class="nav-text">对比训练过程模型表现力</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#下载下来的全唐诗-txt是gbk编码的，首先需要转换为utf-8编码："><span class="nav-number">3.1.</span> <span class="nav-text">下载下来的全唐诗.txt是gbk编码的，首先需要转换为utf-8编码：</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">江南消夏</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'frankchen';
      var disqus_identifier = '2016/05/26/2016-05-26-char-rnn-chinese/';

      var disqus_title = "char-rnn-chinese";


      function run_disqus_script(disqus_script) {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');

      
        var disqus_config = function () {
            this.page.url = disqus_url;
            this.page.identifier = disqus_identifier;
            this.page.title = disqus_title;
        };
        run_disqus_script('embed.js');
      

    </script>
  










  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("fI3sRwLoiR3Em423yqCIWgeU-gzGzoHsz", "Cs92ouK8fpRmjGwdLvadr6vP");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  


  

</body>
</html>
