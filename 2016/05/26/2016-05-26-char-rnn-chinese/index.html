<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"frankchen.xyz","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本文主要根据Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch的内容来进行试验。 #准备工作 根据原文“This code is written in Lua and requires Torch. Additionally, you need to">
<meta property="og:type" content="article">
<meta property="og:title" content="char-rnn-chinese">
<meta property="og:url" content="https://frankchen.xyz/2016/05/26/2016-05-26-char-rnn-chinese/index.html">
<meta property="og:site_name" content="不正经数据科学家">
<meta property="og:description" content="本文主要根据Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch的内容来进行试验。 #准备工作 根据原文“This code is written in Lua and requires Torch. Additionally, you need to">
<meta property="og:locale">
<meta property="article:published_time" content="2016-05-26T07:37:32.000Z">
<meta property="article:modified_time" content="2016-11-11T13:50:33.000Z">
<meta property="article:author" content="frankchen0130">
<meta property="article:tag" content="DNN">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://frankchen.xyz/2016/05/26/2016-05-26-char-rnn-chinese/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>char-rnn-chinese | 不正经数据科学家</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="不正经数据科学家" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">不正经数据科学家</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Marketing/Python/Clojure/Game</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://frankchen.xyz/2016/05/26/2016-05-26-char-rnn-chinese/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="frankchen0130">
      <meta itemprop="description" content="Marketing/Python/Clojure/Game">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不正经数据科学家">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          char-rnn-chinese
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-05-26 15:37:32" itemprop="dateCreated datePublished" datetime="2016-05-26T15:37:32+08:00">2016-05-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2016-11-11 21:50:33" itemprop="dateModified" datetime="2016-11-11T21:50:33+08:00">2016-11-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tutorial/" itemprop="url" rel="index"><span itemprop="name">tutorial</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文主要根据<a target="_blank" rel="noopener" href="https://github.com/zhangzibin/char-rnn-chinese">Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch</a>的内容来进行试验。</p>
<p>#准备工作</p>
<p>根据原文“This code is written in Lua and requires Torch. Additionally, you need to install the nngraph and optim packages using LuaRocks”，安装以下依赖。</p>
<span id="more"></span>

<p>##安装Torch</p>
<p>使用如下的命令安装Torch</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ~/</span><br><span class="line">curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash</span><br><span class="line">git clone https://github.com/torch/distro.git ~/torch --recursive</span><br><span class="line">cd ~/torch; ./install.sh</span><br></pre></td></tr></table></figure>
<p>再用如下命令更新：<br><code>source ~/.bashrc</code></p>
<p>出现如下画面，代表Torch已经装好！</p>
<p>![](/images/2016/05/Screenshot from 2016-05-26 19-51-17.png)</p>
<p>##安装lua<br><code>sudo apt-get install lua5.2</code></p>
<p>##安装其他依赖</p>
<p>使用<code>LuaRocks</code>来安装<code>nngraph</code>和<code>optim</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">luarocks install nngraph</span><br><span class="line">luarocks install optim</span><br></pre></td></tr></table></figure>
<p>首先安装<a target="_blank" rel="noopener" href="https://luarocks.org/"><code>LuaRocks</code></a><br>安装时在<code>config</code>部分遇到问题，参考<a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000003920034">安装Luarocks</a>和<a target="_blank" rel="noopener" href="http://www.2cto.com/os/201506/412629.html">linux下lua开发环境安装</a><br>这时可能遇到安装了<code>lua</code>但是却提示无法找到<code>lua.h</code>可能是因为还需要安装<code>liblua5.1-0-dev</code>的缘故。<br><del>使用<code>apt-get</code>安装luarocks后在安装<code>nngraph</code>时报错，需要解决</del></p>
<p>==其实使用<code>torch</code>内自带的<code>luarocks</code>安装即可==：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ~/torch/install/bin/luarocks install</span><br></pre></td></tr></table></figure>
<p>因为本机只有英特尔核显，所以只打算用CPU计算，就不安装<code>CUDA</code>了。</p>
<p>#开始实验</p>
<h2 id="karpathy的example实验-cpu版本"><a href="#karpathy的example实验-cpu版本" class="headerlink" title="karpathy的example实验-cpu版本"></a>karpathy的example实验-cpu版本</h2><p>###training过程</p>
<p>使用<code>th train.lua --help</code>查看一下各参数的作用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">Options</span><br><span class="line">  -data_dir                  data directory. Should contain the file input.txt with input data [data/tinyshakespeare] 训练语料</span><br><span class="line">  -min_freq                  min frequent of character [0]</span><br><span class="line">  -rnn_size                  size of LSTM internal state [128]</span><br><span class="line">  -num_layers                number of layers in the LSTM [2]</span><br><span class="line">  -model                     for now only lstm is supported. keep fixed [lstm]</span><br><span class="line">  -learning_rate             learning rate [0.002]</span><br><span class="line">  -learning_rate_decay       learning rate decay [0.97]</span><br><span class="line">  -learning_rate_decay_after in number of epochs, when to start decaying the learning rate [10]</span><br><span class="line">  -decay_rate                decay rate for rmsprop [0.95]</span><br><span class="line">  -dropout                   dropout for regularization, used after each RNN hidden layer. 0 = no dropout [0]</span><br><span class="line">  -seq_length                number of timesteps to unroll for [50]</span><br><span class="line">  -batch_size                number of sequences to train on in parallel [50]</span><br><span class="line">  -max_epochs                number of full passes through the training data [50]</span><br><span class="line">  -grad_clip                 clip gradients at this value [5]</span><br><span class="line">  -train_frac                fraction of data that goes into train set [0.95]</span><br><span class="line">  -val_frac                  fraction of data that goes into validation set [0.05]</span><br><span class="line">  -init_from                 initialize network parameters from checkpoint at this path []</span><br><span class="line">  -seed                      torch manual random number generator seed [123]</span><br><span class="line">  -print_every               how many steps/minibatches between printing out the loss [1]</span><br><span class="line">  -eval_val_every            every how many iterations should we evaluate on validation data? [2000]</span><br><span class="line">  -checkpoint_dir            output directory where checkpoints get written [cv]</span><br><span class="line">  -savefile                  filename to autosave the checkpont to. Will be inside checkpoint_dir/ [lstm]</span><br><span class="line">  -accurate_gpu_timing       set this flag to 1 to get precise timings when using GPU. Might make code bit slower but reports accurate timings. [0]</span><br><span class="line">  -gpuid                     which gpu to use. -1 = use CPU [0]</span><br><span class="line">  -opencl                    use OpenCL (instead of CUDA) [0]</span><br><span class="line">  -use_ss                    whether use scheduled sampling during training [1]</span><br><span class="line">  -start_ss                  start amount of truth data to be given to the model when using ss [1]</span><br><span class="line">  -decay_ss                  ss amount decay rate of each epoch [0.005]</span><br><span class="line">  -min_ss                    minimum amount of truth data to be given to the model when using ss [0.9]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>按照Github上的说明进行实验，使用原文件夹里的语料，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">th train.lua -data_dir data/tinyshakespeare/shakespeare_input.txt -gpuid -1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>报错：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"> th train.lua -data_dir data/tinyshakespeare/shakespeare_input.txt -gpuid -1</span><br><span class="line">vocab.t7 and data.t7 do not exist. Running preprocessing...</span><br><span class="line">one-time setup: preprocessing input text file data/tinyshakespeare/shakespeare_input.txt/input.txt...</span><br><span class="line">loading text file...</span><br><span class="line">/home/frank/torch/install/bin/luajit: cannot open &lt;data/tinyshakespeare/shakespeare_input.txt/input.txt&gt; in mode r  at /home/frank/torch/pkg/torch/lib/TH/THDiskFile.c:649</span><br><span class="line">stack traceback:</span><br><span class="line">	[C]: at 0x7f9c42473540</span><br><span class="line">	[C]: in function &#x27;DiskFile&#x27;</span><br><span class="line">	./util/CharSplitLMMinibatchLoader.lua:201: in function &#x27;text_to_tensor&#x27;</span><br><span class="line">	./util/CharSplitLMMinibatchLoader.lua:38: in function &#x27;create&#x27;</span><br><span class="line">	train.lua:118: in main chunk</span><br><span class="line">	[C]: in function &#x27;dofile&#x27;</span><br><span class="line">	...rank/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk</span><br><span class="line">	[C]: at 0x00405d70</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里出现了问题，因为本文是中国作者按照原<a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn">karpathy的char-rnn
</a>改写的，我认为或许使用karpathy作者的原版本教程可能会更加方便一些。于是使用<em>As a sanity check</em>，运行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">th train.lua -gpuid -1</span><br></pre></td></tr></table></figure>
<p>这指的是使用CPU并不指定任何参数来训练example。</p>
<p>15:42开始训练</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> th train.lua -gpuid -1</span><br><span class="line">loading data files...</span><br><span class="line">cutting off end of data so that the batches/sequences divide evenly</span><br><span class="line">reshaping tensor...</span><br><span class="line">data load done. Number of data batches in train: 423, val: 23, test: 0</span><br><span class="line">vocab size: 65</span><br><span class="line">creating an LSTM with 2 layers</span><br><span class="line">setting forget gate biases to 1 in LSTM layer 1</span><br><span class="line">setting forget gate biases to 1 in LSTM layer 2</span><br><span class="line">number of parameters in the model: 240321</span><br><span class="line">cloning rnn</span><br><span class="line">cloning criterion</span><br><span class="line">1/21150 (epoch 0.002), train_loss = 4.19803724, grad/param norm = 5.1721e-01, time/batch = 2.3129s</span><br><span class="line">2/21150 (epoch 0.005), train_loss = 3.93712133, grad/param norm = 1.4679e+00, time/batch = 2.3114s</span><br><span class="line">3/21150 (epoch 0.007), train_loss = 3.43764434, grad/param norm = 9.5800e-01, time/batch = 2.3022s</span><br><span class="line">4/21150 (epoch 0.009), train_loss = 3.41313742, grad/param norm = 7.5143e-01, time/batch = 2.5311s</span><br><span class="line">5/21150 (epoch 0.012), train_loss = 3.33707270, grad/param norm = 6.9269e-01, time/batch = 2.4913s</span><br></pre></td></tr></table></figure>
<p>到第300次迭代后，<code>time/batch</code>稳定在2.3s左右，也就是说，使用GPU训练这个1Mb的example，需要约14小时！<br>次日08:24训练完毕</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">21148/21150 (epoch 49.995), train_loss = 1.53254314, grad/param norm = 5.9157e-02, time/batch = 2.8658s</span><br><span class="line">21149/21150 (epoch 49.998), train_loss = 1.50882624, grad/param norm = 5.7123e-02, time/batch = 2.8737s</span><br><span class="line">decayed learning rate by a factor 0.97 to 0.00057368183755432</span><br><span class="line">evaluating loss over split index 2</span><br><span class="line">saving checkpoint to cv/lm_lstm_epoch50.00_1.3568.t7</span><br><span class="line">21150/21150 (epoch 50.000), train_loss = 1.46142484, grad/param norm = 5.9032e-02, time/batch = 2.8834s</span><br></pre></td></tr></table></figure>

<p>###Sample过程<br>查看<code>help</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">th sample.lua --help</span><br><span class="line">Usage: /home/frank/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th [options] &lt;model&gt;</span><br><span class="line"></span><br><span class="line">Sample from a character-level language model</span><br><span class="line"></span><br><span class="line">Options</span><br><span class="line">  &lt;model&gt;      model checkpoint to use for sampling</span><br><span class="line">  -seed        random number generator&#x27;s seed [123]</span><br><span class="line">  -sample       0 to use max at each timestep, 1 to sample at each timestep [1]</span><br><span class="line">  -primetext   used as a prompt to &quot;seed&quot; the state of the LSTM using a given sequence, before we sample. []</span><br><span class="line">  -length      max number of characters to sample [2000] 采样字符大小，最大2000</span><br><span class="line">  -temperature temperature of sampling [1]</span><br><span class="line">  -gpuid       which gpu to use. -1 = use CPU [0] 和训练时设置应该保持一致</span><br><span class="line">  -verbose     set to 0 to ONLY print the sampled text, no diagnostics [1]</span><br><span class="line">  -stop        stop sampling when detected [</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>先试运行一下<br><code>th sample.lua cv/lm_lstm_epoch50.00_1.3568.t7 -gpuid -1 </code><br>生成了如下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">giff,</span><br><span class="line">Some sweet amends, aasher, had therein, not he had wot on</span><br><span class="line">man&#x27;s friends, for her own blow: for my men&#x27;s</span><br><span class="line">ackingly knight, it cannot hear upon yield.</span><br><span class="line"></span><br><span class="line">GLOUCESTER:</span><br><span class="line">How now again?</span><br><span class="line">i</span><br><span class="line">March, that&#x27;s my arm determitness,</span><br><span class="line">The temper of the vrowal; ere from the grove</span><br><span class="line">Tut wilh &#x27;goom&#x27;d Carulea.</span><br><span class="line">&#x27;Mwailich, tne shy had lost in When Ied the way</span><br><span class="line">The bower to a late his body, grim on you:</span><br><span class="line">His opicious shames a booy, infairs,&#x27;</span><br><span class="line">From her, I tell you, ay, as we mean him</span><br><span class="line">Dear &#x27;tis a giving o&#x27; thur back&#x27;s empass&#x27;d,</span><br><span class="line">That nrost, I&#x27;ll havk him cume thee for &#x27;t.</span><br><span class="line"></span><br><span class="line">LAONTES:</span><br><span class="line">&#x27;Twi&#x27;l love thy sronowing.</span><br><span class="line"></span><br><span class="line">VALYRAN:</span><br><span class="line">Beord mocdoch him for thy follight</span><br><span class="line">snn</span><br><span class="line">hours,</span><br><span class="line">But thank yours lodkes, my good journeding,</span><br><span class="line">His jealousisposour thee are both abomish</span><br><span class="line">That noom that&#x27;s easembelland. Camest, sir, more</span><br><span class="line">kia; one, in this highty be the un</span><br><span class="line">Since of a gournor on thy friendshall swow</span><br><span class="line">Some painon; and I, and lord, the  at the kins</span><br><span class="line">Wise rit hable surliments. Shd, believh gone.</span><br><span class="line"></span><br><span class="line">voisted tleace:</span><br><span class="line">Tock him what all you di turn up to celent</span><br><span class="line">To my sistinge. Frranch, good night, your child, so fatus;</span><br><span class="line">Aor he shall be my trueking:</span><br><span class="line">Come on my quarrel of the way:</span><br><span class="line">Methinks the letters; for this ctome-steers</span><br><span class="line">Tad mousd my smodered pouncy to</span><br><span class="line">haw up another sense tlays underttry</span><br><span class="line">Tut bonscuration fair all purpose,</span><br><span class="line">then be vesegt me: do not, yet rustle cannot,</span><br><span class="line">But for thy mustered a dust, let me</span><br><span class="line">Tncerfact me tresmer of his father:</span><br><span class="line">therefore by hanging,</span><br><span class="line">ANd</span><br><span class="line">Ays, my lord: you do here in coumisant.</span><br><span class="line"></span><br><span class="line">LORD:</span><br><span class="line">How lond the brown!</span><br><span class="line">So majp me; bonch, smmily  lovely blotters,</span><br><span class="line">When Ie my hoeaty threat and virlume these things,</span><br><span class="line">Make fasting garlands dfar the sack&#x27;d my servictught</span><br><span class="line">Not knows the crowns: one air, Aumerle,</span><br><span class="line">Ere wear not so nour Bidagle? What Aphark is fury</span><br><span class="line">Tld meens them, faireyou consides to no more</span><br><span class="line">Ihis wantond frown and pollitueser&#x27;d city.</span><br><span class="line">Can should put him more recounders to impudesnt poison on</span><br><span class="line">thet hour from hunt to Rame, supp to bere</span><br><span class="line">Flowerd and his friend is une dewn ao pirt,</span><br><span class="line">You know by join&#x27;d guilty, whathout we e.</span><br><span class="line"></span><br><span class="line">ANd</span><br><span class="line">Ays, my lord: you do here in coumisant.</span><br><span class="line"></span><br><span class="line">LORD:</span><br><span class="line">How lond the brown!</span><br><span class="line">So majp me; bonch, smmily  lovely blotters,</span><br><span class="line">When Ie my hoeaty threat and virlume these things,</span><br><span class="line">Make fasting garlands dfar the sack&#x27;d my servictught</span><br><span class="line">Not knows the crowns: one air, Aumerle,</span><br><span class="line">Ere wear not so nour Bidagle? What Aphark is fury</span><br><span class="line">Tld meens them, faire</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="karpathy的example实验-gpu版本"><a href="#karpathy的example实验-gpu版本" class="headerlink" title="karpathy的example实验-gpu版本"></a>karpathy的example实验-gpu版本</h2><p>使用和cpu版本相同的指令，只是<code>th train.lua -gpuid 0</code><br>得到的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">th sample.lua cv/lm_lstm_epoch50.00_1.3622.t7 -gpuid 0</span><br></pre></td></tr></table></figure>
<p>sample为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">--------------------------</span><br><span class="line">ge</span><br><span class="line">I prithee; nor in the day of all report?</span><br><span class="line">Nou shall be you that lances stoson quarrel:</span><br><span class="line">We mits most stone, &#x27;aim, upeed his forticent</span><br><span class="line">Ahen I was, yet for her, but of lovels Clarencel</span><br><span class="line">And past her got and father there, one weep</span><br><span class="line">In mine  wroth nightlys, smileed. Cantleye An York,</span><br><span class="line">A druls marriage, that though Service hated me</span><br><span class="line">Their duen of iflock, we are here in berieved</span><br><span class="line">and more than tanise them with suterept</span><br><span class="line">He rt in some pickle.</span><br><span class="line"></span><br><span class="line">SePSERTER:</span><br><span class="line">Or I have safe all thou depustere andthe before him.</span><br><span class="line"></span><br><span class="line">FRIAR LAURENCE:</span><br><span class="line">What art my government, cosbude, and hence; if Onfrawn? provest tor my duty?</span><br><span class="line"></span><br><span class="line">CATESBY:</span><br><span class="line"></span><br><span class="line">KARIANA:</span><br><span class="line">My love noe is are  with t herman and his,</span><br><span class="line">It should she well deeauring our consent:</span><br><span class="line">They hang me bointed on the king, let so two</span><br><span class="line">Nature by my sighsing pleasing &#x27;jabe</span><br><span class="line">That leaven and grue, at her Richard&#x27;s blood.</span><br><span class="line">More ends it likipenortnive, nor each of him</span><br><span class="line">ic.</span><br><span class="line"></span><br><span class="line">SLY:</span><br><span class="line">How dachors, Richmend, henr dack but like it?</span><br><span class="line">Be long, anon since your kingdom and us,</span><br><span class="line">And we that aver el</span><br><span class="line">aunter, my eee to toucurt tomends.</span><br><span class="line">It this her great fawn&#x27;s birds,, sir! you&#x27;er head.</span><br><span class="line"></span><br><span class="line">PAULINA:</span><br><span class="line">Upternalt cost of his hands for their tricks my father,</span><br><span class="line">Who ts it most seunt to live te and she were all.</span><br><span class="line">-kill</span><br><span class="line">O to thy son os shall not on your childrs,</span><br><span class="line">one next, for she did formly consixent</span><br><span class="line">Above, my life, and wew me worthy deeming tvenge!</span><br><span class="line">My mustere be exploience, aot come n leave where ahe knees in.</span><br><span class="line">dear, thus wild up tilt on the county, hath be one.</span><br><span class="line">See this sword of thee with the deepito man,</span><br><span class="line">For sunier ene first sears. Where&#x27;s turn on to be.</span><br><span class="line">Unctious blunlest terrocate doves</span><br><span class="line">Trades Marcius aines of hlends</span><br><span class="line">My&#x27;s learth an old--ay.</span><br><span class="line"></span><br><span class="line">LEONTES:</span><br><span class="line">Marcius?</span><br><span class="line"></span><br><span class="line">PRONVO:</span><br><span class="line">You would no gue.</span><br><span class="line"></span><br><span class="line">VOLUMNIA:</span><br><span class="line">Oovine s fetch to tight, thou must but loods.</span><br><span class="line"></span><br><span class="line">HASTINGS:</span><br><span class="line">And was with her, nor yonder to be sworn,</span><br><span class="line">What are you allady that I should have purpose.</span><br><span class="line">What men  revenge is a well patient</span><br><span class="line">And who seth sxoleng to knowled ed to myself;</span><br><span class="line">And married me in the joy:</span><br><span class="line">So reve I made to find me speak,</span><br><span class="line">how he been tou.</span><br><span class="line"></span><br><span class="line">PETCA:</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>##《水浒传》语料实验</p>
<p>###cpu版本</p>
<p>把下载好的《水浒传》改名为<code>input.txt</code><br>使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">th train.lua -data_dir data/mydata/ -gpuid -1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>训练，可以看到很明显，速度很慢</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">th train.lua -data_dir data/mydata/ -gpuid -1</span><br><span class="line">vocab.t7 and data.t7 do not exist. Running preprocessing...</span><br><span class="line">one-time setup: preprocessing input text file data/mydata/input.txt...</span><br><span class="line">loading text file...</span><br><span class="line">creating vocabulary mapping...</span><br><span class="line">putting data into tensor, it takes a lot of time...</span><br><span class="line">saving data/mydata/vocab.t7</span><br><span class="line">saving data/mydata/data.t7</span><br><span class="line">loading data files...</span><br><span class="line">cutting off end of data so that the batches/sequences divide evenly</span><br><span class="line">reshaping tensor...</span><br><span class="line">data load done. Number of data batches in train: 345, val: 19, test: 0</span><br><span class="line">vocab size: 4129</span><br><span class="line">creating an LSTM with 2 layers</span><br><span class="line">setting forget gate biases to 1 in LSTM layer 1</span><br><span class="line">setting forget gate biases to 1 in LSTM layer 2</span><br><span class="line">number of parameters in the model: 2845345</span><br><span class="line">cloning rnn</span><br><span class="line">cloning criterion</span><br><span class="line">1/17250 (epoch 0.003), train_loss = 8.32795887, grad/param norm = 9.6310e-02, time/batch = 28.8711s</span><br><span class="line">2/17250 (epoch 0.006), train_loss = 8.06433859, grad/param norm = 4.2826e-01, time/batch = 25.2184s</span><br><span class="line">3/17250 (epoch 0.009), train_loss = 7.28941094, grad/param norm = 3.9537e-01, time/batch = 25.2195s</span><br><span class="line">4/17250 (epoch 0.012), train_loss = 6.85331761, grad/param norm = 4.0576e-01, time/batch = 25.2011s</span><br><span class="line">5/17250 (epoch 0.014), train_loss = 6.69327439, grad/param norm = 3.8309e-01, time/batch = 24.9642s</span><br><span class="line">6/17250 (epoch 0.017), train_loss = 6.50776019, grad/param norm = 3.1042e-01, time/batch = 24.9203s</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>预计需要120小时训练时间！但是，这都是未经过处理的语料，后续使用处理过的余料（如去掉低频词语等）再来训练应该会更快。因为时间太长，所以这个实验被放弃了。<br>###GPU版本的未处理语料实验<br>首先对未处理语料做训练：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">th train.lua -data_dir data/mydata/ -gpuid 0</span><br></pre></td></tr></table></figure>
<p>begin at 10:20<br>可以看到<code>time/batch</code>稳定在0.08s左右，也就是半小时就可以训练完成！GPU比cpu在科学计算上面实在是太强大了。<br>训练完毕，使用：<br><code>th sample.lua cv/lm_lstm_epoch50.00_3.9309.t7 </code></p>
<blockquote>
<p>者，却得出来的物细物，没面少管生渔人。后来的知府时是乱色早晚，拾了几个。李逵惊得忙忙轻梳药穿在大牢里，摆在延安家处，推慰九节的。当下径到居中饮酒，牌门头，戴宗又焦躁。只见屏风背后转出一个小风大来，暗暗听得道：“反细放俺!兄弟拿着，趁这为害天明地清，我休要推道别事的都要做伴当拆投到会耳，便有进漏？”时迁舞起树下探人，的了夹搭，都拽了拽开，胸皮虽是好了六分惊得，是他麻。吐女棒放火了，走不向前，及宋江那道个留守他做个辩察的，先自去州里请明地烧了钱用，但有过京回家，听得状子好!这高老袋内却是出张招安，又都是他的欺负民，如何是计信?必须要和郎相会。且。”趁早起楼去了。两个连夜时候，仓治时，年二十八执迷者多要都要去行叶，早是亲家洒家，径挨到府前来。灯烛纸敌官，方才脱漏。亦被乱窝中有人等好人知蔡京道：“那个人也是他甚么?因此教大军打劫俺那干干金珠的。”母子那妇人来到大王尚安着，相交酒追惹三清酒搦战。不过半夜之事，早饭相烦，心里出对知府说：“官家初时在时，欲要市上时，兀自和我觅家劫犯了他，如何不赶我里来?我大小心定得，不分便了。”叉军转回，已做些小头，要打抵敌他!且把身，带了七个人时，都抢出家，但见：　　<br>    壮中醪浑纷领，腰细轻露。阔尺三层挺刀，鱼厚夸敌庆孙。高人有八句诗单道身心强似莽?；小付敲柴的，真多呼圣殿之主贞锋欣乐词。头邬闻丹腊良夫，耍达缘矮岁龙；四虎间，寒暮难以偷黄；牢记仁诸作像，显宝一根红佛。微。善得山迎能指挥，直救清天马星。　　<br>    话说上阵法， 师皇帝展得有?宫殿，雨翠云也上田地里。幸观非乃帝重了，宋江心如有誓，同宋受迷。却诗名唤，只传说开门，因此是贼人心腹事务，到宋江纠合生灵害，在忠渐存母亲来宋江以心却才，惊得义既灵垂德，对公孙胜为然无智真道好法，正为：须游六十为聚义，好像原林密寨郎山神保。　　<br>    当日宋庄客帐前，与晁保、公二位头领，众头领发起作法。　　<br>    石裂更兼地分都拨人汉，且不杀得蒙恩干人结义，下山只是锦袋百把，们有父亲孔宾，同商量。宋江又道：“自是好生，莫非也只是是有哥哥下了。”吴用笑道：“兄弟，不到山寨，吴用命作商量，将军不与他长犬马，力休曾平他：一话难以安身，宋江一力不东昌下几日，谁想大哥哥教小晁盖哥哥会合当的事。我们人投随天军来，又有伤损；若不连环甲关，着李横其不似火体，车藏御上尽挂玉水；军卒许多，无无不难之际?他但**，可等兵，可以斩遣。”众军健都管入庄，要把鲜血迸成，赶起来，背后解水边，唤车军跨城疮只等，原来正是之福，后往，来不见三个使汉。因弟清风船救应。路，至是路买酒，又拨五七百名、罩、白、孔亮，正将费珍、薛霸，尽是钱十二十军，其余的人在彼，欲得众兵险道地广花荣抵敌人住。这一队节度使士都军，被两个军猛，呐声喊，都抢过城里，并无腰迎敌，被贼兵赶上，时，却被花荣战箭射来。童太人、杨志正是南安江韩杨龙、穆弘、李逵、索超正定敌。孙军纵马。琼清马挺着枪，入来，尽被史进和贼人杀死贼兵，擒做霹雳。邬梨因成让风，连鼓上马，将股斧，却飞入阵，大小张清见了见乱军阵前卒法败坑回阵，宋江旭前只是：　　<br>    主人问姓，五应风万。侯海道正：“恶平可逃奔：。时们村野阴血，呼往天兵消波。正被杨志聚领渡江，望宋江攻还山庵；拔寨教活林冲、公万一通，并添下山南二王庆名事虚权，再被小人在戴上探山泊路，几路去报，不敢准备。不知这个人说起是百庄小黑凌州，已曾见了，对别无缘。”吴用道道：“便队军马解到此时，必是殡隘为百谷岭。原师悬流水军头二头领，结识江湖上好汉姓石，名给鬼，便乃五家庄二多情。我去这里地路，望会便行。”廊进雷车把人来不止，李应拈着诏书，自此付话。　　<br>    且说山客渡过了三只路，教穆弘扮做伴当，扮做阎婆者，带拢是臭镇一个没赌什门，分顺了同行，自去寻闭了的。原自去被人运烧将下去了。宋江等远远，一路进兵。十来县不在大路途来，又怕了到得闲意的张社长，听得监押一声，货钱便是。任原陈达在中，不知处打那华州，特使他来掳去太安军肆，只待下山。戴宗告随张小人，蔡九知府不得，连夜回话，同张招讨干办、众部吉。于路，忽报探知样悔，景珍全过，领回商议，“军师赵枢密喜绯金带，身上悬面草板，护道国师，服，神色不通。是奇诈将丘留的人，准备起船走径来借粮，业不同何遇一深困马灵夫，便因密的月色渐砂来完，斋。小温皇威，被宣刚引军来，武松彼朵并顾大嫂，赴了逃去，自逃命探了。被那几人娅?在古靖军吟涂炭，，态纪士，接应喊道，漏转身来，复有神诗，燕世曰：“寡人仰云监斩辽王康公外交法，何”奈阵圣怒须性重。铁挨填丹靴，万边狄行鉴。见。田户观看草畔，红日影豪困催急绩。宋玉游战，听听了大喜。话说宿太师诏奏道：“宿元帅差有敕入请罗真人，密封官军等八员高名，封当同达宋先锋。”日收选润之主，奏为圣旨，特着州殿府探知。太尉宿太尉回到内，启转马，众军方可亦成开大事，放起出来，更兼小一个唤做</p>
</blockquote>
<p>##使用增强RNN网络训练</p>
<p>如下，使用512个隐藏节点的3层RNN网络训练模型</p>
<p><code>th train.lua -data_dir data/shuihuzhuan/ -gpuid 0  -rnn_size 512 -num_layers 3 </code><br><code>th sample.lua cv/lm_lstm_epoch50.00_5.4830.t7 -sample 0 -temperature 0.8 -verbose 0 -length 500</code></p>
<blockquote>
<p>弟兄两个，也得个信名之人。”那个也是个道理。童家四更，被张顺斩的粉碎，以下人人家拿去了。一面叫酒保打两桶酒来。小二哥叫道：“师父，你不是我来也！”那小牢子道：“我也不曾你，你便叫我上来。”　　 石秀道：“你且说他三宫百里吃酒了来，你便抢入去，你便先来看，却被这畜生说不得了。”那妇人道：“你真人要打这里话?你却不认你，你便叫我儿来寻。”李逵道：“你敢作吃的，便揪我做脚！”赤条条地寻谁，只得骂道：“爹娘，你且休了，我自不信，砍我头便打那老娘。”那妇人道：“也好。”便把袖儿丢下去了。那妇人也把刀带在一边，却似小窗????胡乱道：“好拳脚！”急叫开了店看。”王庆听了，连声叫道：“阿也!你不要吃！”把手一指，提倒上岸来，把朴刀倚在被里。就把篙子门内，倒做五六斤了，将把木鱼来摆下桶桶。少时，张顺吃了一回。两个回到店前，再出来赏赐，解了戒刀，包了水出去，到四更，把船渡入去，便叫艄公下楼，买了些鱼吃，把些酒肉吃了，酒保做些桶汤、盘酒、些肉。下来穿瓶与酒。一瓶儿酒肉，买些肉吃，只见店主人把包裹插下，那妇人也吃得饱了，口里说道：“娘子，老身等这几个泼，不要吃酒钱。”店小二道：“好酒好肉要打，我吃便饱</p>
</blockquote>
<p>很明显，此时sample的样本语句更加通顺，错误很少，从品味小说的角度来讲，增强了的RNN训练得到的模型更加完美了。 可以看到，**增加了节点数和隐藏层的RNN具有更强的学习能力。<br>**</p>
<h2 id="对比训练过程模型表现力"><a href="#对比训练过程模型表现力" class="headerlink" title="对比训练过程模型表现力"></a>对比训练过程模型表现力</h2><p>与此同时我们可以对比一下，训练开始阶段与训练结束时的模型表现力的差异：</p>
<p><code>th sample.lua cv/06-01-shz_sp/lm_lstm_epoch5.80_4.0451.t7 -sample 0 -temperature 0.8 -verbose 0 -length 500</code></p>
<p>训练刚刚进行到5.8（为50时完成）得到的是：</p>
<blockquote>
<p>只见一个人从来，一个人，都来做一个。那人道：“你这厮们，我自去寻你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不</p>
</blockquote>
<p>此时语料具有较多的重复，模型还没有良好的收敛。</p>
<p>继续观察，当进行到四分之一左右时：</p>
<blockquote>
<p>那汉子听了，便问道：“你这厮不是歹人，如何不来?我们不曾有这般的，如何不来?你的那里去了？”那汉道：“我们不曾说谎。”那汉道：“我不曾说谎。”那汉道：“我不曾说谎。”那汉道：“我不曾说谎。”那汉道：“我不曾说谎。”那汉道：“既是恁地，我们自去。”那汉道：“既是恁地，我们自去。”那汉道：“既然恁地，我们自去买碗酒吃。”那汉道：“既是恁地，我们自去。”那汉道：“既然恁地，我们自去买碗酒吃。”那汉道：“既是恁地，我们自去。”那挑酒的汉子道：“我们自有计较，我们自有些钱，与你些银两，却去商议。”董超道：“我们不曾有这般的事，如何不来?你们自去取路，我和你如何不去?若还了他时，便是要去的。”李逵道：“你们不曾说的，你便是个老儿，如何不来?我们自去取他，你便不曾与他厮见。”那妇人道：“你的，你不省得。”那妇人道：“你的，不要胡说!我们自有钱帛，便要去便了。”那妇人道：“既是恁地，我们自去买些银子与我。”那妇人道：“我们自有钱与你，你便不曾去了。”那妇人道：“既是恁地，我们便去。”那妇人道：“既是恁地，我们便去。”那妇人道：“你的，不要胡说!我如何肯去这里？”那妇人道：“便是这般使棒，不曾得得他。</p>
</blockquote>
<p>再看看训练继续进行到一半左右时候的表现力：</p>
<blockquote>
<p>张都监为副将急体己人，不敢不依，只得随行众军，掌行送行。只留下降，尽皆欢喜，以此忠义。在府行军中，有使枪棒卖药的，将王庆领军到来，并不必说。当下宋江传令，教中军计策，与卢俊义等商议：“今日折了两阵，俺们自去了。”宋江道：“军师之言甚善！”当下即日便传将令，教军士点营，斩动军马。将及初五更战后，攻打常州，催趱军兵，一齐进发。 寨中，只听得高声叫道：“萧让等救兵！”宋江看那军将，尽数放起，对幽州一个大汉，乃古头上大叫道：“水洼草寇，怎敢轻慢！”只见里面关胜、呼延灼、关胜等探有一人，只见唐斌从人骑马，直到宋江寨前，喝请宋先锋。宋江听了大喜，传令令军士且去寨中坐地，备说宋先锋军马，攻打北京。吴用道：“且教两路军马，攻打北门。”宋江便令吴用、朱武商议：“今日可去，只是国师吴用，坐一件事，我等随顺到此，可用两处夹攻，那厮必然有人来。”宋江道：“军师言之极当。”便唤军士计策。”宋江道：“军师言之极当。”吴用道：“小生直作妙计，即且闻见。”宋江道：“先生之言，是不得这般忧疑。”宋江道：“既然如此，与你四位豪杰，不堪员外大王。”宋江道：“贤弟，你休要疑心，我便去请来。”吴用道：“不须你两个与我箭，只</p>
</blockquote>
<p>此时低级的重复没有了，但是可以看到，“宋江道”反复出现，而且说的内容类似，可以得到模型已在进一步完善之中。</p>
<p>这是模型接近训练完成的时候的sample：</p>
<blockquote>
<p>张青、孙二娘、顾大嫂、孙二娘，并四个好汉，引着一千余人，吹造大小船只，都投水路。不多时，只见松树背后转出一个小小人来，簇拥着两个人，各提着朴刀，背后有人，叫一声：“捉下！”　　 那汉子把船只一招，扶着一干人，把那碗饭打伤，打的粉碎，把头头割在一边，口里放火。那人见了是惊得呆了，又不来吃了一惊，扑地只顾走。却待再走，再去脱人避凉。李逵却亦不肯拦他，只得走了。可怜救他两个性命，那里敢?别人。前日被捉死了性命，杀了人，逃走在江州，被害人陷害，方得正中了。今日幸得相见，如何使得?便得是个知县过来的，也喜得及。他便是本人的人，须是高太尉的人，却不知是那里人。”任原道：“这个便是我的儿么？”王婆道：“便是前日那官司亲亲叔孝，为何到此？”那婆子答道：“老身只道不妨，只怕小人自有措置。老身看了，便忘了回去。”老都管道：“这个容易。老身先把银酒去了。”老儿道：“你们自不要吃酒。”那婆子也笑起来道：“这个便是我的老小人家。”那婆子道：“便是老身也不怕你，休要胡主干娘，只怕你疑心。”那妇人道：“不干了。你的女儿，老娘儿只做买些衣服来送与你。”王婆道：“娘子，你要知这几个字？”那婆子道：“有甚么哭处？”</p>
</blockquote>
<p>模型进一步完善，接近完成训练。</p>
<p>##《全唐诗》实验</p>
<h3 id="下载下来的全唐诗-txt是gbk编码的，首先需要转换为utf-8编码："><a href="#下载下来的全唐诗-txt是gbk编码的，首先需要转换为utf-8编码：" class="headerlink" title="下载下来的全唐诗.txt是gbk编码的，首先需要转换为utf-8编码："></a>下载下来的<code>全唐诗.txt</code>是<code>gbk</code>编码的，首先需要转换为<code>utf-8</code>编码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    s = <span class="built_in">open</span>(<span class="string">&quot;全唐诗.txt&quot;</span>)</span><br><span class="line">    r = s.read()</span><br><span class="line">    r_uni = r.decode(<span class="string">&#x27;gb2312&#x27;</span>,<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    r_en = r_uni.encode(<span class="string">&#x27;utf-8&#x27;</span>,<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    fp=<span class="built_in">open</span>(<span class="string">&quot;全唐诗转换.txt&quot;</span>,<span class="string">&quot;w&quot;</span>)</span><br><span class="line">    fp.write(r_en)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span> :</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>这样就得到了<code>utf-8</code>编码的文件。</p>
<p>开始想直接把这个文件丢给rnn训练，后来想一想，“全唐诗”包括了众多诗人的杰作，也许训练出来的模型sample不到什么特点鲜明的诗句，于是我首先想对诗仙李白的诗做一个实验。把得到的全唐诗文件中李白的诗切分出来作为一个文件。<br>开始训练：<br><code>th train.lua -data_dir data/tangshi/ -gpuid 0</code><br>得到模型后sample：</p>
<p><code>th sample.lua cv/lm_lstm_epoch50.00_5.3967.t7 </code></p>
<blockquote>
<p>耶翳妃在。流明湖草，岂舞高散纷。小剑宫底寒，石思怀士。<br>归来相烟叹，又余未老此。此在见携节，气帝皇彩川。<br>尝君凌天鸟，羞从臂山中。何旋俱所偃，造愧翻遗耻。<br>悠挥李明云，坐月得相迟。君作成景鸡，暮日清膺发。<br>胡步垂洞松，三年延未莱。贤迢坐橐山，嗤楼谋疏川。<br>诗君九安菲，别去写日流。五时谢及洒，相魄三有玄。</p>
</blockquote>
<blockquote>
<p>卷177_21 【长道送秀士寄之十南国古松游明，》妓此蛾书此下见逃之始崔至六以吟酒宁】李白</p>
</blockquote>
<blockquote>
<p>仙阳壶我王，谣荣日茫过。幸生天行寒，半持见九忧。<br>早毂此楼宰歌，鹗税金归名。宾托昼宇闻，高汶堕臣泉。<br>峻悟湍素都，凤生鸟远才。虎傥亦成一，蹭据锁炳垣。<br>回谷闻叹波，摧人翼昆衣。丑德皆复贵，何袂自见宝。<br>黄然奉傍及，戾酒悲溪情。何悟下罗灭，壁令还济然。</p>
</blockquote>
<blockquote>
<p> 卷177_16 【送沙门饯元佛之嵩使归少丞寺辅晔年亭山云】李白</p>
</blockquote>
<blockquote>
<p>行道一狂日，陆杯欲我园。相寄属相者，不歌流成书。<br>幽景神兰马，今云乃相知。相能拂此门，娟笑无生魂。<br>肠后扫天所，罗瑟心世桥。</p>
</blockquote>
<blockquote>
<p> 卷169_11 【金松二炎师】李白</p>
</blockquote>
<blockquote>
<p>丽劝发何凤，含东药宛锦。且忍咏尺鳌，梦杯池月<br>。<br>罗鸟思归人，兼我无风歇。太色东草树，壮早游罗息。<br>别来青神极，谒长不陵寒。君忧东海鹤，吹欲得彩好。<br>梦君来太情，秦子忆延薪。闻干凌楼息，松水接廉才。<br>且识辞犹之，众断罗里中。</p>
</blockquote>
<blockquote>
<p>水羊远重，引飒高新策。目布愁霜亲，，随火赤云道。<br>解毂四上牛，以且汶云年。宁迎清巴寒，种欺清名风。<br>海坐去不意，思丹酩期然。闻钓曾帆景，一弄暗长雪。<br>且亦留我辉，杀讼韵中楼。</p>
</blockquote>
<blockquote>
<p>  卷174_7 【赠崔司州十三青寺黔洞姑圣毛闲华忘兼塔宅石】李白</p>
</blockquote>
<blockquote>
<p>窜笑敬鹤见梦走去留僧。地筑从尺人，泪树平众烂。<br>窈箸有吉氲，久聪欲洲真。朝春离相兽，飞此何垣发。<br>绿日偶可言，我言经精存。君卒限汉水，绿月相成失。<br>恋虏一登事，但乘涂应星。</p>
</blockquote>
<blockquote>
<p> 卷177_18 【咏夜别（人作帝阳之为昆者）】李白</p>
</blockquote>
<blockquote>
<p>笑乃度将明，蛾洞蕊山发。花坐新合露镜日边归溪。<br>长面云蓬立，自颜谢鸳分。灭用欲得碑，不云当天舟。<br>一来但不霄，乘我涉路来。怀子广陵远，夕人不元阙。<br>笑天亦望好，驱令适谁手。思此有门上，举与满丝君。<br>醉年归敬山，松藏谢未笑。却思笑古兴，凭凌泪高尺。</p>
</blockquote>
<blockquote>
<p>钱文1淮作3<br> 　<br>卷一两十七三北欢宁三元四慈平难名】李白</p>
</blockquote>
<blockquote>
<p>水乘黄楼镜君客绿1起，山门天阙已忍春。不喜出庭作，乎<br>晖酒苔。麒惊美。绮门，我见秦心遂天雪，只行天花流。<br>相随夔山肉烟喧，武斗吹齐而公还。以开玉去戟如雨，<br>欲席上醉鹊里洪。世闻燕弦对士去，曳不笑丘瞳中嗟。<br>国昔逢眼青山鸟，扫水长丘双泉空。</p>
</blockquote>
<blockquote>
<p>  卷165_8 【玉崔将言刺判池，一还精孰日君）】李白</p>
</blockquote>
<blockquote>
<p>军莫荆壮悲，揽迈宋钟客。高当愤酒酒，渴臣达庭边。<br>沙箸佳花诏，映河讵窗中。为交上梧空，空在猛杯闻。<br>惜缅何烦柯，屈笑献神然。万情上者溪，相以陶毫逸。</p>
</blockquote>
<blockquote>
<p>更闻无袂，小必笑伤穷。</p>
</blockquote>
<blockquote>
<p>  卷169_12 【高陵黄入蜀，寄纪侍御二首】李白</p>
</blockquote>
<blockquote>
<p>爱来游山子，北照有华宅。岩歧难碑李，独承崔滹鼙。<br>巨君亦不处，常阖之此酒。思君穷莫术，却成愁庄隈。征六此与玄，，羞逐但应君。</p>
</blockquote>
<blockquote>
<p>  卷189_26 【登溪马归歌，归石怀道怀山】】。<br>长腾明花屏爱心春，寄心入良素断。</p>
</blockquote>
<blockquote>
<p>  卷188_7 【送侍御从尔史崔崖赴天】白山年粲赤浑，书别诗游泛风】李白</p>
</blockquote>
<blockquote>
<p>祖国一官食，二藏系冠鹇。无砚号盆水，浮发王青吟。<br>登水惜不母，殷古启与游。萧觉留见去，待泪复相思。<br>绿风吹中信，江山知洞宫。宾阔未孤里，空可向江峤。<br>西舟青溟云，一浪摇苗彩。流镜沙青辉，夕落得寒洲。<br>妾头海弦弓，而多何皇笑。欢辰虽欢心，此鸣暗清飞。<br>广松春人草，为啸李田。。思君鸾可得，萧论以天风。<br>今家高去路，酌藏无云功。为时壮罢邑，陈不见长名。<br>闻啸不可在，玉服汉寒情。愿昆隐山水，更将谢敬离。</p>
</blockquote>
<blockquote>
<p>大说词隐开，夜子庐鹤行。他烛不知尽，逸君徒风草。<br>把柯南幽山，超血彩森兵。吊窈赤微鸟，娇若相踪。<br>笑乏惜香门，夜酌闻岩欢。<br>我此一可驰，三刀瓦风樽。东秋清柱晚，意歌送高颜。<br>别留一失在，推时悲紫踪。群远亭我顶，机赠沾酒旋。<br>明镜若相巨，明窗忘语平。桃水凌瑶心，茫讼落清眉。<br>横产无商娥，万流应长生。</p>
</blockquote>
<blockquote>
<p>  卷171_2 【酬纪寿阳送官】李白</p>
</blockquote>
<blockquote>
<p>白浦欲溪山，去入北酒人。云水薄阶弄，乃啾迹风声。<br>诗命侍飞儿，百结清霞杯。何言思君去，但然谢无歇。</p>
</blockquote>
<blockquote>
<p>  卷176_21 【荆闺崔嵩人宰】李白</p>
</blockquote>
<blockquote>
<p>常鹉别帝家，绮书逐惟安。西世东山玉，雕是金东辉。<br>绿登紫鹿色，张看弄月萝。思手穷津水，弄是俨归人。<br>相恐新鹉道，渌声赠恨公。</p>
</blockquote>
<p>这里出来的效果就很惊人了，我们从小就在课本上学习了诗仙李白的许多佳作，可以说大家对于一个诗人的诗的韵味是怎样是很有体会的，在这些字里行间仔细品味，我们完全可以体会到李太白的豪放与洒脱。</p>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://twitter.com/frankchen0130">
            <span class="icon">
              <i class="fa fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://t.me/frankchen0130">
            <span class="icon">
              <i class="fa fa-telegram"></i>
            </span>

            <span class="label">Telegram</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/DNN/" rel="tag"># DNN</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2016/05/25/2016-05-25-pythonoop/" rel="prev" title="Python OOP（Object Oriented Programming）">
      <i class="fa fa-chevron-left"></i> Python OOP（Object Oriented Programming）
    </a></div>
      <div class="post-nav-item">
    <a href="/2016/06/04/2016-06-04-note-for-cs224d-1/" rel="next" title="note for CS224d:1">
      note for CS224d:1 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC8yODQ5NS81MDY2"></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#karpathy%E7%9A%84example%E5%AE%9E%E9%AA%8C-cpu%E7%89%88%E6%9C%AC"><span class="nav-number">1.</span> <span class="nav-text">karpathy的example实验-cpu版本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#karpathy%E7%9A%84example%E5%AE%9E%E9%AA%8C-gpu%E7%89%88%E6%9C%AC"><span class="nav-number">2.</span> <span class="nav-text">karpathy的example实验-gpu版本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%8E%B0%E5%8A%9B"><span class="nav-number">3.</span> <span class="nav-text">对比训练过程模型表现力</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E4%B8%8B%E6%9D%A5%E7%9A%84%E5%85%A8%E5%94%90%E8%AF%97-txt%E6%98%AFgbk%E7%BC%96%E7%A0%81%E7%9A%84%EF%BC%8C%E9%A6%96%E5%85%88%E9%9C%80%E8%A6%81%E8%BD%AC%E6%8D%A2%E4%B8%BAutf-8%E7%BC%96%E7%A0%81%EF%BC%9A"><span class="nav-number">3.1.</span> <span class="nav-text">下载下来的全唐诗.txt是gbk编码的，首先需要转换为utf-8编码：</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">frankchen0130</p>
  <div class="site-description" itemprop="description">Marketing/Python/Clojure/Game</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">106</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/frankchen0130" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;frankchen0130" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:znwindy@gmail.com" title="E-Mail → mailto:znwindy@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/znwindy" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;znwindy" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/frankchen0130" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;frankchen0130" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/5985526/frankchen0130" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;5985526&#x2F;frankchen0130" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i>StackOverflow</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://my.csdn.net/yywan1314520" title="http:&#x2F;&#x2F;my.csdn.net&#x2F;yywan1314520" rel="noopener" target="_blank">-dragon-</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://cuiqingcai.com/" title="http:&#x2F;&#x2F;cuiqingcai.com&#x2F;" rel="noopener" target="_blank">静觅</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://hubojing.github.io/" title="https:&#x2F;&#x2F;hubojing.github.io&#x2F;" rel="noopener" target="_blank">胡博靖</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://darrenliuwei.com/" title="https:&#x2F;&#x2F;darrenliuwei.com&#x2F;" rel="noopener" target="_blank">刘伟</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2015 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">frankchen0130</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
