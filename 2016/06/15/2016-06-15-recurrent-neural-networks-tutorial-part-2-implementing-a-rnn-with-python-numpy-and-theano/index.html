<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"frankchen.xyz","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本文将用Python实现完整的RNN，并且用Theano来优化。 语言模型我们的目标是使用RNN建立一个语言模型。以下我们举例说明什么是语言模型。例如，你说了一句包括$$m$$个词语的句子，语言模型可以为这句话出现的概率打分： $$P(w_1,\cdots,w_m) &#x3D; \prod_{i&#x3D;1}^m P(w_i \mid w_1,\cdots,w_{i-1})$$">
<meta property="og:type" content="article">
<meta property="og:title" content="RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO">
<meta property="og:url" content="https://frankchen.xyz/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/index.html">
<meta property="og:site_name" content="不正经的社长">
<meta property="og:description" content="本文将用Python实现完整的RNN，并且用Theano来优化。 语言模型我们的目标是使用RNN建立一个语言模型。以下我们举例说明什么是语言模型。例如，你说了一句包括$$m$$个词语的句子，语言模型可以为这句话出现的概率打分： $$P(w_1,\cdots,w_m) &#x3D; \prod_{i&#x3D;1}^m P(w_i \mid w_1,\cdots,w_{i-1})$$">
<meta property="og:locale">
<meta property="og:image" content="https://frankchen.xyz/images/2016/06/rnn.jpg">
<meta property="article:published_time" content="2016-06-15T09:54:18.000Z">
<meta property="article:modified_time" content="2017-01-13T09:47:18.000Z">
<meta property="article:author" content="frankchen0130">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="RNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://frankchen.xyz/images/2016/06/rnn.jpg">

<link rel="canonical" href="https://frankchen.xyz/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO | 不正经的社长</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="不正经的社长" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">不正经的社长</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Affiliate Marketing/Python/Clojure/Game/Resource</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://frankchen.xyz/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="frankchen0130">
      <meta itemprop="description" content="Affiliate Marketing/Python/Clojure/Game/Resource">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不正经的社长">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-06-15 17:54:18" itemprop="dateCreated datePublished" datetime="2016-06-15T17:54:18+08:00">2016-06-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2017-01-13 17:47:18" itemprop="dateModified" datetime="2017-01-13T17:47:18+08:00">2017-01-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tutorial/" itemprop="url" rel="index"><span itemprop="name">tutorial</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文将用Python实现完整的RNN，并且用Theano来优化。</p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>我们的目标是使用RNN建立一个语言模型。以下我们举例说明什么是语言模型。例如，你说了一句包括$$m$$个词语的句子，语言模型可以为这句话出现的概率打分：</p>
<p>$$P(w_1,\cdots,w_m) = \prod_{i=1}^m P(w_i \mid w_1,\cdots,w_{i-1})$$</p>
<span id="more"></span>

<p> 每一个词语的概率都取决于它之前的所有的词的概率。</p>
<p>这样的模型有什么用处呢？</p>
<ul>
<li>可以用于机器翻译或者语音识别中的正确句子打分</li>
<li>以概率生成新的句子</li>
</ul>
<p>注意到在上面的公式内，我们使用了所有的之前的词的概率，实际上这在计算和存储时的耗费都是巨大的，通常而言只会取2~4个词左右。</p>
<h2 id="预处理并训练数据"><a href="#预处理并训练数据" class="headerlink" title="预处理并训练数据"></a>预处理并训练数据</h2><h3 id="1-标记化"><a href="#1-标记化" class="headerlink" title="1.标记化"></a>1.标记化</h3><p>原始的文本需要被标记化，例如需要把文本标记为句子，句子标记为词语，并且还需要处理标点符号。我们将使用NLTK的<code>word_tokenize\sent_tokenize</code>方法。</p>
<h3 id="2-移除低频词"><a href="#2-移除低频词" class="headerlink" title="2.移除低频词"></a>2.移除低频词</h3><p>移除低频词不管是对于训练和预测都是有帮助的。这里我们设置一个上限<code>vocabulary_size</code>为8000，出现次数少于它的词都会被替换为<code>UNKNOWN_TOKEN</code>输入，而当输出是<code>UNKNOWN_TOKEN</code>时，它将被随机替换为一个不在词表内的词，亦或者持续预测直到不出现<code>UNKNOWN_TOKEN</code>为止。</p>
<h3 id="3-放置句子开始和结束标记"><a href="#3-放置句子开始和结束标记" class="headerlink" title="3.放置句子开始和结束标记"></a>3.放置句子开始和结束标记</h3><p>为了解句子的开始和结束，我们把<code>SENTENCE_START</code>放置在句子开头，并且把<code>SENTENCE_END</code>放置在句子结尾。</p>
<h3 id="4-建立训练数据的矩阵"><a href="#4-建立训练数据的矩阵" class="headerlink" title="4.建立训练数据的矩阵"></a>4.建立训练数据的矩阵</h3><p>RNN的输入和输出都是向量而不是字符串，我们需要把词与向量一一对应，通过<code>index_to_word</code>和<code>word_to_index</code>。比如一个训练的例子$$x$$为[0, 179, 341, 416]（注意到其中每个元素都是长度为<code>vocabulary_size</code>的one-hot向量，所以$$x$$实际上是一个矩阵），那么其label-$$y$$为[179, 341, 416, 1]，注意到我们的目标是预测下一个词，所以$$y$$就是$$x$$移动一位，并添加上最后的一个元素（预测词）的结果，其中<code>SENTENCE_START</code>和<code>SENTENCE_END</code>分别为0和1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">vocabulary_size = <span class="number">8000</span></span><br><span class="line">unknown_token = <span class="string">&quot;UNKNOWN_TOKEN&quot;</span></span><br><span class="line">sentence_start_token = <span class="string">&quot;SENTENCE_START&quot;</span></span><br><span class="line">sentence_end_token = <span class="string">&quot;SENTENCE_END&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the data and append SENTENCE_START and SENTENCE_END tokens</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;Reading CSV file...&quot;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/reddit-comments-2015-08.csv&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    reader = csv.reader(f, skipinitialspace=<span class="literal">True</span>)</span><br><span class="line">    reader.<span class="built_in">next</span>()</span><br><span class="line">    <span class="comment"># Split full comments into sentences</span></span><br><span class="line">    sentences = itertools.chain(*[nltk.sent_tokenize(x[<span class="number">0</span>].decode(<span class="string">&#x27;utf-8&#x27;</span>).lower()) <span class="keyword">for</span> x <span class="keyword">in</span> reader])</span><br><span class="line">    <span class="comment"># Append SENTENCE_START and SENTENCE_END</span></span><br><span class="line">    sentences = [<span class="string">&quot;%s %s %s&quot;</span> % (sentence_start_token, x, sentence_end_token) <span class="keyword">for</span> x <span class="keyword">in</span> sentences]</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;Parsed %d sentences.&quot;</span> % (<span class="built_in">len</span>(sentences))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenize the sentences into words</span></span><br><span class="line">tokenized_sentences = [nltk.word_tokenize(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> sentences]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Count the word frequencies</span></span><br><span class="line">word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;Found %d unique words tokens.&quot;</span> % <span class="built_in">len</span>(word`_freq.items())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the most common words and build index_to_word and word_to_index vectors</span></span><br><span class="line">vocab = word_freq.most_common(vocabulary_size-<span class="number">1</span>)</span><br><span class="line">index_to_word = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> vocab]</span><br><span class="line">index_to_word.append(unknown_token)</span><br><span class="line">word_to_index = <span class="built_in">dict</span>([(w,i) <span class="keyword">for</span> i,w <span class="keyword">in</span> <span class="built_in">enumerate</span>(index_to_word)])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;Using vocabulary size %d.&quot;</span> % vocabulary_size</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;The least frequent word in our vocabulary is &#x27;%s&#x27; and appeared %d times.&quot;</span> % (vocab[-<span class="number">1</span>][<span class="number">0</span>], vocab[-<span class="number">1</span>][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace all words not in our vocabulary with the unknown token</span></span><br><span class="line"><span class="keyword">for</span> i, sent <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokenized_sentences):</span><br><span class="line">    tokenized_sentences[i] = [w <span class="keyword">if</span> w <span class="keyword">in</span> word_to_index <span class="keyword">else</span> unknown_token <span class="keyword">for</span> w <span class="keyword">in</span> sent]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;\nExample sentence: &#x27;%s&#x27;&quot;</span> % sentences[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;\nExample sentence after Pre-processing: &#x27;%s&#x27;&quot;</span> % tokenized_sentences[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the training data</span></span><br><span class="line">X_train = np.asarray([[word_to_index[w] <span class="keyword">for</span> w <span class="keyword">in</span> sent[:-<span class="number">1</span>]] <span class="keyword">for</span> sent <span class="keyword">in</span> tokenized_sentences])</span><br><span class="line">y_train = np.asarray([[word_to_index[w] <span class="keyword">for</span> w <span class="keyword">in</span> sent[<span class="number">1</span>:]] <span class="keyword">for</span> sent <span class="keyword">in</span> tokenized_sentences])</span><br></pre></td></tr></table></figure>
<p>以下是一个训练样本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x:</span><br><span class="line">SENTENCE_START what are n<span class="string">&#x27;t you understanding about this ? !</span></span><br><span class="line"><span class="string">[0, 51, 27, 16, 10, 856, 53, 25, 34, 69]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">y:</span></span><br><span class="line"><span class="string">what are n&#x27;</span>t you understanding about this ? ! SENTENCE_END</span><br><span class="line">[<span class="number">51</span>, <span class="number">27</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">856</span>, <span class="number">53</span>, <span class="number">25</span>, <span class="number">34</span>, <span class="number">69</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>接下来我们开始建立RNN。</p>
<p>##建立RNN</p>
<p><img src="/images/2016/06/rnn.jpg"></p>
<p>总结一下我们的RNN模型的形式。初始输入$$x$$是一个代表一条句子的矩阵，每一时刻的输入$$x_t$$是一个代表一个词语的向量，每一时刻的输出$$o_t$$也是一个向量，其中每个元素代表词表内每一个词被预测的概率。</p>
<p>RNN中的等式：<br>$$<br>s_t = tanh(Ux_t + Ws_{t-1})<br>o_t = softmax(Vs_t)<br>$$</p>
<p>介绍一下各个变量的维度：假设我们的词表大小$$C$$为8000，隐藏层大小$$H$$为100，可以把隐藏层大小理解为网络内存的大小，内存越大，网络能记忆的信息也越多，但是也要耗费更多的代价来计算。综上，我们的模型参数维度为：</p>
<p>$$<br>x_t \in \Bbb{R}^{8000} \ o_t \in \Bbb{R}^{8000} \ s_t \in \Bbb{R}^{100} \ U_t \in \Bbb{R}^{100 \times 8000} \ V_t \in \Bbb{R}^{8000 \times 100} \ W_t \in \Bbb{R}^{100 \times 100} <br>$$</p>
<p>其中$$U，V，W$$是网络的参数，根据上面的等式，我们需要学习$$2HC+H^2$$个参数，由于$$x_t$$是稀疏的one-hot向量，所以其与$$U$$的乘积一步即可算出，$$W$$和$$S_t$$的维度都比较小，所以最繁琐的计算就是$$VS_t$$了。</p>
<p>###初始化</p>
<p>通过Numpy实现第一个版本，对$$U，V，W$$的初始化比较tricky，通常是把它们的初始值置于$$[-\frac{1}{\sqrt n},\frac{1}{\sqrt n}]$$较好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNNumpy</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, word_dim, hidden_dim=<span class="number">100</span>, bptt_truncate=<span class="number">4</span></span>):</span></span><br><span class="line">        <span class="comment"># Assign instance variables</span></span><br><span class="line">        self.word_dim = word_dim</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.bptt_truncate = bptt_truncate</span><br><span class="line">        <span class="comment"># Randomly initialize the network parameters</span></span><br><span class="line">        self.U = np.random.uniform(-np.sqrt(<span class="number">1.</span>/word_dim), np.sqrt(<span class="number">1.</span>/word_dim), (hidden_dim, word_dim))</span><br><span class="line">        self.V = np.random.uniform(-np.sqrt(<span class="number">1.</span>/hidden_dim), np.sqrt(<span class="number">1.</span>/hidden_dim), (word_dim, hidden_dim))</span><br><span class="line">        self.W = np.random.uniform(-np.sqrt(<span class="number">1.</span>/hidden_dim), np.sqrt(<span class="number">1.</span>/hidden_dim), (hidden_dim, hidden_dim))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其中<code>word_dim</code>是词表大小，<code>hidden_dim</code>是隐藏层大小。</p>
<p>###前向计算</p>
<p>以下是前向计算（预测词语的概率）的过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="comment"># The total number of time steps</span></span><br><span class="line">    T = <span class="built_in">len</span>(x)</span><br><span class="line">    <span class="comment"># During forward propagation we save all hidden states in s because need them later.</span></span><br><span class="line">    <span class="comment"># We add one additional element for the initial hidden, which we set to 0</span></span><br><span class="line">    s = np.zeros((T + <span class="number">1</span>, self.hidden_dim))</span><br><span class="line">    s[-<span class="number">1</span>] = np.zeros(self.hidden_dim)</span><br><span class="line">    <span class="comment"># The outputs at each time step. Again, we save them for later.</span></span><br><span class="line">    o = np.zeros((T, self.word_dim))</span><br><span class="line">    <span class="comment"># For each time step...</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(T):</span><br><span class="line">        <span class="comment"># Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.</span></span><br><span class="line">        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-<span class="number">1</span>]))</span><br><span class="line">        o[t] = softmax(self.V.dot(s[t]))</span><br><span class="line">    <span class="keyword">return</span> [o, s]</span><br><span class="line"></span><br><span class="line">RNNNumpy.forward_propagation = forward_propagation</span><br></pre></td></tr></table></figure>
<p>我们同时返回输出及隐藏层状态，隐藏层状态之后会用于梯度计算。$$o_t$$是词表内每个词的概率，但是有时我们需要直接预测出概率最高的词：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="comment"># Perform forward propagation and return index of the highest score</span></span><br><span class="line">    o, s = self.forward_propagation(x)</span><br><span class="line">    <span class="keyword">return</span> np.argmax(o, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">RNNNumpy.predict = predict</span><br></pre></td></tr></table></figure>
<p>尝试输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line">model = RNNNumpy(vocabulary_size)</span><br><span class="line">o, s = model.forward_propagation(X_train[<span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span> o.shape</span><br><span class="line"><span class="built_in">print</span> o</span><br><span class="line">(<span class="number">45</span>, <span class="number">8000</span>)</span><br><span class="line">[[ <span class="number">0.00012408</span>  <span class="number">0.0001244</span>   <span class="number">0.00012603</span> ...,  <span class="number">0.00012515</span>  <span class="number">0.00012488</span></span><br><span class="line">   <span class="number">0.00012508</span>]</span><br><span class="line"> [ <span class="number">0.00012536</span>  <span class="number">0.00012582</span>  <span class="number">0.00012436</span> ...,  <span class="number">0.00012482</span>  <span class="number">0.00012456</span></span><br><span class="line">   <span class="number">0.00012451</span>]</span><br><span class="line"> [ <span class="number">0.00012387</span>  <span class="number">0.0001252</span>   <span class="number">0.00012474</span> ...,  <span class="number">0.00012559</span>  <span class="number">0.00012588</span></span><br><span class="line">   <span class="number">0.00012551</span>]</span><br><span class="line"> ...,</span><br><span class="line"> [ <span class="number">0.00012414</span>  <span class="number">0.00012455</span>  <span class="number">0.0001252</span>  ...,  <span class="number">0.00012487</span>  <span class="number">0.00012494</span></span><br><span class="line">   <span class="number">0.0001263</span> ]</span><br><span class="line"> [ <span class="number">0.0001252</span>   <span class="number">0.00012393</span>  <span class="number">0.00012509</span> ...,  <span class="number">0.00012407</span>  <span class="number">0.00012578</span></span><br><span class="line">   <span class="number">0.00012502</span>]</span><br><span class="line"> [ <span class="number">0.00012472</span>  <span class="number">0.0001253</span>   <span class="number">0.00012487</span> ...,  <span class="number">0.00012463</span>  <span class="number">0.00012536</span></span><br><span class="line">   <span class="number">0.00012665</span>]]</span><br></pre></td></tr></table></figure>
<p>对上面句子（包括45个单词）中的每个词，模型都预测了8000个概率值。因为模型参数这时候是随机初始值，所以预测也是随机的。接下来我们给出预测的词的位置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">predictions = model.predict(X_train[<span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span> predictions.shape</span><br><span class="line"><span class="built_in">print</span> predictions</span><br><span class="line">(<span class="number">45</span>,)</span><br><span class="line">[<span class="number">1284</span> <span class="number">5221</span> <span class="number">7653</span> <span class="number">7430</span> <span class="number">1013</span> <span class="number">3562</span> <span class="number">7366</span> <span class="number">4860</span> <span class="number">2212</span> <span class="number">6601</span> <span class="number">7299</span> <span class="number">4556</span> <span class="number">2481</span> <span class="number">238</span> <span class="number">2539</span></span><br><span class="line"> <span class="number">21</span> <span class="number">6548</span> <span class="number">261</span> <span class="number">1780</span> <span class="number">2005</span> <span class="number">1810</span> <span class="number">5376</span> <span class="number">4146</span> <span class="number">477</span> <span class="number">7051</span> <span class="number">4832</span> <span class="number">4991</span> <span class="number">897</span> <span class="number">3485</span> <span class="number">21</span></span><br><span class="line"> <span class="number">7291</span> <span class="number">2007</span> <span class="number">6006</span> <span class="number">760</span> <span class="number">4864</span> <span class="number">2182</span> <span class="number">6569</span> <span class="number">2800</span> <span class="number">2752</span> <span class="number">6821</span> <span class="number">4437</span> <span class="number">7021</span> <span class="number">7875</span> <span class="number">6912</span> <span class="number">3575</span>]</span><br></pre></td></tr></table></figure>
<p>接下来我们计算损失。</p>
<p>###计算损失</p>
<p>我们使用交叉熵函数作为损失函数。若我们有$$N$$个训练样本（text中的词语数），$$C$$个类别（词表大小），预测是$$o$$，label是$$y$$，那么损失计算为：</p>
<p>$$<br>L(y,o) = - \frac{1}{N} \sum_{n \in N} y_n \log o_n<br>$$</p>
<p>损失函数计算的是我们的预测oo与正确的词yy的差距的大小。通过以下计算损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_total_loss</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">    L = <span class="number">0</span></span><br><span class="line">    <span class="comment"># For each sentence...</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="built_in">len</span>(y)):</span><br><span class="line">        o, s = self.forward_propagation(x[i])</span><br><span class="line">        <span class="comment"># We only care about our prediction of the &quot;correct&quot; words</span></span><br><span class="line">        correct_word_predictions = o[np.arange(<span class="built_in">len</span>(y[i])), y[i]]</span><br><span class="line">        <span class="comment"># Add to the loss based on how off we were</span></span><br><span class="line">        L += -<span class="number">1</span> * np.<span class="built_in">sum</span>(np.log(correct_word_predictions))</span><br><span class="line">    <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_loss</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">    <span class="comment"># Divide the total loss by the number of training examples</span></span><br><span class="line">    N = np.<span class="built_in">sum</span>((<span class="built_in">len</span>(y_i) <span class="keyword">for</span> y_i <span class="keyword">in</span> y))</span><br><span class="line">    <span class="keyword">return</span> self.calculate_total_loss(x,y)/N</span><br><span class="line"></span><br><span class="line">RNNNumpy.calculate_total_loss = calculate_total_loss</span><br><span class="line">RNNNumpy.calculate_loss = calculate_loss</span><br></pre></td></tr></table></figure>
<p>让我们稍微检验一下，如果我们词表大小为$$C$$，那么开始时每个词被随机预测的概率为$$\frac{1}{C}$$，那么损失为$$L = - \frac{1}{C} N \log \ \frac{1}{C} = \log C$$：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> Limit to <span class="number">1000</span> examples to save time</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;Expected Loss for random predictions: %f&quot;</span> % np.log(vocabulary_size)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;Actual loss: %f&quot;</span> % model.calculate_loss(X_train[:<span class="number">1000</span>], y_train[:<span class="number">1000</span>])</span><br></pre></td></tr></table></figure>
<p>很接近。这里我们应当记住，如果data很大，那么计算损失会变得非常耗费时间。</p>
<p>###通过SGD和BPTT（BACKPROPAGATION THROUGH TIME ）训练RNN</p>
<p>给定训练样本$$(x,y)$$我们需要计算梯度$$\frac{\partial L}{\partial {U}},\frac{\partial L}{\partial {V}},\frac{\partial L}{\partial {W}}$$。通过以下代码实现BPTT：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bptt</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">    T = <span class="built_in">len</span>(y)</span><br><span class="line">    <span class="comment"># Perform forward propagation</span></span><br><span class="line">    o, s = self.forward_propagation(x)</span><br><span class="line">    <span class="comment"># We accumulate the gradients in these variables</span></span><br><span class="line">    dLdU = np.zeros(self.U.shape)</span><br><span class="line">    dLdV = np.zeros(self.V.shape)</span><br><span class="line">    dLdW = np.zeros(self.W.shape)</span><br><span class="line">    delta_o = o</span><br><span class="line">    delta_o[np.arange(<span class="built_in">len</span>(y)), y] -= <span class="number">1.</span></span><br><span class="line">    <span class="comment"># For each output backwards...</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(T)[::-<span class="number">1</span>]:</span><br><span class="line">        dLdV += np.outer(delta_o[t], s[t].T)</span><br><span class="line">        <span class="comment"># Initial delta calculation</span></span><br><span class="line">        delta_t = self.V.T.dot(delta_o[t]) * (<span class="number">1</span> - (s[t] ** <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># Backpropagation through time (for at most self.bptt_truncate steps)</span></span><br><span class="line">        <span class="keyword">for</span> bptt_step <span class="keyword">in</span> np.arange(<span class="built_in">max</span>(<span class="number">0</span>, t-self.bptt_truncate), t+<span class="number">1</span>)[::-<span class="number">1</span>]:</span><br><span class="line">            <span class="comment"># print &quot;Backpropagation step t=%d bptt step=%d &quot; % (t, bptt_step)</span></span><br><span class="line">            dLdW += np.outer(delta_t, s[bptt_step-<span class="number">1</span>])</span><br><span class="line">            dLdU[:,x[bptt_step]] += delta_t</span><br><span class="line">            <span class="comment"># Update delta for next step</span></span><br><span class="line">            delta_t = self.W.T.dot(delta_t) * (<span class="number">1</span> - s[bptt_step-<span class="number">1</span>] ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> [dLdU, dLdV, dLdW]</span><br><span class="line"></span><br><span class="line">RNNNumpy.bptt = bptt</span><br></pre></td></tr></table></figure>
<p>接下来我们测试梯度。</p>
<p>###测试梯度</p>
<p>实现BP算法过程中，测试梯度是一个良好的习惯。通过以下公式：</p>
<p>$$<br>\frac{\partial L}{\partial{\theta}} \approx \lim_{h \rightarrow0} \frac{J(\theta + h)-J(\theta - h)}{2h}<br>$$</p>
<p>使用以上的公式来测试梯度，同样，由于需要计算所以的参数，梯度测试也是很耗时间的，在部分数据上测试是比较好的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span>(<span class="params">self, x, y, h=<span class="number">0.001</span>, error_threshold=<span class="number">0.01</span></span>):</span></span><br><span class="line">    <span class="comment"># Calculate the gradients using backpropagation. We want to checker if these are correct.</span></span><br><span class="line">    bptt_gradients = self.bptt(x, y)</span><br><span class="line">    <span class="comment"># List of all parameters we want to check.</span></span><br><span class="line">    model_parameters = [<span class="string">&#x27;U&#x27;</span>, <span class="string">&#x27;V&#x27;</span>, <span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">    <span class="comment"># Gradient check for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> pidx, pname <span class="keyword">in</span> <span class="built_in">enumerate</span>(model_parameters):</span><br><span class="line">        <span class="comment"># Get the actual parameter value from the mode, e.g. model.W</span></span><br><span class="line">        parameter = operator.attrgetter(pname)(self)</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;Performing gradient check for parameter %s with size %d.&quot;</span> % (pname, np.prod(parameter.shape))</span><br><span class="line">        <span class="comment"># Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...</span></span><br><span class="line">        it = np.nditer(parameter, flags=[<span class="string">&#x27;multi_index&#x27;</span>], op_flags=[<span class="string">&#x27;readwrite&#x27;</span>])</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line">            ix = it.multi_index</span><br><span class="line">            <span class="comment"># Save the original value so we can reset it later</span></span><br><span class="line">            original_value = parameter[ix]</span><br><span class="line">            <span class="comment"># Estimate the gradient using (f(x+h) - f(x-h))/(2*h)</span></span><br><span class="line">            parameter[ix] = original_value + h</span><br><span class="line">            gradplus = self.calculate_total_loss([x],[y])</span><br><span class="line">            parameter[ix] = original_value - h</span><br><span class="line">            gradminus = self.calculate_total_loss([x],[y])</span><br><span class="line">            estimated_gradient = (gradplus - gradminus)/(<span class="number">2</span>*h)</span><br><span class="line">            <span class="comment"># Reset parameter to original value</span></span><br><span class="line">            parameter[ix] = original_value</span><br><span class="line">            <span class="comment"># The gradient for this parameter calculated using backpropagation</span></span><br><span class="line">            backprop_gradient = bptt_gradients[pidx][ix]</span><br><span class="line">            <span class="comment"># calculate The relative error: (|x - y|/(|x| + |y|))</span></span><br><span class="line">            relative_error = np.<span class="built_in">abs</span>(backprop_gradient - estimated_gradient)/(np.<span class="built_in">abs</span>(backprop_gradient) + np.<span class="built_in">abs</span>(estimated_gradient))</span><br><span class="line">            <span class="comment"># If the error is to large fail the gradient check</span></span><br><span class="line">            <span class="keyword">if</span> relative_error &gt; error_threshold:</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Gradient Check ERROR: parameter=%s ix=%s&quot;</span> % (pname, ix)</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;+h Loss: %f&quot;</span> % gradplus</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;-h Loss: %f&quot;</span> % gradminus</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Estimated_gradient: %f&quot;</span> % estimated_gradient</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Backpropagation gradient: %f&quot;</span> % backprop_gradient</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Relative Error: %f&quot;</span> % relative_error</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            it.iternext()</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;Gradient check for parameter %s passed.&quot;</span> % (pname)</span><br><span class="line"></span><br><span class="line">RNNNumpy.gradient_check = gradient_check</span><br><span class="line"></span><br><span class="line"><span class="comment"># To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.</span></span><br><span class="line">grad_check_vocab_size = <span class="number">100</span></span><br><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line">model = RNNNumpy(grad_check_vocab_size, <span class="number">10</span>, bptt_truncate=<span class="number">1000</span>)</span><br><span class="line">model.gradient_check([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<p>接下来我们实现SGD。</p>
<p>###实现SGD</p>
<p>通过两步实现：</p>
<ul>
<li><code>sdg_step</code>计算计算梯度对每个batch更新</li>
<li>外部循环迭代整个训练数据并且调整学习率</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numpy_sdg_step</span>(<span class="params">self, x, y, learning_rate</span>):</span></span><br><span class="line">    <span class="comment"># Calculate the gradients</span></span><br><span class="line">    dLdU, dLdV, dLdW = self.bptt(x, y)</span><br><span class="line">    <span class="comment"># Change parameters according to gradients and learning rate</span></span><br><span class="line">    self.U -= learning_rate * dLdU</span><br><span class="line">    self.V -= learning_rate * dLdV</span><br><span class="line">    self.W -= learning_rate * dLdW</span><br><span class="line"></span><br><span class="line">RNNNumpy.sgd_step = numpy_sdg_step</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Outer SGD Loop</span><br><span class="line"><span class="comment"># - model: The RNN model instance</span></span><br><span class="line"><span class="comment"># - X_train: The training data set</span></span><br><span class="line"><span class="comment"># - y_train: The training data labels</span></span><br><span class="line"><span class="comment"># - learning_rate: Initial learning rate for SGD</span></span><br><span class="line"><span class="comment"># - nepoch: Number of times to iterate through the complete dataset</span></span><br><span class="line"><span class="comment"># - evaluate_loss_after: Evaluate the loss after this many epochs</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_with_sgd</span>(<span class="params">model, X_train, y_train, learning_rate=<span class="number">0.005</span>, nepoch=<span class="number">100</span>, evaluate_loss_after=<span class="number">5</span></span>):</span></span><br><span class="line">    <span class="comment"># We keep track of the losses so we can plot them later</span></span><br><span class="line">    losses = []</span><br><span class="line">    num_examples_seen = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(nepoch):</span><br><span class="line">        <span class="comment"># Optionally evaluate the loss</span></span><br><span class="line">        <span class="keyword">if</span> (epoch % evaluate_loss_after == <span class="number">0</span>):</span><br><span class="line">            loss = model.calculate_loss(X_train, y_train)</span><br><span class="line">            losses.append((num_examples_seen, loss))</span><br><span class="line">            time = datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;%s: Loss after num_examples_seen=%d epoch=%d: %f&quot;</span> % (time, num_examples_seen, epoch, loss)</span><br><span class="line">            <span class="comment"># Adjust the learning rate if loss increases</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">len</span>(losses) &gt; <span class="number">1</span> <span class="keyword">and</span> losses[-<span class="number">1</span>][<span class="number">1</span>] &gt; losses[-<span class="number">2</span>][<span class="number">1</span>]):</span><br><span class="line">                learning_rate = learning_rate * <span class="number">0.5</span></span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Setting learning rate to %f&quot;</span> % learning_rate</span><br><span class="line">            sys.stdout.flush()</span><br><span class="line">        <span class="comment"># For each training example...</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y_train)):</span><br><span class="line">            <span class="comment"># One SGD step</span></span><br><span class="line">            model.sgd_step(X_train[i], y_train[i], learning_rate)</span><br><span class="line">            num_examples_seen += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>完成。我们来测试一下训练耗费多长的时间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line">model = RNNNumpy(vocabulary_size)</span><br><span class="line">%timeit model.sgd_step(X_train[<span class="number">10</span>], y_train[<span class="number">10</span>], <span class="number">0.005</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到在我的机器上需要SGD的每一步需要180ms，这代表整个训练将耗费数天甚至更多。我们可以通过许多的方法来加速这一过程，如改善代码和调整模型。这里我们希望使用GPU来加速。在这之前，我们先测试一下SGD的效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># Train on a small subset of the data to see what happens</span></span><br><span class="line">model = RNNNumpy(vocabulary_size)</span><br><span class="line">losses = train_with_sgd(model, X_train[:<span class="number">100</span>], y_train[:<span class="number">100</span>], nepoch=<span class="number">10</span>, evaluate_loss_after=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2016</span>-06-<span class="number">13</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">46</span>: Loss after num_examples_seen=<span class="number">0</span> epoch=<span class="number">0</span>: <span class="number">8.987425</span></span><br><span class="line"><span class="number">2016</span>-06-<span class="number">13</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">56</span>: Loss after num_examples_seen=<span class="number">100</span> epoch=<span class="number">1</span>: <span class="number">8.976270</span></span><br><span class="line"><span class="number">2016</span>-06-<span class="number">13</span> <span class="number">17</span>:<span class="number">00</span>:06: Loss after num_examples_seen=<span class="number">200</span> epoch=<span class="number">2</span>: <span class="number">8.960212</span></span><br><span class="line"><span class="number">2016</span>-06-<span class="number">13</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">16</span>: Loss after num_examples_seen=<span class="number">300</span> epoch=<span class="number">3</span>: <span class="number">8.930430</span></span><br><span class="line"><span class="number">2016</span>-06-<span class="number">13</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">26</span>: Loss after num_examples_seen=<span class="number">400</span> epoch=<span class="number">4</span>: <span class="number">8.862264</span></span><br><span class="line"><span class="number">2016</span>-06-<span class="number">13</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">37</span>: Loss after num_examples_seen=<span class="number">500</span> epoch=<span class="number">5</span>: <span class="number">6.913570</span></span><br><span class="line"><span class="number">2016</span>-06-<span class="number">13</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">46</span>: Loss after num_examples_seen=<span class="number">600</span> epoch=<span class="number">6</span>: <span class="number">6.302493</span></span><br><span class="line"><span class="number">2016</span>-06-<span class="number">13</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">56</span>: Loss after num_examples_seen=<span class="number">700</span> epoch=<span class="number">7</span>: <span class="number">6.014995</span></span><br><span class="line"><span class="number">2016</span>-06-<span class="number">13</span> <span class="number">17</span>:01:06: Loss after num_examples_seen=<span class="number">800</span> epoch=<span class="number">8</span>: <span class="number">5.833877</span></span><br><span class="line"><span class="number">2016</span>-06-<span class="number">13</span> <span class="number">17</span>:01:<span class="number">16</span>: Loss after num_examples_seen=<span class="number">900</span> epoch=<span class="number">9</span>: <span class="number">5.710718</span></span><br></pre></td></tr></table></figure>
<p>看起来，SGD起到了效果。</p>
<p>##通过Theano和GPU训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">enp.random.seed(<span class="number">10</span>)</span><br><span class="line">model = RNNTheano(vocabulary_size)</span><br><span class="line">%timeit model.sgd_step(X_train[<span class="number">10</span>], y_train[<span class="number">10</span>], <span class="number">0.005</span>)</span><br></pre></td></tr></table></figure>
<p>这次一次SGD步骤耗费为73.7ms。</p>
<p>这里我们直接使用训练好的的参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_model_parameters_theano, save_model_parameters_theano</span><br><span class="line"></span><br><span class="line">model = RNNTheano(vocabulary_size, hidden_dim=<span class="number">50</span>)</span><br><span class="line"><span class="comment"># losses = train_with_sgd(model, X_train, y_train, nepoch=50)</span></span><br><span class="line"><span class="comment"># save_model_parameters_theano(&#x27;./data/trained-model-theano.npz&#x27;, model)</span></span><br><span class="line">load_model_parameters_theano(<span class="string">&#x27;./data/trained-model-theano.npz&#x27;</span>, model)</span><br></pre></td></tr></table></figure>

<p>###生成语句</p>
<p>使用如下生成语句：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_sentence</span>(<span class="params">model</span>):</span></span><br><span class="line">    <span class="comment"># We start the sentence with the start token</span></span><br><span class="line">    new_sentence = [word_to_index[sentence_start_token]]</span><br><span class="line">    <span class="comment"># Repeat until we get an end token</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> new_sentence[-<span class="number">1</span>] == word_to_index[sentence_end_token]:</span><br><span class="line">        next_word_probs = model.forward_propagation(new_sentence)</span><br><span class="line">        sampled_word = word_to_index[unknown_token]</span><br><span class="line">        <span class="comment"># We don&#x27;t want to sample unknown words</span></span><br><span class="line">        <span class="keyword">while</span> sampled_word == word_to_index[unknown_token]:</span><br><span class="line">            samples = np.random.multinomial(<span class="number">1</span>, next_word_probs[-<span class="number">1</span>])</span><br><span class="line">            sampled_word = np.argmax(samples)</span><br><span class="line">        new_sentence.append(sampled_word)</span><br><span class="line">    sentence_str = [index_to_word[x] <span class="keyword">for</span> x <span class="keyword">in</span> new_sentence[<span class="number">1</span>:-<span class="number">1</span>]]</span><br><span class="line">    <span class="keyword">return</span> sentence_str</span><br><span class="line"></span><br><span class="line">num_sentences = <span class="number">10</span></span><br><span class="line">senten_min_length = <span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_sentences):</span><br><span class="line">    sent = []</span><br><span class="line">    <span class="comment"># We want long sentences, not sentences with one or two words</span></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(sent) &lt; senten_min_length:</span><br><span class="line">        sent = generate_sentence(model)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot; &quot;</span>.join(sent)</span><br></pre></td></tr></table></figure>
<p>得到结果为：</p>
<ul>
<li>no to blame their stuff go at all .</li>
<li>consider via under gear but equal every game .</li>
<li>no similar work on the ui birth a ce nightmare .</li>
<li>the challenging what is absolutely hard .</li>
<li>me do you research getting +2 .</li>
<li>ugh is much good , no .</li>
<li>me so many different lines hair .</li>
<li>probably not very a bot or gain .</li>
<li>correct this is affected so why ?</li>
<li>register but a grown gun environment .</li>
</ul>
<p>看起来还不错！不过还是有一些缺陷，这些都是由于vanilla RNN不能解决长期依赖问题导致的。下一篇我们将讨论BPTT并且关注梯度消失/爆炸问题。</p>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://twitter.com/frankchen0130">
            <span class="icon">
              <i class="fa fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://t.me/frankchen0130">
            <span class="icon">
              <i class="fa fa-telegram"></i>
            </span>

            <span class="label">Telegram</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
              <a href="/tags/RNN/" rel="tag"># RNN</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2016/06/07/2016-06-07-edit-dns-of-ubuntu/" rel="prev" title="ubuntu下修改DNS并且避免重启失效的方法">
      <i class="fa fa-chevron-left"></i> ubuntu下修改DNS并且避免重启失效的方法
    </a></div>
      <div class="post-nav-item">
    <a href="/2016/06/22/2016-06-22-vim-lian-ji-gong-lue-1/" rel="next" title="Vim 练级攻略一">
      Vim 练级攻略一 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC8yODQ5NS81MDY2"></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">语言模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E5%B9%B6%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="nav-number">2.</span> <span class="nav-text">预处理并训练数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%A0%87%E8%AE%B0%E5%8C%96"><span class="nav-number">2.1.</span> <span class="nav-text">1.标记化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%A7%BB%E9%99%A4%E4%BD%8E%E9%A2%91%E8%AF%8D"><span class="nav-number">2.2.</span> <span class="nav-text">2.移除低频词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%94%BE%E7%BD%AE%E5%8F%A5%E5%AD%90%E5%BC%80%E5%A7%8B%E5%92%8C%E7%BB%93%E6%9D%9F%E6%A0%87%E8%AE%B0"><span class="nav-number">2.3.</span> <span class="nav-text">3.放置句子开始和结束标记</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%BB%BA%E7%AB%8B%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E7%9A%84%E7%9F%A9%E9%98%B5"><span class="nav-number">2.4.</span> <span class="nav-text">4.建立训练数据的矩阵</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">frankchen0130</p>
  <div class="site-description" itemprop="description">Affiliate Marketing/Python/Clojure/Game/Resource</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">106</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/frankchen0130" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;frankchen0130" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:znwindy@gmail.com" title="E-Mail → mailto:znwindy@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/znwindy" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;znwindy" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/frankchen0130" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;frankchen0130" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/5985526/frankchen0130" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;5985526&#x2F;frankchen0130" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i>StackOverflow</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://my.csdn.net/yywan1314520" title="http:&#x2F;&#x2F;my.csdn.net&#x2F;yywan1314520" rel="noopener" target="_blank">-dragon-</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://cuiqingcai.com/" title="http:&#x2F;&#x2F;cuiqingcai.com&#x2F;" rel="noopener" target="_blank">静觅</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://hubojing.github.io/" title="https:&#x2F;&#x2F;hubojing.github.io&#x2F;" rel="noopener" target="_blank">胡博靖</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://darrenliuwei.com/" title="https:&#x2F;&#x2F;darrenliuwei.com&#x2F;" rel="noopener" target="_blank">刘伟</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2015 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">frankchen0130</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
