<!DOCTYPE html>
<html >
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="江南消夏" />



<meta name="description" content="本文将用Python实现完整的RNN，并且用Theano来优化。
语言模型我们的目标是使用RNN建立一个语言模型。以下我们举例说明什么是语言模型。例如，你说了一句包括$$m$$个词语的句子，语言模型可以为这句话出现的概率打分：
$$P(w_1,\cdots,wm) = \prod{i=1}^m P(w_i \mid w1,\cdots,w{i-1})$$">
<meta property="og:type" content="article">
<meta property="og:title" content="RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO">
<meta property="og:url" content="http://frankchen.xyz/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/index.html">
<meta property="og:site_name" content="loop in data">
<meta property="og:description" content="本文将用Python实现完整的RNN，并且用Theano来优化。
语言模型我们的目标是使用RNN建立一个语言模型。以下我们举例说明什么是语言模型。例如，你说了一句包括$$m$$个词语的句子，语言模型可以为这句话出现的概率打分：
$$P(w_1,\cdots,wm) = \prod{i=1}^m P(w_i \mid w1,\cdots,w{i-1})$$">
<meta property="og:image" content="http://frankchen.xyz/images/2016/06/rnn.jpg">
<meta property="og:updated_time" content="2017-01-13T09:47:18.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO">
<meta name="twitter:description" content="本文将用Python实现完整的RNN，并且用Theano来优化。
语言模型我们的目标是使用RNN建立一个语言模型。以下我们举例说明什么是语言模型。例如，你说了一句包括$$m$$个词语的句子，语言模型可以为这句话出现的概率打分：
$$P(w_1,\cdots,wm) = \prod{i=1}^m P(w_i \mid w1,\cdots,w{i-1})$$">
<meta name="twitter:image" content="http://frankchen.xyz/images/2016/06/rnn.jpg">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="loop in data" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">



<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO | loop in data</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: undefined
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">江南消夏</a></h1>
        </hgroup>

        
        <p class="header-subtitle">Enjoy everything fun and challenging</p>
        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>Menu</li>
                        <li>Tags</li>
                        
                        <li>Friends</li>
                        
                        
                        <li>About Me</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:947118251@qq.com" title="Email"></a>
                            
                                <a class="fa 新浪微博" href="http://weibo.com/znwindy" title="新浪微博"></a>
                            
                                <a class="fa GitHub" href="https://github.com/frankchen0130" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa 知乎" href="https://www.zhihu.com/people/frankchen0130" title="知乎"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/DS-Store/">.DS_Store</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNN/">DNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LDA/">LDA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mac/">Mac</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine-Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Old-Driver/">Old_Driver</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learning/">deep-learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/life/">life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tips/">tips</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tutorial/">tutorial</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://hujiaweibujidao.github.io/">胡家威</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://my.csdn.net/yywan1314520">-dragon-</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://e1ias.github.io/">E1ias</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://cuiqingcai.com/">静觅</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">机器学习、数据挖掘</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">江南消夏</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">江南消夏</a></h1>
            </hgroup>
            
            <p class="header-subtitle">Enjoy everything fun and challenging</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:947118251@qq.com" title="Email"></a>
                            
                                <a class="fa 新浪微博" target="_blank" href="http://weibo.com/znwindy" title="新浪微博"></a>
                            
                                <a class="fa GitHub" target="_blank" href="https://github.com/frankchen0130" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa 知乎" target="_blank" href="https://www.zhihu.com/people/frankchen0130" title="知乎"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="Tags" friends="Friends" about="About Me"/>
</nav>
      <div class="body-wrap"><article id="post-2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/" class="article-date">
      <time datetime="2016-06-15T09:54:18.000Z" itemprop="datePublished">2016-06-15</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/tutorial/">tutorial</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/">RNN</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>本文将用Python实现完整的RNN，并且用Theano来优化。</p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>我们的目标是使用RNN建立一个语言模型。以下我们举例说明什么是语言模型。例如，你说了一句包括$$m$$个词语的句子，语言模型可以为这句话出现的概率打分：</p>
<p>$$P(w_1,\cdots,w<em>m) = \prod</em>{i=1}^m P(w_i \mid w<em>1,\cdots,w</em>{i-1})$$</p>
<a id="more"></a>
<p> 每一个词语的概率都取决于它之前的所有的词的概率。</p>
<p>这样的模型有什么用处呢？</p>
<ul>
<li>可以用于机器翻译或者语音识别中的正确句子打分</li>
<li>以概率生成新的句子</li>
</ul>
<p>注意到在上面的公式内，我们使用了所有的之前的词的概率，实际上这在计算和存储时的耗费都是巨大的，通常而言只会取2~4个词左右。</p>
<h2 id="预处理并训练数据"><a href="#预处理并训练数据" class="headerlink" title="预处理并训练数据"></a>预处理并训练数据</h2><h3 id="1-标记化"><a href="#1-标记化" class="headerlink" title="1.标记化"></a>1.标记化</h3><p>原始的文本需要被标记化，例如需要把文本标记为句子，句子标记为词语，并且还需要处理标点符号。我们将使用NLTK的<code>word_tokenize\sent_tokenize</code>方法。</p>
<h3 id="2-移除低频词"><a href="#2-移除低频词" class="headerlink" title="2.移除低频词"></a>2.移除低频词</h3><p>移除低频词不管是对于训练和预测都是有帮助的。这里我们设置一个上限<code>vocabulary_size</code>为8000，出现次数少于它的词都会被替换为<code>UNKNOWN_TOKEN</code>输入，而当输出是<code>UNKNOWN_TOKEN</code>时，它将被随机替换为一个不在词表内的词，亦或者持续预测直到不出现<code>UNKNOWN_TOKEN</code>为止。</p>
<h3 id="3-放置句子开始和结束标记"><a href="#3-放置句子开始和结束标记" class="headerlink" title="3.放置句子开始和结束标记"></a>3.放置句子开始和结束标记</h3><p>为了解句子的开始和结束，我们把<code>SENTENCE_START</code>放置在句子开头，并且把<code>SENTENCE_END</code>放置在句子结尾。</p>
<h3 id="4-建立训练数据的矩阵"><a href="#4-建立训练数据的矩阵" class="headerlink" title="4.建立训练数据的矩阵"></a>4.建立训练数据的矩阵</h3><p>RNN的输入和输出都是向量而不是字符串，我们需要把词与向量一一对应，通过<code>index_to_word</code>和<code>word_to_index</code>。比如一个训练的例子$$x$$为[0, 179, 341, 416]（注意到其中每个元素都是长度为<code>vocabulary_size</code>的one-hot向量，所以$$x$$实际上是一个矩阵），那么其label-$$y$$为[179, 341, 416, 1]，注意到我们的目标是预测下一个词，所以$$y$$就是$$x$$移动一位，并添加上最后的一个元素（预测词）的结果，其中<code>SENTENCE_START</code>和<code>SENTENCE_END</code>分别为0和1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">vocabulary_size = <span class="number">8000</span></div><div class="line">unknown_token = <span class="string">"UNKNOWN_TOKEN"</span></div><div class="line">sentence_start_token = <span class="string">"SENTENCE_START"</span></div><div class="line">sentence_end_token = <span class="string">"SENTENCE_END"</span></div><div class="line"></div><div class="line"><span class="comment"># Read the data and append SENTENCE_START and SENTENCE_END tokens</span></div><div class="line"><span class="keyword">print</span> <span class="string">"Reading CSV file..."</span></div><div class="line"><span class="keyword">with</span> open(<span class="string">'data/reddit-comments-2015-08.csv'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</div><div class="line">    reader = csv.reader(f, skipinitialspace=<span class="keyword">True</span>)</div><div class="line">    reader.next()</div><div class="line">    <span class="comment"># Split full comments into sentences</span></div><div class="line">    sentences = itertools.chain(*[nltk.sent_tokenize(x[<span class="number">0</span>].decode(<span class="string">'utf-8'</span>).lower()) <span class="keyword">for</span> x <span class="keyword">in</span> reader])</div><div class="line">    <span class="comment"># Append SENTENCE_START and SENTENCE_END</span></div><div class="line">    sentences = [<span class="string">"%s %s %s"</span> % (sentence_start_token, x, sentence_end_token) <span class="keyword">for</span> x <span class="keyword">in</span> sentences]</div><div class="line"><span class="keyword">print</span> <span class="string">"Parsed %d sentences."</span> % (len(sentences))</div><div class="line"></div><div class="line"><span class="comment"># Tokenize the sentences into words</span></div><div class="line">tokenized_sentences = [nltk.word_tokenize(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> sentences]</div><div class="line"></div><div class="line"><span class="comment"># Count the word frequencies</span></div><div class="line">word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))</div><div class="line"><span class="keyword">print</span> <span class="string">"Found %d unique words tokens."</span> % len(word`_freq.items())</div><div class="line"></div><div class="line"><span class="comment"># Get the most common words and build index_to_word and word_to_index vectors</span></div><div class="line">vocab = word_freq.most_common(vocabulary_size<span class="number">-1</span>)</div><div class="line">index_to_word = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> vocab]</div><div class="line">index_to_word.append(unknown_token)</div><div class="line">word_to_index = dict([(w,i) <span class="keyword">for</span> i,w <span class="keyword">in</span> enumerate(index_to_word)])</div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">"Using vocabulary size %d."</span> % vocabulary_size</div><div class="line"><span class="keyword">print</span> <span class="string">"The least frequent word in our vocabulary is '%s' and appeared %d times."</span> % (vocab[<span class="number">-1</span>][<span class="number">0</span>], vocab[<span class="number">-1</span>][<span class="number">1</span>])</div><div class="line"></div><div class="line"><span class="comment"># Replace all words not in our vocabulary with the unknown token</span></div><div class="line"><span class="keyword">for</span> i, sent <span class="keyword">in</span> enumerate(tokenized_sentences):</div><div class="line">    tokenized_sentences[i] = [w <span class="keyword">if</span> w <span class="keyword">in</span> word_to_index <span class="keyword">else</span> unknown_token <span class="keyword">for</span> w <span class="keyword">in</span> sent]</div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">"\nExample sentence: '%s'"</span> % sentences[<span class="number">0</span>]</div><div class="line"><span class="keyword">print</span> <span class="string">"\nExample sentence after Pre-processing: '%s'"</span> % tokenized_sentences[<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="comment"># Create the training data</span></div><div class="line">X_train = np.asarray([[word_to_index[w] <span class="keyword">for</span> w <span class="keyword">in</span> sent[:<span class="number">-1</span>]] <span class="keyword">for</span> sent <span class="keyword">in</span> tokenized_sentences])</div><div class="line">y_train = np.asarray([[word_to_index[w] <span class="keyword">for</span> w <span class="keyword">in</span> sent[<span class="number">1</span>:]] <span class="keyword">for</span> sent <span class="keyword">in</span> tokenized_sentences])</div></pre></td></tr></table></figure>
<p>以下是一个训练样本：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">x:</div><div class="line">SENTENCE_START what are n't you understanding about this ? !</div><div class="line">[0, 51, 27, 16, 10, 856, 53, 25, 34, 69]</div><div class="line"></div><div class="line">y:</div><div class="line">what are n't you understanding about this ? ! SENTENCE_END</div><div class="line">[51, 27, 16, 10, 856, 53, 25, 34, 69, 1]</div></pre></td></tr></table></figure></p>
<p>接下来我们开始建立RNN。</p>
<p>##建立RNN</p>
<p><img src="/images/2016/06/rnn.jpg" alt=""></p>
<p>总结一下我们的RNN模型的形式。初始输入$$x$$是一个代表一条句子的矩阵，每一时刻的输入$$x_t$$是一个代表一个词语的向量，每一时刻的输出$$o_t$$也是一个向量，其中每个元素代表词表内每一个词被预测的概率。</p>
<p>RNN中的等式：<br>$$<br>s_t = tanh(Ux<em>t + Ws</em>{t-1})<br>o_t = softmax(Vs_t)<br>$$</p>
<p>介绍一下各个变量的维度：假设我们的词表大小$$C$$为8000，隐藏层大小$$H$$为100，可以把隐藏层大小理解为网络内存的大小，内存越大，网络能记忆的信息也越多，但是也要耗费更多的代价来计算。综上，我们的模型参数维度为：</p>
<p>$$<br>x_t \in \Bbb{R}^{8000} \ o_t \in \Bbb{R}^{8000} \ s_t \in \Bbb{R}^{100} \ U_t \in \Bbb{R}^{100 \times 8000} \ V_t \in \Bbb{R}^{8000 \times 100} \ W_t \in \Bbb{R}^{100 \times 100} \<br>$$</p>
<p>其中$$U，V，W$$是网络的参数，根据上面的等式，我们需要学习$$2HC+H^2$$个参数，由于$$x_t$$是稀疏的one-hot向量，所以其与$$U$$的乘积一步即可算出，$$W$$和$$S_t$$的维度都比较小，所以最繁琐的计算就是$$VS_t$$了。</p>
<p>###初始化</p>
<p>通过Numpy实现第一个版本，对$$U，V，W$$的初始化比较tricky，通常是把它们的初始值置于$$[-\frac{1}{\sqrt n},\frac{1}{\sqrt n}]$$较好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNNumpy</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, word_dim, hidden_dim=<span class="number">100</span>, bptt_truncate=<span class="number">4</span>)</span>:</span></div><div class="line">        <span class="comment"># Assign instance variables</span></div><div class="line">        self.word_dim = word_dim</div><div class="line">        self.hidden_dim = hidden_dim</div><div class="line">        self.bptt_truncate = bptt_truncate</div><div class="line">        <span class="comment"># Randomly initialize the network parameters</span></div><div class="line">        self.U = np.random.uniform(-np.sqrt(<span class="number">1.</span>/word_dim), np.sqrt(<span class="number">1.</span>/word_dim), (hidden_dim, word_dim))</div><div class="line">        self.V = np.random.uniform(-np.sqrt(<span class="number">1.</span>/hidden_dim), np.sqrt(<span class="number">1.</span>/hidden_dim), (word_dim, hidden_dim))</div><div class="line">        self.W = np.random.uniform(-np.sqrt(<span class="number">1.</span>/hidden_dim), np.sqrt(<span class="number">1.</span>/hidden_dim), (hidden_dim, hidden_dim))</div></pre></td></tr></table></figure>
<p>其中<code>word_dim</code>是词表大小，<code>hidden_dim</code>是隐藏层大小。</p>
<p>###前向计算</p>
<p>以下是前向计算（预测词语的概率）的过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(self, x)</span>:</span></div><div class="line">    <span class="comment"># The total number of time steps</span></div><div class="line">    T = len(x)</div><div class="line">    <span class="comment"># During forward propagation we save all hidden states in s because need them later.</span></div><div class="line">    <span class="comment"># We add one additional element for the initial hidden, which we set to 0</span></div><div class="line">    s = np.zeros((T + <span class="number">1</span>, self.hidden_dim))</div><div class="line">    s[<span class="number">-1</span>] = np.zeros(self.hidden_dim)</div><div class="line">    <span class="comment"># The outputs at each time step. Again, we save them for later.</span></div><div class="line">    o = np.zeros((T, self.word_dim))</div><div class="line">    <span class="comment"># For each time step...</span></div><div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(T):</div><div class="line">        <span class="comment"># Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.</span></div><div class="line">        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t<span class="number">-1</span>]))</div><div class="line">        o[t] = softmax(self.V.dot(s[t]))</div><div class="line">    <span class="keyword">return</span> [o, s]</div><div class="line"></div><div class="line">RNNNumpy.forward_propagation = forward_propagation</div></pre></td></tr></table></figure>
<p>我们同时返回输出及隐藏层状态，隐藏层状态之后会用于梯度计算。$$o_t$$是词表内每个词的概率，但是有时我们需要直接预测出概率最高的词：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></div><div class="line">    <span class="comment"># Perform forward propagation and return index of the highest score</span></div><div class="line">    o, s = self.forward_propagation(x)</div><div class="line">    <span class="keyword">return</span> np.argmax(o, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">RNNNumpy.predict = predict</div></pre></td></tr></table></figure>
<p>尝试输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">np.random.seed(<span class="number">10</span>)</div><div class="line">model = RNNNumpy(vocabulary_size)</div><div class="line">o, s = model.forward_propagation(X_train[<span class="number">10</span>])</div><div class="line"><span class="keyword">print</span> o.shape</div><div class="line"><span class="keyword">print</span> o</div><div class="line">(<span class="number">45</span>, <span class="number">8000</span>)</div><div class="line">[[ <span class="number">0.00012408</span>  <span class="number">0.0001244</span>   <span class="number">0.00012603</span> ...,  <span class="number">0.00012515</span>  <span class="number">0.00012488</span></div><div class="line">   <span class="number">0.00012508</span>]</div><div class="line"> [ <span class="number">0.00012536</span>  <span class="number">0.00012582</span>  <span class="number">0.00012436</span> ...,  <span class="number">0.00012482</span>  <span class="number">0.00012456</span></div><div class="line">   <span class="number">0.00012451</span>]</div><div class="line"> [ <span class="number">0.00012387</span>  <span class="number">0.0001252</span>   <span class="number">0.00012474</span> ...,  <span class="number">0.00012559</span>  <span class="number">0.00012588</span></div><div class="line">   <span class="number">0.00012551</span>]</div><div class="line"> ...,</div><div class="line"> [ <span class="number">0.00012414</span>  <span class="number">0.00012455</span>  <span class="number">0.0001252</span>  ...,  <span class="number">0.00012487</span>  <span class="number">0.00012494</span></div><div class="line">   <span class="number">0.0001263</span> ]</div><div class="line"> [ <span class="number">0.0001252</span>   <span class="number">0.00012393</span>  <span class="number">0.00012509</span> ...,  <span class="number">0.00012407</span>  <span class="number">0.00012578</span></div><div class="line">   <span class="number">0.00012502</span>]</div><div class="line"> [ <span class="number">0.00012472</span>  <span class="number">0.0001253</span>   <span class="number">0.00012487</span> ...,  <span class="number">0.00012463</span>  <span class="number">0.00012536</span></div><div class="line">   <span class="number">0.00012665</span>]]</div></pre></td></tr></table></figure></p>
<p>对上面句子（包括45个单词）中的每个词，模型都预测了8000个概率值。因为模型参数这时候是随机初始值，所以预测也是随机的。接下来我们给出预测的词的位置：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">predictions = model.predict(X_train[<span class="number">10</span>])</div><div class="line"><span class="keyword">print</span> predictions.shape</div><div class="line"><span class="keyword">print</span> predictions</div><div class="line">(<span class="number">45</span>,)</div><div class="line">[<span class="number">1284</span> <span class="number">5221</span> <span class="number">7653</span> <span class="number">7430</span> <span class="number">1013</span> <span class="number">3562</span> <span class="number">7366</span> <span class="number">4860</span> <span class="number">2212</span> <span class="number">6601</span> <span class="number">7299</span> <span class="number">4556</span> <span class="number">2481</span> <span class="number">238</span> <span class="number">2539</span></div><div class="line"> <span class="number">21</span> <span class="number">6548</span> <span class="number">261</span> <span class="number">1780</span> <span class="number">2005</span> <span class="number">1810</span> <span class="number">5376</span> <span class="number">4146</span> <span class="number">477</span> <span class="number">7051</span> <span class="number">4832</span> <span class="number">4991</span> <span class="number">897</span> <span class="number">3485</span> <span class="number">21</span></div><div class="line"> <span class="number">7291</span> <span class="number">2007</span> <span class="number">6006</span> <span class="number">760</span> <span class="number">4864</span> <span class="number">2182</span> <span class="number">6569</span> <span class="number">2800</span> <span class="number">2752</span> <span class="number">6821</span> <span class="number">4437</span> <span class="number">7021</span> <span class="number">7875</span> <span class="number">6912</span> <span class="number">3575</span>]</div></pre></td></tr></table></figure></p>
<p>接下来我们计算损失。</p>
<p>###计算损失</p>
<p>我们使用交叉熵函数作为损失函数。若我们有$$N$$个训练样本（text中的词语数），$$C$$个类别（词表大小），预测是$$o$$，label是$$y$$，那么损失计算为：</p>
<p>$$<br>L(y,o) = - \frac{1}{N} \sum_{n \in N} y_n \log o_n<br>$$</p>
<p>损失函数计算的是我们的预测oo与正确的词yy的差距的大小。通过以下计算损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_total_loss</span><span class="params">(self, x, y)</span>:</span></div><div class="line">    L = <span class="number">0</span></div><div class="line">    <span class="comment"># For each sentence...</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(len(y)):</div><div class="line">        o, s = self.forward_propagation(x[i])</div><div class="line">        <span class="comment"># We only care about our prediction of the "correct" words</span></div><div class="line">        correct_word_predictions = o[np.arange(len(y[i])), y[i]]</div><div class="line">        <span class="comment"># Add to the loss based on how off we were</span></div><div class="line">        L += <span class="number">-1</span> * np.sum(np.log(correct_word_predictions))</div><div class="line">    <span class="keyword">return</span> L</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_loss</span><span class="params">(self, x, y)</span>:</span></div><div class="line">    <span class="comment"># Divide the total loss by the number of training examples</span></div><div class="line">    N = np.sum((len(y_i) <span class="keyword">for</span> y_i <span class="keyword">in</span> y))</div><div class="line">    <span class="keyword">return</span> self.calculate_total_loss(x,y)/N</div><div class="line"></div><div class="line">RNNNumpy.calculate_total_loss = calculate_total_loss</div><div class="line">RNNNumpy.calculate_loss = calculate_loss</div></pre></td></tr></table></figure>
<p>让我们稍微检验一下，如果我们词表大小为$$C$$，那么开始时每个词被随机预测的概率为$$\frac{1}{C}$$，那么损失为$$L = - \frac{1}{C} N \log \ \frac{1}{C} = \log C$$：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"> Limit to <span class="number">1000</span> examples to save time</div><div class="line"><span class="keyword">print</span> <span class="string">"Expected Loss for random predictions: %f"</span> % np.log(vocabulary_size)</div><div class="line"><span class="keyword">print</span> <span class="string">"Actual loss: %f"</span> % model.calculate_loss(X_train[:<span class="number">1000</span>], y_train[:<span class="number">1000</span>])</div></pre></td></tr></table></figure></p>
<p>很接近。这里我们应当记住，如果data很大，那么计算损失会变得非常耗费时间。</p>
<p>###通过SGD和BPTT（BACKPROPAGATION THROUGH TIME ）训练RNN</p>
<p>给定训练样本$$(x,y)$$我们需要计算梯度$$\frac{\partial L}{\partial {U}},\frac{\partial L}{\partial {V}},\frac{\partial L}{\partial {W}}$$。通过以下代码实现BPTT：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bptt</span><span class="params">(self, x, y)</span>:</span></div><div class="line">    T = len(y)</div><div class="line">    <span class="comment"># Perform forward propagation</span></div><div class="line">    o, s = self.forward_propagation(x)</div><div class="line">    <span class="comment"># We accumulate the gradients in these variables</span></div><div class="line">    dLdU = np.zeros(self.U.shape)</div><div class="line">    dLdV = np.zeros(self.V.shape)</div><div class="line">    dLdW = np.zeros(self.W.shape)</div><div class="line">    delta_o = o</div><div class="line">    delta_o[np.arange(len(y)), y] -= <span class="number">1.</span></div><div class="line">    <span class="comment"># For each output backwards...</span></div><div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(T)[::<span class="number">-1</span>]:</div><div class="line">        dLdV += np.outer(delta_o[t], s[t].T)</div><div class="line">        <span class="comment"># Initial delta calculation</span></div><div class="line">        delta_t = self.V.T.dot(delta_o[t]) * (<span class="number">1</span> - (s[t] ** <span class="number">2</span>))</div><div class="line">        <span class="comment"># Backpropagation through time (for at most self.bptt_truncate steps)</span></div><div class="line">        <span class="keyword">for</span> bptt_step <span class="keyword">in</span> np.arange(max(<span class="number">0</span>, t-self.bptt_truncate), t+<span class="number">1</span>)[::<span class="number">-1</span>]:</div><div class="line">            <span class="comment"># print "Backpropagation step t=%d bptt step=%d " % (t, bptt_step)</span></div><div class="line">            dLdW += np.outer(delta_t, s[bptt_step<span class="number">-1</span>])</div><div class="line">            dLdU[:,x[bptt_step]] += delta_t</div><div class="line">            <span class="comment"># Update delta for next step</span></div><div class="line">            delta_t = self.W.T.dot(delta_t) * (<span class="number">1</span> - s[bptt_step<span class="number">-1</span>] ** <span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span> [dLdU, dLdV, dLdW]</div><div class="line"></div><div class="line">RNNNumpy.bptt = bptt</div></pre></td></tr></table></figure></p>
<p>接下来我们测试梯度。</p>
<p>###测试梯度</p>
<p>实现BP算法过程中，测试梯度是一个良好的习惯。通过以下公式：</p>
<p>$$<br>\frac{\partial L}{\partial{\theta}} \approx \lim_{h \rightarrow0} \frac{J(\theta + h)-J(\theta - h)}{2h}<br>$$</p>
<p>使用以上的公式来测试梯度，同样，由于需要计算所以的参数，梯度测试也是很耗时间的，在部分数据上测试是比较好的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span><span class="params">(self, x, y, h=<span class="number">0.001</span>, error_threshold=<span class="number">0.01</span>)</span>:</span></div><div class="line">    <span class="comment"># Calculate the gradients using backpropagation. We want to checker if these are correct.</span></div><div class="line">    bptt_gradients = self.bptt(x, y)</div><div class="line">    <span class="comment"># List of all parameters we want to check.</span></div><div class="line">    model_parameters = [<span class="string">'U'</span>, <span class="string">'V'</span>, <span class="string">'W'</span>]</div><div class="line">    <span class="comment"># Gradient check for each parameter</span></div><div class="line">    <span class="keyword">for</span> pidx, pname <span class="keyword">in</span> enumerate(model_parameters):</div><div class="line">        <span class="comment"># Get the actual parameter value from the mode, e.g. model.W</span></div><div class="line">        parameter = operator.attrgetter(pname)(self)</div><div class="line">        <span class="keyword">print</span> <span class="string">"Performing gradient check for parameter %s with size %d."</span> % (pname, np.prod(parameter.shape))</div><div class="line">        <span class="comment"># Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...</span></div><div class="line">        it = np.nditer(parameter, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</div><div class="line">        <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</div><div class="line">            ix = it.multi_index</div><div class="line">            <span class="comment"># Save the original value so we can reset it later</span></div><div class="line">            original_value = parameter[ix]</div><div class="line">            <span class="comment"># Estimate the gradient using (f(x+h) - f(x-h))/(2*h)</span></div><div class="line">            parameter[ix] = original_value + h</div><div class="line">            gradplus = self.calculate_total_loss([x],[y])</div><div class="line">            parameter[ix] = original_value - h</div><div class="line">            gradminus = self.calculate_total_loss([x],[y])</div><div class="line">            estimated_gradient = (gradplus - gradminus)/(<span class="number">2</span>*h)</div><div class="line">            <span class="comment"># Reset parameter to original value</span></div><div class="line">            parameter[ix] = original_value</div><div class="line">            <span class="comment"># The gradient for this parameter calculated using backpropagation</span></div><div class="line">            backprop_gradient = bptt_gradients[pidx][ix]</div><div class="line">            <span class="comment"># calculate The relative error: (|x - y|/(|x| + |y|))</span></div><div class="line">            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))</div><div class="line">            <span class="comment"># If the error is to large fail the gradient check</span></div><div class="line">            <span class="keyword">if</span> relative_error &gt; error_threshold:</div><div class="line">                <span class="keyword">print</span> <span class="string">"Gradient Check ERROR: parameter=%s ix=%s"</span> % (pname, ix)</div><div class="line">                <span class="keyword">print</span> <span class="string">"+h Loss: %f"</span> % gradplus</div><div class="line">                <span class="keyword">print</span> <span class="string">"-h Loss: %f"</span> % gradminus</div><div class="line">                <span class="keyword">print</span> <span class="string">"Estimated_gradient: %f"</span> % estimated_gradient</div><div class="line">                <span class="keyword">print</span> <span class="string">"Backpropagation gradient: %f"</span> % backprop_gradient</div><div class="line">                <span class="keyword">print</span> <span class="string">"Relative Error: %f"</span> % relative_error</div><div class="line">                <span class="keyword">return</span></div><div class="line">            it.iternext()</div><div class="line">        <span class="keyword">print</span> <span class="string">"Gradient check for parameter %s passed."</span> % (pname)</div><div class="line"></div><div class="line">RNNNumpy.gradient_check = gradient_check</div><div class="line"></div><div class="line"><span class="comment"># To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.</span></div><div class="line">grad_check_vocab_size = <span class="number">100</span></div><div class="line">np.random.seed(<span class="number">10</span>)</div><div class="line">model = RNNNumpy(grad_check_vocab_size, <span class="number">10</span>, bptt_truncate=<span class="number">1000</span>)</div><div class="line">model.gradient_check([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</div></pre></td></tr></table></figure>
<p>接下来我们实现SGD。</p>
<p>###实现SGD</p>
<p>通过两步实现：</p>
<ul>
<li><code>sdg_step</code>计算计算梯度对每个batch更新</li>
<li>外部循环迭代整个训练数据并且调整学习率</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">numpy_sdg_step</span><span class="params">(self, x, y, learning_rate)</span>:</span></div><div class="line">    <span class="comment"># Calculate the gradients</span></div><div class="line">    dLdU, dLdV, dLdW = self.bptt(x, y)</div><div class="line">    <span class="comment"># Change parameters according to gradients and learning rate</span></div><div class="line">    self.U -= learning_rate * dLdU</div><div class="line">    self.V -= learning_rate * dLdV</div><div class="line">    self.W -= learning_rate * dLdW</div><div class="line"></div><div class="line">RNNNumpy.sgd_step = numpy_sdg_step</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">Outer SGD Loop</div><div class="line"><span class="comment"># - model: The RNN model instance</span></div><div class="line"><span class="comment"># - X_train: The training data set</span></div><div class="line"><span class="comment"># - y_train: The training data labels</span></div><div class="line"><span class="comment"># - learning_rate: Initial learning rate for SGD</span></div><div class="line"><span class="comment"># - nepoch: Number of times to iterate through the complete dataset</span></div><div class="line"><span class="comment"># - evaluate_loss_after: Evaluate the loss after this many epochs</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_with_sgd</span><span class="params">(model, X_train, y_train, learning_rate=<span class="number">0.005</span>, nepoch=<span class="number">100</span>, evaluate_loss_after=<span class="number">5</span>)</span>:</span></div><div class="line">    <span class="comment"># We keep track of the losses so we can plot them later</span></div><div class="line">    losses = []</div><div class="line">    num_examples_seen = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(nepoch):</div><div class="line">        <span class="comment"># Optionally evaluate the loss</span></div><div class="line">        <span class="keyword">if</span> (epoch % evaluate_loss_after == <span class="number">0</span>):</div><div class="line">            loss = model.calculate_loss(X_train, y_train)</div><div class="line">            losses.append((num_examples_seen, loss))</div><div class="line">            time = datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</div><div class="line">            <span class="keyword">print</span> <span class="string">"%s: Loss after num_examples_seen=%d epoch=%d: %f"</span> % (time, num_examples_seen, epoch, loss)</div><div class="line">            <span class="comment"># Adjust the learning rate if loss increases</span></div><div class="line">            <span class="keyword">if</span> (len(losses) &gt; <span class="number">1</span> <span class="keyword">and</span> losses[<span class="number">-1</span>][<span class="number">1</span>] &gt; losses[<span class="number">-2</span>][<span class="number">1</span>]):</div><div class="line">                learning_rate = learning_rate * <span class="number">0.5</span></div><div class="line">                <span class="keyword">print</span> <span class="string">"Setting learning rate to %f"</span> % learning_rate</div><div class="line">            sys.stdout.flush()</div><div class="line">        <span class="comment"># For each training example...</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y_train)):</div><div class="line">            <span class="comment"># One SGD step</span></div><div class="line">            model.sgd_step(X_train[i], y_train[i], learning_rate)</div><div class="line">            num_examples_seen += <span class="number">1</span></div></pre></td></tr></table></figure>
<p>完成。我们来测试一下训练耗费多长的时间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">np.random.seed(<span class="number">10</span>)</div><div class="line">model = RNNNumpy(vocabulary_size)</div><div class="line">%timeit model.sgd_step(X_train[<span class="number">10</span>], y_train[<span class="number">10</span>], <span class="number">0.005</span>)</div></pre></td></tr></table></figure>
<p>可以看到在我的机器上需要SGD的每一步需要180ms，这代表整个训练将耗费数天甚至更多。我们可以通过许多的方法来加速这一过程，如改善代码和调整模型。这里我们希望使用GPU来加速。在这之前，我们先测试一下SGD的效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">np.random.seed(<span class="number">10</span>)</div><div class="line"><span class="comment"># Train on a small subset of the data to see what happens</span></div><div class="line">model = RNNNumpy(vocabulary_size)</div><div class="line">losses = train_with_sgd(model, X_train[:<span class="number">100</span>], y_train[:<span class="number">100</span>], nepoch=<span class="number">10</span>, evaluate_loss_after=<span class="number">1</span>)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="number">2016</span><span class="number">-06</span><span class="number">-13</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">46</span>: Loss after num_examples_seen=<span class="number">0</span> epoch=<span class="number">0</span>: <span class="number">8.987425</span></div><div class="line"><span class="number">2016</span><span class="number">-06</span><span class="number">-13</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">56</span>: Loss after num_examples_seen=<span class="number">100</span> epoch=<span class="number">1</span>: <span class="number">8.976270</span></div><div class="line"><span class="number">2016</span><span class="number">-06</span><span class="number">-13</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">06</span>: Loss after num_examples_seen=<span class="number">200</span> epoch=<span class="number">2</span>: <span class="number">8.960212</span></div><div class="line"><span class="number">2016</span><span class="number">-06</span><span class="number">-13</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">16</span>: Loss after num_examples_seen=<span class="number">300</span> epoch=<span class="number">3</span>: <span class="number">8.930430</span></div><div class="line"><span class="number">2016</span><span class="number">-06</span><span class="number">-13</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">26</span>: Loss after num_examples_seen=<span class="number">400</span> epoch=<span class="number">4</span>: <span class="number">8.862264</span></div><div class="line"><span class="number">2016</span><span class="number">-06</span><span class="number">-13</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">37</span>: Loss after num_examples_seen=<span class="number">500</span> epoch=<span class="number">5</span>: <span class="number">6.913570</span></div><div class="line"><span class="number">2016</span><span class="number">-06</span><span class="number">-13</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">46</span>: Loss after num_examples_seen=<span class="number">600</span> epoch=<span class="number">6</span>: <span class="number">6.302493</span></div><div class="line"><span class="number">2016</span><span class="number">-06</span><span class="number">-13</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">56</span>: Loss after num_examples_seen=<span class="number">700</span> epoch=<span class="number">7</span>: <span class="number">6.014995</span></div><div class="line"><span class="number">2016</span><span class="number">-06</span><span class="number">-13</span> <span class="number">17</span>:<span class="number">01</span>:<span class="number">06</span>: Loss after num_examples_seen=<span class="number">800</span> epoch=<span class="number">8</span>: <span class="number">5.833877</span></div><div class="line"><span class="number">2016</span><span class="number">-06</span><span class="number">-13</span> <span class="number">17</span>:<span class="number">01</span>:<span class="number">16</span>: Loss after num_examples_seen=<span class="number">900</span> epoch=<span class="number">9</span>: <span class="number">5.710718</span></div></pre></td></tr></table></figure>
<p>看起来，SGD起到了效果。</p>
<p>##通过Theano和GPU训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">enp.random.seed(<span class="number">10</span>)</div><div class="line">model = RNNTheano(vocabulary_size)</div><div class="line">%timeit model.sgd_step(X_train[<span class="number">10</span>], y_train[<span class="number">10</span>], <span class="number">0.005</span>)</div></pre></td></tr></table></figure>
<p>这次一次SGD步骤耗费为73.7ms。</p>
<p>这里我们直接使用训练好的的参数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_model_parameters_theano, save_model_parameters_theano</div><div class="line"></div><div class="line">model = RNNTheano(vocabulary_size, hidden_dim=<span class="number">50</span>)</div><div class="line"><span class="comment"># losses = train_with_sgd(model, X_train, y_train, nepoch=50)</span></div><div class="line"><span class="comment"># save_model_parameters_theano('./data/trained-model-theano.npz', model)</span></div><div class="line">load_model_parameters_theano(<span class="string">'./data/trained-model-theano.npz'</span>, model)</div></pre></td></tr></table></figure></p>
<p>###生成语句</p>
<p>使用如下生成语句：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_sentence</span><span class="params">(model)</span>:</span></div><div class="line">    <span class="comment"># We start the sentence with the start token</span></div><div class="line">    new_sentence = [word_to_index[sentence_start_token]]</div><div class="line">    <span class="comment"># Repeat until we get an end token</span></div><div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> new_sentence[<span class="number">-1</span>] == word_to_index[sentence_end_token]:</div><div class="line">        next_word_probs = model.forward_propagation(new_sentence)</div><div class="line">        sampled_word = word_to_index[unknown_token]</div><div class="line">        <span class="comment"># We don't want to sample unknown words</span></div><div class="line">        <span class="keyword">while</span> sampled_word == word_to_index[unknown_token]:</div><div class="line">            samples = np.random.multinomial(<span class="number">1</span>, next_word_probs[<span class="number">-1</span>])</div><div class="line">            sampled_word = np.argmax(samples)</div><div class="line">        new_sentence.append(sampled_word)</div><div class="line">    sentence_str = [index_to_word[x] <span class="keyword">for</span> x <span class="keyword">in</span> new_sentence[<span class="number">1</span>:<span class="number">-1</span>]]</div><div class="line">    <span class="keyword">return</span> sentence_str</div><div class="line"></div><div class="line">num_sentences = <span class="number">10</span></div><div class="line">senten_min_length = <span class="number">7</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_sentences):</div><div class="line">    sent = []</div><div class="line">    <span class="comment"># We want long sentences, not sentences with one or two words</span></div><div class="line">    <span class="keyword">while</span> len(sent) &lt; senten_min_length:</div><div class="line">        sent = generate_sentence(model)</div><div class="line">    <span class="keyword">print</span> <span class="string">" "</span>.join(sent)</div></pre></td></tr></table></figure></p>
<p>得到结果为：</p>
<ul>
<li>no to blame their stuff go at all .</li>
<li>consider via under gear but equal every game .</li>
<li>no similar work on the ui birth a ce nightmare .</li>
<li>the challenging what is absolutely hard .</li>
<li>me do you research getting +2 .</li>
<li>ugh is much good , no .</li>
<li>me so many different lines hair .</li>
<li>probably not very a bot or gain .</li>
<li>correct this is affected so why ?</li>
<li>register but a grown gun environment .</li>
</ul>
<p>看起来还不错！不过还是有一些缺陷，这些都是由于vanilla RNN不能解决长期依赖问题导致的。下一篇我们将讨论BPTT并且关注梯度消失/爆炸问题。</p>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>Title:</span><a href="/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/">RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO</a></p>
        <p><span>Author:</span><a href="/" title="Back to Homepage">江南消夏</a></p>
        <p><span>Created:</span>2016-06-15, 17:54:18</p>
        <p><span>Updated:</span>2017-01-13, 17:47:18</p>
        <p>
            <span>Full URL:</span><a class="post-url" href="/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/" title="RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO">http://frankchen.xyz/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/</a>
            <span class="copy-path" data-clipboard-text="From http://frankchen.xyz/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/　　By 江南消夏" title="Copy Article&#39;s Link &amp; Author"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>License:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"CC BY-NC-SA 4.0"</a> Keep Link &amp; Author if Distribute.
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2016/06/19/2016-06-19-yi-chuan-suan-fa-jian-shu/">
                    遗传算法简述
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2016/06/07/2016-06-07-edit-dns-of-ubuntu/">
                    ubuntu下修改DNS并且避免重启失效的方法
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#语言模型"><span class="toc-number">1.</span> <span class="toc-text">语言模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#预处理并训练数据"><span class="toc-number">2.</span> <span class="toc-text">预处理并训练数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-标记化"><span class="toc-number">2.1.</span> <span class="toc-text">1.标记化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-移除低频词"><span class="toc-number">2.2.</span> <span class="toc-text">2.移除低频词</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-放置句子开始和结束标记"><span class="toc-number">2.3.</span> <span class="toc-text">3.放置句子开始和结束标记</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-建立训练数据的矩阵"><span class="toc-number">2.4.</span> <span class="toc-text">4.建立训练数据的矩阵</span></a></li></ol></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="Hide"  title="Show or Hide Table of Contents">

    <script>
        yiliaConfig.toc = ["Hide", "Show", !!"false"];
    </script>



    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></i></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO　| loop in data　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    
      <div class="duoshuo" id="comments">
    <div id="comment-box" ></div>
    <div class="ds-thread" id="ds-thread" data-thread-key="2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/" data-title="RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO" data-url="http://frankchen.xyz/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/"></div>
    <script>
        var duoshuoQuery = {short_name:"frankchen0130"};
        var loadComment = function(){
            var d = document, s = d.createElement('script');
            s.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
            s.async = true; s.charset = 'UTF-8';
            (d.head || d.body).appendChild(s);
        }

        
    </script>
    
    <script> loadComment(); </script>

</div>
    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2016/06/19/2016-06-19-yi-chuan-suan-fa-jian-shu/" title="Pre: 遗传算法简述">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="Mini Archives"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2016/06/07/2016-06-07-edit-dns-of-ubuntu/" title="Next: ubuntu下修改DNS并且避免重启失效的方法">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/01/13/Map-0f-Java/">Map 0f Java</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/11/good commander/">To be a great commander</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/09/logistic-regression/">logistic regression的公式手推相关</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/06/linear-regression/">linear regression 的公式手推相关</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/25/AliPay-RedEnvelope-Hacker/">AliPay RedEnvelope Hacker</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/11/pointer-and/">pointer and ++--</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/09/C++ Basic/">C++ Cheat Sheet</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/02/QuickSort-in-C/">QuickSort in C & Python</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/28/Workflow-for-automatic-extract-and-jump-to-the-true-URL-of-91porn-com/">Workflow for automatic extract and jump to the true URL of 91porn.com</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/25/Devide-and-Conquer-Counting-Inversions-with-Python/">Devide and Conquer Counting Inversions with Python</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/18/Understanding-EM-algorithm/">Understanding EM algorithm</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/17/Way-to-solving-the-DS-Store-problem-of-Mac/">Way to solving the .DS_Store problem of Mac</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/11/2016-11-11-a-efficiency-comparison-between-while-slash-for-slash-generator-slash-comprehension/">A efficiency comparison between while for generator comprehension - Python while、for、生成器、列表推导等语句的执行效率对比</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/21/2016-09-21-jing-que-lu-,-zhao-hui-lu-,-f1-zhi-,-roc-slash-slash-auc-,-star-star-prc-star-star-ge-zi-de-you-que-dian-shi-shi-yao-?/">精确率、召回率、F1 值、ROC/AUC 、PRC各自的优缺点是什么？</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/07/2016-09-07-pdf-click-return/">PDF click return</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/07/16/2016-07-16-ru-he-jie-jue-linux-xia-zip-wen-jian-jie-ya-luan-ma/">如何解决Linux 下 zip 文件解压乱码</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/22/2016-06-22-recurrent-neural-network-tutorial-part-4-implementing-a-gru-slash-lstm-rnn-with-python-and-theano/">RECURRENT NEURAL NETWORK TUTORIAL, PART 4 – IMPLEMENTING A GRU/LSTM RNN WITH PYTHON AND THEANO</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/22/2016-06-22-recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">RECURRENT NEURAL NETWORKS TUTORIAL, PART 3 – BACKPROPAGATION THROUGH TIME AND VANISHING GRADIENTS</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/22/2016-06-22-vim-lian-ji-gong-er/">Vim 练级攻略二</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/22/2016-06-22-vim-lian-ji-gong-lue-1/">Vim 练级攻略一</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/19/2016-06-19-yi-chuan-suan-fa-jian-shu/">遗传算法简述</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/">RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/07/2016-06-07-edit-dns-of-ubuntu/">ubuntu下修改DNS并且避免重启失效的方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/06/2016-06-06-recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">RECURRENT NEURAL NETWORKS TUTORIAL, PART 1 – INTRODUCTION TO RNNS</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/04/2016-06-04-understanding-lstm-networks/">理解LSTM网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/04/2016-06-04-rephil-and-mapreduce/">Rephil和MapReduce： 描述长尾数据的数学模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/04/2016-06-04-note-for-cs224d-1/">note for CS224d:1</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/26/2016-05-26-char-rnn-chinese/">char-rnn-chinese</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/25/2016-05-25-pythonoop/">Python OOP（Object Oriented Programming）</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/24/2016-05-24-di-pian-bo-ke/">使用文本处理命令获取链接批量下载</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2016-2017 江南消夏
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="A fast, simple &amp; powerful blog framework">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="Another simple and elegant theme for Hexo  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="Site Visitors"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="Page Hits"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 4;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="Back to Top"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="Comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="Go to Bottom"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
             menu: ".header-menu a", 
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

    <script>
        var originTitle = document.title;
        var titleTime;
        document.addEventListener("visibilitychange", function() {
            if (document.hidden) {
                document.title = "(つェ⊂) 我藏好了哦~ " + originTitle;
                clearTimeout(titleTime);
            }
            else {
                document.title = "(*´∇｀*) 被你发现啦~ " + originTitle;
                titleTime = setTimeout(function() {
                    document.title = originTitle;
                }, 2000);
            }
        })
    </script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>