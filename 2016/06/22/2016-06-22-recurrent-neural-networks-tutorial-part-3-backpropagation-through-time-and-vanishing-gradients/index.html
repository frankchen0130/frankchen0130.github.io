<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>RECURRENT NEURAL NETWORKS TUTORIAL, PART 3 – BACKPROPAGATION THROUGH TIME AND VANISHING GRADIENTS | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="在之前的部分我们实现了RNN，但是并未深入探究时间反向传播算法，本文将对这一点作详细说明。我们将了解关于梯度消失问题的知识，它促使了LSTM和GRU的出现，而这两者都是NLP领域非常常见的模型。
##BACKPROPAGATION THROUGH TIME (BPTT)
首先我们回顾一下RNN的基本等式：">
<meta property="og:type" content="article">
<meta property="og:title" content="RECURRENT NEURAL NETWORKS TUTORIAL, PART 3 – BACKPROPAGATION THROUGH TIME AND VANISHING GRADIENTS">
<meta property="og:url" content="http://yoursite.com/2016/06/22/2016-06-22-recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="在之前的部分我们实现了RNN，但是并未深入探究时间反向传播算法，本文将对这一点作详细说明。我们将了解关于梯度消失问题的知识，它促使了LSTM和GRU的出现，而这两者都是NLP领域非常常见的模型。
##BACKPROPAGATION THROUGH TIME (BPTT)
首先我们回顾一下RNN的基本等式：">
<meta property="og:image" content="http://yoursite.com/images/2016/06/latex.png">
<meta property="og:image" content="http://yoursite.com/images/2016/06/latex1.png">
<meta property="og:image" content="http://yoursite.com/images/2016/06/latex2.png">
<meta property="og:image" content="http://yoursite.com/images/2016/06/latex3.png">
<meta property="og:image" content="http://yoursite.com/images/2016/06/latex4.png">
<meta property="og:image" content="http://yoursite.com/images/2016/06/rnn-bptt-with-gradients.png">
<meta property="og:image" content="http://yoursite.com/images/2016/06/latex5.png">
<meta property="og:image" content="http://yoursite.com/images/2016/06/latex6.png">
<meta property="og:image" content="http://yoursite.com/images/2016/06/tanh.png">
<meta property="og:updated_time" content="2016-11-11T13:50:33.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RECURRENT NEURAL NETWORKS TUTORIAL, PART 3 – BACKPROPAGATION THROUGH TIME AND VANISHING GRADIENTS">
<meta name="twitter:description" content="在之前的部分我们实现了RNN，但是并未深入探究时间反向传播算法，本文将对这一点作详细说明。我们将了解关于梯度消失问题的知识，它促使了LSTM和GRU的出现，而这两者都是NLP领域非常常见的模型。
##BACKPROPAGATION THROUGH TIME (BPTT)
首先我们回顾一下RNN的基本等式：">
<meta name="twitter:image" content="http://yoursite.com/images/2016/06/latex.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-2016-06-22-recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/06/22/2016-06-22-recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" class="article-date">
  <time datetime="2016-06-22T07:42:14.000Z" itemprop="datePublished">2016-06-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/tutorial/">tutorial</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      RECURRENT NEURAL NETWORKS TUTORIAL, PART 3 – BACKPROPAGATION THROUGH TIME AND VANISHING GRADIENTS
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在之前的部分我们实现了RNN，但是并未深入探究时间反向传播算法，本文将对这一点作详细说明。我们将了解关于梯度消失问题的知识，它促使了LSTM和GRU的出现，而这两者都是NLP领域非常常见的模型。</p>
<p>##BACKPROPAGATION THROUGH TIME (BPTT)</p>
<p>首先我们回顾一下RNN的基本等式：</p>
<p><img src="/images/2016/06/latex.png" alt=""></p>
<a id="more"></a>
<p>我们也定义了损失函数（交叉熵）：</p>
<p><img src="/images/2016/06/latex1.png" alt=""></p>
<p>在这里，$$y_t$$是 $$t$$时刻的正确的词语， $$\tilde{y_t}$$ 是我们的预测。因为我们把一整个序列（句子）当做是输入，那么错误等同于每个时间step（词语）的错误的和。</p>
<p>![](/images/2016/06/rnn-bptt1.png）</p>
<p>需要注意，我们的目标是计算基于参数$$U, V, W$$错误的梯度，并且通过SGD来学习到好的参数。类似于我们将错误相加的做法，对于一个训练样本，我们将各个时间点的梯度相加。</p>
<p>$$<br>\frac{\partial{E}}{\partial{W}} = \sum_{t} \frac{\partial{E_t}}{\partial{W}}<br>$$</p>
<p>我们使用链式求导法则来计算这些梯度，这就是反向传播算法:从错误处反向计算。以下我们使用$$E_3$$作为例子。</p>
<p><img src="/images/2016/06/latex2.png" alt=""></p>
<p>其中，$$z_3 = V s_3$$，并且$$\otimes$$指的是向量外积。在这里我们需要注意到，$$\frac{\partial{E_3}}{\partial{V}}$$只取决于当前时刻的值$$\tilde{y_3}, y_3, s_3$$。如果你明确了这一点，那么计算$$V$$的梯度只是矩阵计算罢了。</p>
<p>但是，对于$$\frac{\partial{E_3}}{\partial{W}}$$和$$V$$就不一样了。我们写出链式法则：</p>
<p><img src="/images/2016/06/latex3.png" alt=""></p>
<p>可以看到，$$s_3 = \tanh(U x_t + W s_2)$$取决于$$s_2$$，而$$s_2$$又取决于$$W$$和$$s_1$$，以此类推。所以我们计算$$W$$的偏导，我们不能把$$s_2$$当做一个常量，相反我们需要一遍遍的应用链式法则：</p>
<p><img src="/images/2016/06/latex4.png" alt=""></p>
<p>我们把每个时间点对于梯度贡献综合起来。换句话说，因为$$W$$在直到我们需要输出的时刻都被用到，所以我们需要计算$$t=3$$时刻直到$$t=0$$时刻：</p>
<p><img src="/images/2016/06/rnn-bptt-with-gradients.png" alt=""></p>
<p>这其实和深度前馈神经网络里的标准的反向传播算法是类似的。主要的不同点在于我们把每个时间点的$$W$$的梯度综合起来。传统的神经网络的不同层之间不共享参数，于是我们也不需要综合什么。但是在我看来，BPTT只不过是在没有展开的RNN上的标准BP算法的别名罢了。类似于标准的BP算法，你也可以定义一个徳塔项来表示反向传播的内容。例如：$$\delta_{2}^{(3)} = \frac{\partial{E_3}}{\partial{z_2}} = \frac{\partial{E_3}}{\partial{s_3}} \frac{\partial{s_3}}{\partial{s_2}} \frac{\partial{s_2}}{\partial{z_2}}$$，其中$$z_2 = Ux_2 + Ws_1$$。以此类推。</p>
<p>代码实现BPTT如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bptt</span><span class="params">(self, x, y)</span>:</span></div><div class="line">    T = len(y)</div><div class="line">    <span class="comment"># Perform forward propagation</span></div><div class="line">    o, s = self.forward_propagation(x)</div><div class="line">    <span class="comment"># We accumulate the gradients in these variables</span></div><div class="line">    dLdU = np.zeros(self.U.shape)</div><div class="line">    dLdV = np.zeros(self.V.shape)</div><div class="line">    dLdW = np.zeros(self.W.shape)</div><div class="line">    delta_o = o</div><div class="line">    delta_o[np.arange(len(y)), y] -= <span class="number">1.</span></div><div class="line">    <span class="comment"># For each output backwards...</span></div><div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(T)[::<span class="number">-1</span>]:</div><div class="line">        dLdV += np.outer(delta_o[t], s[t].T)</div><div class="line">        <span class="comment"># Initial delta calculation: dL/dz</span></div><div class="line">        delta_t = self.V.T.dot(delta_o[t]) * (<span class="number">1</span> - (s[t] ** <span class="number">2</span>))</div><div class="line">        <span class="comment"># Backpropagation through time (for at most self.bptt_truncate steps)</span></div><div class="line">        <span class="keyword">for</span> bptt_step <span class="keyword">in</span> np.arange(max(<span class="number">0</span>, t-self.bptt_truncate), t+<span class="number">1</span>)[::<span class="number">-1</span>]:</div><div class="line">            <span class="comment"># print "Backpropagation step t=%d bptt step=%d " % (t, bptt_step)</span></div><div class="line">            <span class="comment"># Add to gradients at each previous step</span></div><div class="line">            dLdW += np.outer(delta_t, s[bptt_step<span class="number">-1</span>])</div><div class="line">            dLdU[:,x[bptt_step]] += delta_t</div><div class="line">            <span class="comment"># Update delta for next step dL/dz at t-1</span></div><div class="line">            delta_t = self.W.T.dot(delta_t) * (<span class="number">1</span> - s[bptt_step<span class="number">-1</span>] ** <span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span> [dLdU, dLdV, dLdW]</div></pre></td></tr></table></figure></p>
<p>从代码你也可以观察到RNN不容易训练：由于序列比较长，每次传播需要计算很多层，实践上通常许多人的做法是将传播限制在几步之内。</p>
<p>##梯度消失问题</p>
<p>之前的部分我们已经讲述了关于RNN的长期依赖问题。为了解这一点，我们来观察上面我们计算过的梯度。</p>
<p><img src="/images/2016/06/latex5.png" alt=""></p>
<p>注意到是对于自身的链式法则，如。再注意到，我们这里计算的是向量对向量的偏导，其结果是一个雅可比矩阵。因此可以改写上面的梯度式为：</p>
<p><img src="/images/2016/06/latex6.png" alt=""></p>
<p>式子中的雅克比矩阵的二范数具有上限1.这导致激活函数tanh（或者sigmoid）映射所有的值到[-1,1]，那么偏导也被限制在1（对于sigmoid是$$\frac{1}{4} $$）。</p>
<p><img src="/images/2016/06/tanh.png" alt=""></p>
<p>图中我们可以看到，在tanh函数两边，梯度都接近于0.这导致它之前的梯度也接近于0，那么，与矩阵中的数字多次相乘使得梯度迅速减小，直到接近消失。院出的时间点对于现在的影响接近于0，这就是长期依赖问题的原因。但是，长期依赖问题并不只是对于RNN出现，深度神经网络都具有这个问题，只不过RNN经常要处理长序列问题，所以网络层数很多，这个问题就经常出现了。</p>
<p>容易想到的是，与梯度消失问题对应的是，梯度爆炸问题。当雅克比矩阵中的数值较大时，容易出现这个问题。但是，通常来说，对于梯度爆炸，业界关注并不太多。有两个原因，其一，梯度爆炸发生时通常容易发现，因为可能导致程序崩溃之类的后果；其二，通常为梯度设置上限是应对梯度爆炸的简便做法。</p>
<p>那么怎么应对梯度弥散问题呢？首先，更好的初始化权重可以减少梯度弥散的效果；其次，使用正则化；更加通用的方法是使用ReLU作为激活函数，其梯度要么是1要么是0，所以更少的可能出现梯度弥散的问题。另外，更加流行的做法则是使用 Long Short-Term Memory (LSTM)或者Gated Recurrent Unit (GRU)，LSTM<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="external">出现于1997年</a>，也许是NLP领域近期最流行的网络结构。GRU，<a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">出现于2014年</a>，是LSTM的简化版本。两种网络都是为了应对梯度弥散和长期依赖问题。我们将会在之后的教程中涉及它们。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/06/22/2016-06-22-recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" data-id="civff93s4000wjefyhdbaahcq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN-deep-learning/">RNN deep-learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/06/22/2016-06-22-recurrent-neural-network-tutorial-part-4-implementing-a-gru-slash-lstm-rnn-with-python-and-theano/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          RECURRENT NEURAL NETWORK TUTORIAL, PART 4 – IMPLEMENTING A GRU/LSTM RNN WITH PYTHON AND THEANO
        
      </div>
    </a>
  
  
    <a href="/2016/06/22/2016-06-22-vim-lian-ji-gong-er/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Vim 练级攻略二</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Note/">Note</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Vim/">Vim</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/essay/">essay</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/note/">note</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tutorial/">tutorial</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNN/">DNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Genetic-Algorithm/">Genetic Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LDA/">LDA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine-Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP-deep-learning/">NLP deep-learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN-LSTM/">RNN LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN-deep-learning/">RNN deep-learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/essay/">essay</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tips/">tips</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tutorial/">tutorial</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/Genetic-Algorithm/" style="font-size: 10px;">Genetic Algorithm</a> <a href="/tags/LDA/" style="font-size: 10px;">LDA</a> <a href="/tags/Linux/" style="font-size: 13.33px;">Linux</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine-Learning</a> <a href="/tags/NLP-deep-learning/" style="font-size: 10px;">NLP deep-learning</a> <a href="/tags/RNN-LSTM/" style="font-size: 10px;">RNN LSTM</a> <a href="/tags/RNN-deep-learning/" style="font-size: 20px;">RNN deep-learning</a> <a href="/tags/essay/" style="font-size: 10px;">essay</a> <a href="/tags/python/" style="font-size: 13.33px;">python</a> <a href="/tags/tips/" style="font-size: 10px;">tips</a> <a href="/tags/tutorial/" style="font-size: 16.67px;">tutorial</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/11/13/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2016/11/11/2016-11-11-a-efficiency-comparison-between-while-slash-for-slash-generator-slash-comprehension/">A efficiency comparison between while for generator comprehension - Python while、for、生成器、列表推导等语句的执行效率对比</a>
          </li>
        
          <li>
            <a href="/2016/09/21/2016-09-21-jing-que-lu-,-zhao-hui-lu-,-f1-zhi-,-roc-slash-slash-auc-,-star-star-prc-star-star-ge-zi-de-you-que-dian-shi-shi-yao-?/">精确率、召回率、F1 值、ROC/AUC 、PRC各自的优缺点是什么？</a>
          </li>
        
          <li>
            <a href="/2016/09/07/2016-09-07-pdf-click-return/">PDF click return</a>
          </li>
        
          <li>
            <a href="/2016/07/16/2016-07-16-ru-he-jie-jue-linux-xia-zip-wen-jian-jie-ya-luan-ma/">如何解决Linux 下 zip 文件解压乱码</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>