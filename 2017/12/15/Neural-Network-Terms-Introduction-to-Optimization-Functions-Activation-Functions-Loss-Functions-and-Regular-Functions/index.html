<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>神经网络术语大百科：优化函数、激活函数、损失函数、正则方法的简介 | 不正经数据科学家</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="简述关于神经网络的各种优化函数（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）、各种激活函数（Sigmoid，Tanh、Hard Sigmoid、Softplus、ReLU、ElU、PReLU、RReLU）、各种损失函数以及正则方法的简述，并附带代码实现例子。">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络术语大百科：优化函数、激活函数、损失函数、正则方法的简介">
<meta property="og:url" content="http://frankchen.xyz/2017/12/15/Neural-Network-Terms-Introduction-to-Optimization-Functions-Activation-Functions-Loss-Functions-and-Regular-Functions/index.html">
<meta property="og:site_name" content="不正经数据科学家">
<meta property="og:description" content="简述关于神经网络的各种优化函数（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）、各种激活函数（Sigmoid，Tanh、Hard Sigmoid、Softplus、ReLU、ElU、PReLU、RReLU）、各种损失函数以及正则方法的简述，并附带代码实现例子。">
<meta property="og:locale">
<meta property="og:image" content="http://frankchen.xyz/images/neuralnetworks.png">
<meta property="og:image" content="http://frankchen.xyz/images/2017/05/contours_evaluation_optimizers.gif">
<meta property="og:image" content="http://frankchen.xyz/images/2017/05/saddle_point_evaluation_optimizers.gif">
<meta property="og:image" content="http://frankchen.xyz/images/15134144452960.jpg">
<meta property="og:image" content="http://frankchen.xyz/images/Screen%20Shot%202017-12-16%20at%2017.03.18.png">
<meta property="og:image" content="http://frankchen.xyz/images/15134157378274.jpg">
<meta property="og:image" content="http://frankchen.xyz/images/15134154076033.jpg">
<meta property="og:image" content="http://frankchen.xyz/images/15134156685588.jpg">
<meta property="og:image" content="http://frankchen.xyz/images/15134156615077.jpg">
<meta property="og:image" content="http://frankchen.xyz/images/15134123600394.jpg">
<meta property="og:image" content="http://frankchen.xyz/images/15134129990682.jpg">
<meta property="article:published_time" content="2017-12-15T04:45:44.000Z">
<meta property="article:modified_time" content="2017-12-16T09:21:29.000Z">
<meta property="article:author" content="江南消夏">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Algorithm">
<meta property="article:tag" content="Data Science">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://frankchen.xyz/images/neuralnetworks.png">
  
    <link rel="alternate" href="/atom.xml" title="不正经数据科学家" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">不正经数据科学家</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://frankchen.xyz"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Neural-Network-Terms-Introduction-to-Optimization-Functions-Activation-Functions-Loss-Functions-and-Regular-Functions" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/15/Neural-Network-Terms-Introduction-to-Optimization-Functions-Activation-Functions-Loss-Functions-and-Regular-Functions/" class="article-date">
  <time datetime="2017-12-15T04:45:44.000Z" itemprop="datePublished">2017-12-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      神经网络术语大百科：优化函数、激活函数、损失函数、正则方法的简介
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/neuralnetworks.png" alt="neuralnetworks"></p>
<p>简述关于神经网络的各种优化函数（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）、各种激活函数（Sigmoid，Tanh、Hard Sigmoid、Softplus、ReLU、ElU、PReLU、RReLU）、各种损失函数以及正则方法的简述，并附带代码实现例子。</p>
<span id="more"></span>
<h1 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h1><p>先上两张图</p>
<figure class="three">
   <img src="/images/2017/05/contours_evaluation_optimizers.gif" title="Logo" width="300" />
   <img src="/images/2017/05/saddle_point_evaluation_optimizers.gif" title="Logo" width="300" />
</figure>

<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p>没有激活函数，神经元就只是一个线性函数，那么无论多少层的神经元叠加是没有意义的。而主流激活函数也随着神经网络、深度学习的发展迭代进化了许多次代。</p>
<h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p><img src="/images/15134144452960.jpg"><br>Sigmoid是S形状的意思，又因为它是逻辑回归的激活函数又叫logistic函数，函数式为$<code>y = 1 / (1 + exp(-x))</code>$是很早以前最常用的激活函数，其实也是有一些优点的，比如，</p>
<ul>
<li>值域位于0-1，那么对于逻辑回归，这是对于二分类的一个很自然的表达，也就是概率</li>
<li>处处连续可导</li>
</ul>
<p>不过呢，我们观察它的形状，可以得出，Sigmoid函数在两端（靠近0和1的部分）梯度很小，这也意味着，如果神经元的输出落到了这个地方，那么它几乎没什么梯度可以传到后面，而随着神经网络的层层削弱，后面的层（靠近输入的层）没有多少梯度能传过来，几乎就“学不到什么”了。这叫做梯度消失问题，一度是阻碍神经网络往更深的层进化的主要困难，导致深度学习专家们绞尽脑汁想了许多方法来对抗这个问题，比如“Xavier and He Initialization”，比如我们要把weight随机初始化为如下的范围，<br><img src="/images/Screen%20Shot%202017-12-16%20at%2017.03.18.png" alt="Screen Shot 2017-12-16 at 17.03.18"></p>
<p>sigmoid的另一个问题是它不是0均值的，Sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢。举例来讲，对，如果所有均为正数或负数，那么其对的导数总是正数或负数，这会导致如下图红色箭头所示的阶梯式更新，这显然并非一个好的优化路径。深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。</p>
<p><img src="/images/15134157378274.jpg"></p>
<p>如今，sigmoid函数应用最广泛的在于其变种softmax在多元分类中，比如手写数字识别，经过卷积神经网络的处理，最后我们需要网络输出每个预测的概率值，最后预测为某一个数字，这里就需要用到softmax，<br><img src="/images/15134154076033.jpg"><br>以下是softmax的Keras代码，注意其中一个trick，<code>e = K.exp(x - K.max(x, axis=axis, keepdims=True))</code>这里每个分量减去最大值是为了减少计算量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">x, axis=-<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Softmax activation function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Arguments</span></span><br><span class="line"><span class="string">        x : Tensor.</span></span><br><span class="line"><span class="string">        axis: Integer, axis along which the softmax normalization is applied.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Returns</span></span><br><span class="line"><span class="string">        Tensor, output of softmax transformation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Raises</span></span><br><span class="line"><span class="string">        ValueError: In case `dim(x) == 1`.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ndim = K.ndim(x)</span><br><span class="line">    <span class="keyword">if</span> ndim == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> K.softmax(x)</span><br><span class="line">    <span class="keyword">elif</span> ndim &gt; <span class="number">2</span>:</span><br><span class="line">        e = K.exp(x - K.<span class="built_in">max</span>(x, axis=axis, keepdims=<span class="literal">True</span>))</span><br><span class="line">        s = K.<span class="built_in">sum</span>(e, axis=axis, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> e / s</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Cannot apply softmax to a tensor that is 1D&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h2><p><img src="/images/15134156685588.jpg"></p>
<p>tanh 是sigmoid的变形： $tanh(x)=2sigmoid(2x)-1$，与 sigmoid 不同的是，tanh 是0均值的。因此，实际应用中，tanh 会比 sigmoid 更好一些，</p>
<p><img src="/images/15134156615077.jpg"></p>
<h2 id="ReLU家族"><a href="#ReLU家族" class="headerlink" title="ReLU家族"></a>ReLU家族</h2><p>然而标准ReLU不是完美的，比如因为ReLU在小于0的坐标梯度都是0，那么会造成“死亡”的神经元的问题：一旦神经元的输入与权重之乘积是负的，那么经过ReLU的激活，输出就是0，而ReLU的0梯度让“死亡”的神经元无法“复活”：没办法回到输出不是0的状态，这样就出现了许多在ReLU的变种，一般都是对标准ReLU坐标轴左边的部分做文章，比如<strong>leaky ReLU</strong>。其公式就是$LeakyReLU_ α (z) = max(\alpha z,z)$。如图，<br><img src="/images/15134123600394.jpg"></p>
<p>这篇文章<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1505.00853.pdf">Empirical Evaluation of Rectified Activations in Convolution Network</a>对比了几种leaky ReLU，比如把$\alpha$设置为0.2效果总是好过0.01，并且，对于randomized leaky ReLU (RReLU)（其中$\alpha$设置为一个在指定范围内的随机数），效果也不错，而且还具有一定的正则作用。另外，对于parametric leaky ReLU (PReLU)（其中$\alpha$作为网络的一个参数，被反向传播学习出来，之前的$\alpha$都是超参数，不能学只能调节），这种变种对于大数据集不错，但是数据量过小就有过拟合的风险。以下是Keras里面relu的代码，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span>(<span class="params">x, alpha=<span class="number">0.</span>, max_value=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Rectified linear unit.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    With default values, it returns element-wise `max(x, 0)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Arguments</span></span><br><span class="line"><span class="string">        x: A tensor or variable.</span></span><br><span class="line"><span class="string">        alpha: A scalar, slope of negative section (default=`0.`).</span></span><br><span class="line"><span class="string">        max_value: Saturation threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Returns</span></span><br><span class="line"><span class="string">        A tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> alpha != <span class="number">0.</span>:</span><br><span class="line">        negative_part = tf.nn.relu(-x)</span><br><span class="line">    x = tf.nn.relu(x)</span><br><span class="line">    <span class="keyword">if</span> max_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        max_value = _to_tensor(max_value, x.dtype.base_dtype)</span><br><span class="line">        zero = _to_tensor(<span class="number">0.</span>, x.dtype.base_dtype)</span><br><span class="line">        x = tf.clip_by_value(x, zero, max_value)</span><br><span class="line">    <span class="keyword">if</span> alpha != <span class="number">0.</span>:</span><br><span class="line">        alpha = _to_tensor(alpha, x.dtype.base_dtype)</span><br><span class="line">        x -= alpha * negative_part</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>另外，在这篇文章里面<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.07289v5.pdf">FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)</a>，引入了一种新的ReLU，exponential linear unit (ELU)，公式如下，<br>$$<br>ELU_{\alpha}(z) = \alpha (\exp(z)-1) \ if \ z \lt 0 ; \ z \ if \ z \gt 0;<br>$$<br><img src="/images/15134129990682.jpg"></p>
<p>与标准ReLU最大的区别在于它处处连续可导，这使得梯度下降得到加速，收敛得到了加速，而使用了指数函数使得其测试阶段的计算代价更高。Keras里elu的实现，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">elu</span>(<span class="params">x, alpha=<span class="number">1.</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Exponential linear unit.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Arguments</span></span><br><span class="line"><span class="string">        x: A tenor or variable to compute the activation function for.</span></span><br><span class="line"><span class="string">        alpha: A scalar, slope of positive section.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Returns</span></span><br><span class="line"><span class="string">        A tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    res = tf.nn.elu(x)</span><br><span class="line">    <span class="keyword">if</span> alpha == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> tf.where(x &gt; <span class="number">0</span>, res, alpha * res)</span><br></pre></td></tr></table></figure>
<h2 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a>激活函数的选择</h2><p>一般来说，我们的选择顺序可以理解为：<br>ELU &gt; leaky ReLU (以及其变种) &gt; ReLU &gt; tanh &gt; logistic。但是，</p>
<ul>
<li>如果我们更顾虑模型运行速度，那么leaky ReLU可能比ELU更好；</li>
<li>如果我们不想调节超参数，那么用默认的$\alpha$就行，ReLU和ELU的分别是0.01和1； </li>
<li>如果算力足够可以用来调参，那么如果网络过拟合我们会选择RReLU，如果训练集数据足够多，那可以用PReLU。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://frankchen.xyz/2017/12/15/Neural-Network-Terms-Introduction-to-Optimization-Functions-Activation-Functions-Loss-Functions-and-Regular-Functions/" data-id="cktiql5dl002r6e8zc46j4poo" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithm/" rel="tag">Algorithm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Data-Science/" rel="tag">Data Science</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/12/18/How-to-Use-Word-Embedding-Layers-for-Deep-Learning-with-Keras/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          深度学习中Keras中的Embedding层的理解与使用
        
      </div>
    </a>
  
  
    <a href="/2017/12/12/Understanding-the-axis-parameter-in-Pandas-and-Numpy/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">如何理解Pandas 和 Numpy里的axis</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Vim/">Vim</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/note/">note</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tutorial/">tutorial</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/DS-Store/" rel="tag">.DS_Store</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/" rel="tag">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/" rel="tag">C++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Coursera/" rel="tag">Coursera</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNN/" rel="tag">DNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Science/" rel="tag">Data Science</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Structure/" rel="tag">Data Structure</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Funny/" rel="tag">Funny</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Go/" rel="tag">Go</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hbase/" rel="tag">Hbase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Interview/" rel="tag">Interview</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/" rel="tag">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jupyter-Notebook/" rel="tag">Jupyter Notebook</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kaggle/" rel="tag">Kaggle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keras/" rel="tag">Keras</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LDA/" rel="tag">LDA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/" rel="tag">LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Life/" rel="tag">Life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mac/" rel="tag">Mac</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Note/" rel="tag">Note</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Numpy/" rel="tag">Numpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OOP/" rel="tag">OOP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Old-Driver/" rel="tag">Old Driver</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/" rel="tag">Pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyQt5/" rel="tag">PyQt5</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pycharm/" rel="tag">Pycharm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recommendation-System/" rel="tag">Recommendation System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/" rel="tag">SQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TensorFlow/" rel="tag">TensorFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tex/" rel="tag">Tex</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Time-Management/" rel="tag">Time Management</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VPN/" rel="tag">VPN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/amazon/" rel="tag">amazon</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bash/" rel="tag">bash</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/chicken-soup/" rel="tag">chicken-soup</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/life/" rel="tag">life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/" rel="tag">nginx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/" rel="tag">pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/re/" rel="tag">re</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/" rel="tag">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/time-series/" rel="tag">time series</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tips/" rel="tag">tips</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tutorial/" rel="tag">tutorial</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%90%E7%BB%B4/" rel="tag">运维</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/DS-Store/" style="font-size: 10px;">.DS_Store</a> <a href="/tags/Algorithm/" style="font-size: 15.71px;">Algorithm</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/Coursera/" style="font-size: 10px;">Coursera</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/Data-Science/" style="font-size: 17.14px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 10px;">Data Structure</a> <a href="/tags/Deep-Learning/" style="font-size: 17.14px;">Deep Learning</a> <a href="/tags/Docker/" style="font-size: 11.43px;">Docker</a> <a href="/tags/Funny/" style="font-size: 14.29px;">Funny</a> <a href="/tags/Go/" style="font-size: 10px;">Go</a> <a href="/tags/Hadoop/" style="font-size: 11.43px;">Hadoop</a> <a href="/tags/Hbase/" style="font-size: 11.43px;">Hbase</a> <a href="/tags/Hive/" style="font-size: 10px;">Hive</a> <a href="/tags/Interview/" style="font-size: 10px;">Interview</a> <a href="/tags/Java/" style="font-size: 12.86px;">Java</a> <a href="/tags/Jupyter-Notebook/" style="font-size: 10px;">Jupyter Notebook</a> <a href="/tags/Kaggle/" style="font-size: 10px;">Kaggle</a> <a href="/tags/Keras/" style="font-size: 11.43px;">Keras</a> <a href="/tags/LDA/" style="font-size: 10px;">LDA</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Life/" style="font-size: 10px;">Life</a> <a href="/tags/Linux/" style="font-size: 18.57px;">Linux</a> <a href="/tags/Mac/" style="font-size: 14.29px;">Mac</a> <a href="/tags/Machine-Learning/" style="font-size: 12.86px;">Machine Learning</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/Note/" style="font-size: 10px;">Note</a> <a href="/tags/Numpy/" style="font-size: 11.43px;">Numpy</a> <a href="/tags/OOP/" style="font-size: 10px;">OOP</a> <a href="/tags/Old-Driver/" style="font-size: 11.43px;">Old Driver</a> <a href="/tags/Pandas/" style="font-size: 12.86px;">Pandas</a> <a href="/tags/PyQt5/" style="font-size: 10px;">PyQt5</a> <a href="/tags/Pycharm/" style="font-size: 10px;">Pycharm</a> <a href="/tags/Python/" style="font-size: 11.43px;">Python</a> <a href="/tags/RNN/" style="font-size: 14.29px;">RNN</a> <a href="/tags/Recommendation-System/" style="font-size: 10px;">Recommendation System</a> <a href="/tags/SQL/" style="font-size: 11.43px;">SQL</a> <a href="/tags/Spark/" style="font-size: 10px;">Spark</a> <a href="/tags/TensorFlow/" style="font-size: 12.86px;">TensorFlow</a> <a href="/tags/Tex/" style="font-size: 10px;">Tex</a> <a href="/tags/Time-Management/" style="font-size: 10px;">Time Management</a> <a href="/tags/VPN/" style="font-size: 10px;">VPN</a> <a href="/tags/amazon/" style="font-size: 10px;">amazon</a> <a href="/tags/bash/" style="font-size: 10px;">bash</a> <a href="/tags/chicken-soup/" style="font-size: 11.43px;">chicken-soup</a> <a href="/tags/git/" style="font-size: 11.43px;">git</a> <a href="/tags/life/" style="font-size: 10px;">life</a> <a href="/tags/nginx/" style="font-size: 10px;">nginx</a> <a href="/tags/pandas/" style="font-size: 10px;">pandas</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/re/" style="font-size: 10px;">re</a> <a href="/tags/scala/" style="font-size: 10px;">scala</a> <a href="/tags/spark/" style="font-size: 11.43px;">spark</a> <a href="/tags/time-series/" style="font-size: 10px;">time series</a> <a href="/tags/tips/" style="font-size: 10px;">tips</a> <a href="/tags/tutorial/" style="font-size: 12.86px;">tutorial</a> <a href="/tags/%E8%BF%90%E7%BB%B4/" style="font-size: 11.43px;">运维</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/10/24/amazon-vat-cal/">用python 自动计算亚马逊vat税款</a>
          </li>
        
          <li>
            <a href="/2018/12/20/attachments/">python 发送带各种附件的邮件示例</a>
          </li>
        
          <li>
            <a href="/2018/09/11/Pyqt5-GUI-package-on-Windows/">PyQt5 编写并在Windows上用Cx_Freeze打包GUI程序</a>
          </li>
        
          <li>
            <a href="/2018/08/22/learning-from-donald-trump/">向特朗普同志学习</a>
          </li>
        
          <li>
            <a href="/2018/07/20/numpy-mask/">Numpy中的mask</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 江南消夏<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div>
</body>
</html>