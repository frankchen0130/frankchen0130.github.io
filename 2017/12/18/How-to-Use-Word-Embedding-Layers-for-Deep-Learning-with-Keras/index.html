<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"frankchen.xyz","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="单词嵌入提供了单词的密集表示及其相对含义，它们是对简单包模型表示中使用的稀疏表示的改进，可以从文本数据中学习字嵌入，并在项目之间重复使用。它们也可以作为拟合文本数据的神经网络的一部分来学习。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习中Keras中的Embedding层的理解与使用">
<meta property="og:url" content="https://frankchen.xyz/2017/12/18/How-to-Use-Word-Embedding-Layers-for-Deep-Learning-with-Keras/index.html">
<meta property="og:site_name" content="不正经数据科学家">
<meta property="og:description" content="单词嵌入提供了单词的密集表示及其相对含义，它们是对简单包模型表示中使用的稀疏表示的改进，可以从文本数据中学习字嵌入，并在项目之间重复使用。它们也可以作为拟合文本数据的神经网络的一部分来学习。">
<meta property="og:locale">
<meta property="og:image" content="https://frankchen.xyz/images/15135840798621.jpg">
<meta property="og:image" content="https://frankchen.xyz/images/15204978301733.jpg">
<meta property="article:published_time" content="2017-12-18T07:59:41.000Z">
<meta property="article:modified_time" content="2018-03-08T08:44:07.000Z">
<meta property="article:author" content="frankchen0130">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Keras">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://frankchen.xyz/images/15135840798621.jpg">

<link rel="canonical" href="https://frankchen.xyz/2017/12/18/How-to-Use-Word-Embedding-Layers-for-Deep-Learning-with-Keras/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>深度学习中Keras中的Embedding层的理解与使用 | 不正经数据科学家</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="不正经数据科学家" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">不正经数据科学家</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Marketing/Python/Clojure/Game</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://frankchen.xyz/2017/12/18/How-to-Use-Word-Embedding-Layers-for-Deep-Learning-with-Keras/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="frankchen0130">
      <meta itemprop="description" content="Marketing/Python/Clojure/Game">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不正经数据科学家">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习中Keras中的Embedding层的理解与使用
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-12-18 15:59:41" itemprop="dateCreated datePublished" datetime="2017-12-18T15:59:41+08:00">2017-12-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-03-08 16:44:07" itemprop="dateModified" datetime="2018-03-08T16:44:07+08:00">2018-03-08</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/images/15135840798621.jpg"><br>单词嵌入提供了单词的密集表示及其相对含义，它们是对简单包模型表示中使用的稀疏表示的改进，可以从文本数据中学习字嵌入，并在项目之间重复使用。它们也可以作为拟合文本数据的神经网络的一部分来学习。</p>
<span id="more"></span>

<h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><p>单词嵌入是使用密集的矢量表示来表示单词和文档的一类方法。</p>
<p>词嵌入是对传统的词袋模型编码方案的改进，传统方法使用大而稀疏的矢量来表示每个单词或者在矢量内对每个单词进行评分以表示整个词汇表，这些表示是稀疏的，因为每个词汇的表示是巨大的，给定的词或文档主要由零值组成的大向量表示。</p>
<p>相反，在嵌入中，单词由密集向量表示，其中向量表示将单词投影到连续向量空间中。</p>
<p>向量空间中的单词的位置是从文本中学习的，并且基于在使用单词时围绕单词的单词。</p>
<p>学习到的向量空间中的单词的位置被称为它的嵌入：Embedding。</p>
<p>从文本学习单词嵌入方法的两个流行例子包括：</p>
<ul>
<li>Word2Vec.</li>
<li>GloVe.</li>
</ul>
<p>除了这些精心设计的方法之外，还可以将词嵌入学习作为深度学习模型的一部分。这可能是一个较慢的方法，但可以通过这样为特定数据集定制模型。</p>
<h2 id="Keras-Embedding-Layer"><a href="#Keras-Embedding-Layer" class="headerlink" title="Keras Embedding Layer"></a>Keras Embedding Layer</h2><p>Keras提供了一个嵌入层，适用于文本数据的神经网络。</p>
<p>它要求输入数据是整数编码的，所以每个字都用一个唯一的整数表示。这个数据准备步骤可以使用Keras提供的Tokenizer API来执行。</p>
<p>嵌入层用随机权重进行初始化，并将学习训练数据集中所有单词的嵌入。</p>
<p>它是一个灵活的图层，可以以多种方式使用，例如：</p>
<ul>
<li>它可以单独使用来学习一个单词嵌入，以后可以保存并在另一个模型中使用。</li>
<li>它可以用作深度学习模型的一部分，其中嵌入与模型本身一起学习。</li>
<li>它可以用来加载预先训练的词嵌入模型，这是一种迁移学习。</li>
</ul>
<p>嵌入层被定义为网络的第一个隐藏层。它必须指定3个参数：</p>
<ul>
<li>input_dim：这是文本数据中词汇的取值可能数。例如，如果您的数据是整数编码为0-9之间的值，那么词汇的大小就是10个单词；</li>
<li>output_dim：这是嵌入单词的向量空间的大小。它为每个单词定义了这个层的输出向量的大小。例如，它可能是32或100甚至更大，可以视为具体问题的超参数；</li>
<li>input_length：这是输入序列的长度，就像您为Keras模型的任何输入层所定义的一样，也就是一次输入带有的词汇个数。例如，如果您的所有输入文档都由1000个字组成，那么input_length就是1000。</li>
</ul>
<p>例如，下面我们定义一个词汇表为200的嵌入层（例如从0到199的整数编码的字，包括0到199），一个32维的向量空间，其中将嵌入单词，以及输入文档，每个单词有50个单词。</p>
<p><code>e = Embedding(input_dim=200, output_dim=32, input_length=50)</code></p>
<p>嵌入层自带学习的权重，如果将模型保存到文件中，则将包含嵌入图层的权重。</p>
<p>嵌入层的输出是一个二维向量，每个单词在输入文本（输入文档）序列中嵌入一个。</p>
<p>如果您希望直接将Dense层接到Embedding层后面，则必须先使用Flatten层将Embedding层的2D输出矩阵平铺为一维矢量。</p>
<p>现在，让我们看看我们如何在实践中使用嵌入层。</p>
<h2 id="学习-Embedding的例子"><a href="#学习-Embedding的例子" class="headerlink" title="学习 Embedding的例子"></a>学习 Embedding的例子</h2><p>在本节中，我们将看看如何在文本分类问题上拟合神经网络的同时学习单词嵌入。</p>
<p>我们将定义一个小问题，我们有10个文本文档，每个文档都有一个学生提交的工作评论。每个文本文档被分类为正的“1”或负的“0”。这是一个简单的情感分析问题。</p>
<p>首先，我们将定义文档及其类别标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define documents 定义文档</span></span><br><span class="line">docs = [<span class="string">&#x27;Well done!&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Good work&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Great effort&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;nice work&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Excellent!&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Weak&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Poor effort!&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;not good&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;poor work&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Could have done better.&#x27;</span>]</span><br><span class="line"><span class="comment"># define class labels 定义分类标签</span></span><br><span class="line">labels = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>接下来，我们来整数编码每个文件。这意味着把输入，嵌入层将具有整数序列。我们可以尝试其他更复杂的bag of word 模型比如计数或TF-IDF。</p>
<p>Keras提供<a target="_blank" rel="noopener" href="https://keras.io/preprocessing/text/#one_hot">one_hot()</a>函数来创建每个单词的散列作为一个有效的整数编码。我们用估计50的词汇表大小，这大大减少了hash函数的冲突概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># integer encode the documents 独热编码</span></span><br><span class="line">vocab_size = <span class="number">50</span></span><br><span class="line">encoded_docs = [one_hot(d, vocab_size) <span class="keyword">for</span> d <span class="keyword">in</span> docs]</span><br><span class="line"><span class="built_in">print</span>(encoded_docs)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[6, 16], [42, 24], [2, 17], [42, 24], [18], [17], [22, 17], [27, 42], [22, 24], [49, 46, 16, 34]]</span><br></pre></td></tr></table></figure>
<p>这样以后序列具有不同的长度，但是Keras更喜欢输入矢量化和所有输入具有相同的长度。我们将填充所有输入序列的长度为4，同样，我们可以使用内置的Keras函数（在这种情况下为<a target="_blank" rel="noopener" href="https://keras.io/preprocessing/sequence/#pad_sequences">pad_sequences()</a>函数）执行此操作,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pad documents to a max length of 4 words 将不足长度的用0填充为长度4</span></span><br><span class="line">max_length = <span class="number">4</span></span><br><span class="line">padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(padded_docs)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[ 6 16  0  0]</span><br><span class="line"> [42 24  0  0]</span><br><span class="line"> [ 2 17  0  0]</span><br><span class="line"> [42 24  0  0]</span><br><span class="line"> [18  0  0  0]</span><br><span class="line"> [17  0  0  0]</span><br><span class="line"> [22 17  0  0]</span><br><span class="line"> [27 42  0  0]</span><br><span class="line"> [22 24  0  0]</span><br><span class="line"> [49 46 16 34]]</span><br></pre></td></tr></table></figure>
<p>我们现在准备将我们的嵌入层定义为我们的神经网络模型的一部分。</p>
<p>嵌入的词汇量为50，输入长度为4，我们将选择一个8维的嵌入空间。</p>
<p>该模型是一个简单的二元分类模型。重要的是，嵌入层的输出将是每个8维的4个矢量，每个单词一个。我们将其平铺到一个32个元素的向量上以传递到密集输出层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define the model 定义模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(vocab_size, <span class="number">8</span>, input_length=max_length))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"><span class="comment"># compile the model 编译</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line"><span class="comment"># summarize the model 打印模型信息</span></span><br><span class="line"><span class="built_in">print</span>(model.summary())</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #</span><br><span class="line">=================================================================</span><br><span class="line">embedding_1 (Embedding)      (None, 4, 8)              400</span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten)          (None, 32)                0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 1)                 33</span><br><span class="line">=================================================================</span><br><span class="line">Total params: 433</span><br><span class="line">Trainable params: 433</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>

<p>最后，我们可以拟合和评估分类模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fit the model 拟合</span></span><br><span class="line">model.fit(padded_docs, labels, epochs=<span class="number">50</span>, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># evaluate the model 评估</span></span><br><span class="line">loss, accuracy = model.evaluate(padded_docs, labels, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %f&#x27;</span> % (accuracy*<span class="number">100</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 100.000000</span><br></pre></td></tr></table></figure>
<p>下面是完整的代码，这里我们用函数式API改写了模型定义，不过结构和上面是完全一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Flatten, Input</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> one_hot</span><br><span class="line"></span><br><span class="line"><span class="comment"># define documents</span></span><br><span class="line">docs = [<span class="string">&#x27;Well done!&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Good work&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Great effort&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;nice work&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Excellent!&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Weak&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Poor effort!&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;not good&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;poor work&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Could have done better.&#x27;</span>]</span><br><span class="line"><span class="comment"># define class labels</span></span><br><span class="line">labels = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="comment"># integer encode the documents</span></span><br><span class="line">vocab_size = <span class="number">50</span></span><br><span class="line">encoded_docs = [one_hot(d, vocab_size) <span class="keyword">for</span> d <span class="keyword">in</span> docs]</span><br><span class="line"><span class="built_in">print</span>(encoded_docs)</span><br><span class="line"><span class="comment"># pad documents to a max length of 4 words</span></span><br><span class="line">max_length = <span class="number">4</span></span><br><span class="line">padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(padded_docs)</span><br><span class="line"><span class="comment"># define the model</span></span><br><span class="line"><span class="built_in">input</span> = Input(shape=(<span class="number">4</span>, ))</span><br><span class="line">x = Embedding(vocab_size, <span class="number">8</span>, input_length=max_length)(<span class="built_in">input</span>)</span><br><span class="line">x = Flatten()(x)</span><br><span class="line">x = Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)(x)</span><br><span class="line">model = Model(inputs=<span class="built_in">input</span>, outputs=x)</span><br><span class="line"><span class="comment"># compile the model</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line"><span class="comment"># summarize the model</span></span><br><span class="line"><span class="built_in">print</span>(model.summary())</span><br><span class="line"><span class="comment"># fit the model</span></span><br><span class="line">model.fit(padded_docs, labels, epochs=<span class="number">50</span>, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># evaluate the model</span></span><br><span class="line">loss, accuracy = model.evaluate(padded_docs, labels, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %f&#x27;</span> % (accuracy * <span class="number">100</span>))</span><br></pre></td></tr></table></figure>


<p>之后，我们可以将嵌入图层中学习的权重保存到文件中，以便以后在其他模型中使用。</p>
<p>通常也可以使用这个模型来分类在测试数据集中看到的同类词汇的其他文档。</p>
<p>接下来，让我们看看在Keras中加载预先训练的词嵌入。</p>
<h2 id="使用预训练GloVE嵌入的示例"><a href="#使用预训练GloVE嵌入的示例" class="headerlink" title="使用预训练GloVE嵌入的示例"></a>使用预训练GloVE嵌入的示例</h2><p>Keras嵌入层也可以使用在其他地方学习的嵌入字。</p>
<p>在自然语言处理领域，学习，保存和分享提供词嵌入是很常见的。</p>
<p>例如，GloVe方法背后的研究人员提供了一套在公共领域许可下发布的预先训练的词嵌入。看到：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://nlp.stanford.edu/projects/glove/">GloVe: Global Vectors for Word Representation</a></li>
</ul>
<p>最小的包是822Mb，叫做“glove.6B.zip”。它训练了10亿个词汇（单词）的数据集，词汇量为40万字，有几种不同的嵌入矢量尺寸，包括50,100,200和300size。</p>
<p>您可以下载这个嵌入的集合，可以作为Keras嵌入层中训练数据集中的单词预先训练嵌入的权重。</p>
<p>这个例子受Keras项目中的一个例子的启发：<a target="_blank" rel="noopener" href="https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py">pretrained_word_embeddings.py</a>。</p>
<p>下载并解压缩后，您将看到几个文件，其中一个是“glove.6B.100d.txt”，其中包含一个100维版本的嵌入。</p>
<p>如果你在文件内部偷看，你会看到一个token（单词），后面是每行的权重（100个数字）。例如，下面是嵌入的ASCII文本文件的第一行，显示“the”的嵌入。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062</span><br></pre></td></tr></table></figure>

<p>如前一节所述，第一步是定义这些示例，将它们编码为整数，然后将这些序列填充为相同的长度。</p>
<p>在这种情况下，我们需要能够将单词映射到整数以及整数到单词。</p>
<p>Keras提供了一个<a target="_blank" rel="noopener" href="https://keras.io/preprocessing/text/#tokenizer">Tokenizer</a>类，可以适应训练数据，通过调用Tokenizer类的texts_to_sequences（）方法，可以一致地将文本转换为序列，并且可以访问单词在word_index属性中的整数字典映射。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define documents</span></span><br><span class="line">docs = [<span class="string">&#x27;Well done!&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Good work&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Great effort&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;nice work&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Excellent!&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Weak&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Poor effort!&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;not good&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;poor work&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Could have done better.&#x27;</span>]</span><br><span class="line"><span class="comment"># define class labels</span></span><br><span class="line">labels = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line"><span class="comment"># prepare tokenizer</span></span><br><span class="line">t = Tokenizer()</span><br><span class="line">t.fit_on_texts(docs)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(t.word_index) + <span class="number">1</span></span><br><span class="line"><span class="comment"># integer encode the documents</span></span><br><span class="line">encoded_docs = t.texts_to_sequences(docs)</span><br><span class="line"><span class="built_in">print</span>(encoded_docs)</span><br><span class="line"><span class="comment"># pad documents to a max length of 4 words</span></span><br><span class="line">max_length = <span class="number">4</span></span><br><span class="line">padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(padded_docs)</span><br></pre></td></tr></table></figure>
<p>接下来，我们需要将整个Glove字嵌入文件作为字的字典加载到内存中以嵌入数组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load the whole embedding into memory</span></span><br><span class="line">embeddings_index = <span class="built_in">dict</span>()</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;glove.6B.100d.txt&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">	values = line.split()</span><br><span class="line">	word = values[<span class="number">0</span>]</span><br><span class="line">	coefs = asarray(values[<span class="number">1</span>:], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">	embeddings_index[word] = coefs</span><br><span class="line">f.close()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Loaded %s word vectors.&#x27;</span> % <span class="built_in">len</span>(embeddings_index))</span><br></pre></td></tr></table></figure>
<p>这很慢。在训练数据中过滤特殊字词的嵌入可能会更好。</p>
<p>接下来，我们需要为训练数据集中的每个单词创建一个嵌入矩阵。我们可以通过枚举Tokenizer.word_index中的所有唯一单词并从加载的GloVe嵌入中找到嵌入权重向量来实现这一点。</p>
<p>结果是一个仅用于训练期间将会看到的单词的权重矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a weight matrix for words in training docs</span></span><br><span class="line">embedding_matrix = zeros((vocab_size, <span class="number">100</span>))</span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> t.word_index.items():</span><br><span class="line">	embedding_vector = embeddings_index.get(word)</span><br><span class="line">	<span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">		embedding_matrix[i] = embedding_vector</span><br></pre></td></tr></table></figure>

<p>现在我们可以像以前一样定义我们的模型，并进行评估。</p>
<p>关键的区别是嵌入层可以用GloVe字嵌入权重来播种。我们选择了100维版本，因此必须使用output_dim将其设置为100来定义嵌入层。最后，我们不希望更新此模型中的学习单词权重，因此我们将设置模型的可训练属性为False 。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)</span><br></pre></td></tr></table></figure>
<p>下面列出了完整的工作示例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> asarray</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> zeros</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="comment"># define documents</span></span><br><span class="line">docs = [<span class="string">&#x27;Well done!&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Good work&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Great effort&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;nice work&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Excellent!&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Weak&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Poor effort!&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;not good&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;poor work&#x27;</span>,</span><br><span class="line">		<span class="string">&#x27;Could have done better.&#x27;</span>]</span><br><span class="line"><span class="comment"># define class labels</span></span><br><span class="line">labels = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line"><span class="comment"># prepare tokenizer</span></span><br><span class="line">t = Tokenizer()</span><br><span class="line">t.fit_on_texts(docs)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(t.word_index) + <span class="number">1</span></span><br><span class="line"><span class="comment"># integer encode the documents</span></span><br><span class="line">encoded_docs = t.texts_to_sequences(docs)</span><br><span class="line"><span class="built_in">print</span>(encoded_docs)</span><br><span class="line"><span class="comment"># pad documents to a max length of 4 words</span></span><br><span class="line">max_length = <span class="number">4</span></span><br><span class="line">padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(padded_docs)</span><br><span class="line"><span class="comment"># load the whole embedding into memory</span></span><br><span class="line">embeddings_index = <span class="built_in">dict</span>()</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;../glove_data/glove.6B/glove.6B.100d.txt&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">	values = line.split()</span><br><span class="line">	word = values[<span class="number">0</span>]</span><br><span class="line">	coefs = asarray(values[<span class="number">1</span>:], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">	embeddings_index[word] = coefs</span><br><span class="line">f.close()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Loaded %s word vectors.&#x27;</span> % <span class="built_in">len</span>(embeddings_index))</span><br><span class="line"><span class="comment"># create a weight matrix for words in training docs</span></span><br><span class="line">embedding_matrix = zeros((vocab_size, <span class="number">100</span>))</span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> t.word_index.items():</span><br><span class="line">	embedding_vector = embeddings_index.get(word)</span><br><span class="line">	<span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">		embedding_matrix[i] = embedding_vector</span><br><span class="line"><span class="comment"># define model</span></span><br><span class="line">model = Sequential()</span><br><span class="line">e = Embedding(vocab_size, <span class="number">100</span>, weights=[embedding_matrix], input_length=<span class="number">4</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">model.add(e)</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"><span class="comment"># compile the model</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line"><span class="comment"># summarize the model</span></span><br><span class="line"><span class="built_in">print</span>(model.summary())</span><br><span class="line"><span class="comment"># fit the model</span></span><br><span class="line">model.fit(padded_docs, labels, epochs=<span class="number">50</span>, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># evaluate the model</span></span><br><span class="line">loss, accuracy = model.evaluate(padded_docs, labels, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %f&#x27;</span> % (accuracy*<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<p>运行这个例子可能需要更长的时间，但是这表明它能够适应这个简单的问题。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]</span><br><span class="line"></span><br><span class="line">[[ 6  2  0  0]</span><br><span class="line"> [ 3  1  0  0]</span><br><span class="line"> [ 7  4  0  0]</span><br><span class="line"> [ 8  1  0  0]</span><br><span class="line"> [ 9  0  0  0]</span><br><span class="line"> [10  0  0  0]</span><br><span class="line"> [ 5  4  0  0]</span><br><span class="line"> [11  3  0  0]</span><br><span class="line"> [ 5  1  0  0]</span><br><span class="line"> [12 13  2 14]]</span><br><span class="line"></span><br><span class="line">Loaded 400000 word vectors.</span><br><span class="line"></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #</span><br><span class="line">=================================================================</span><br><span class="line">embedding_1 (Embedding)      (None, 4, 100)            1500</span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten)          (None, 400)               0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 1)                 401</span><br><span class="line">=================================================================</span><br><span class="line">Total params: 1,901</span><br><span class="line">Trainable params: 401</span><br><span class="line">Non-trainable params: 1,500</span><br><span class="line">_________________________________________________________________</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: 100.000000</span><br></pre></td></tr></table></figure>
<p>在实践中，最好还是尝试使用预先训练好的嵌入来学习单词嵌入，因为它是固定的，并尝试在预先训练好的嵌入之上进行学习，这就类似于计算机视觉里面用预训练的VGG或者res-net迁移具体问题那样。</p>
<p>不过这取决于什么最适合你的具体问题。</p>
<h2 id="IMDB-数据集Embedding实例"><a href="#IMDB-数据集Embedding实例" class="headerlink" title="IMDB 数据集Embedding实例"></a>IMDB 数据集Embedding实例</h2><p><img src="/images/15204978301733.jpg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential,Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Flatten, Dense, Embedding, Input</span><br><span class="line"></span><br><span class="line">input_layer = Input(shape=(maxlen,)) </span><br><span class="line">x = Embedding(input_dim=<span class="number">10000</span>,output_dim=<span class="number">8</span>)(input_layer)</span><br><span class="line"><span class="comment"># 单独做一个embedding模型，利于后面观察</span></span><br><span class="line">embedding = Model(input_layer,x)</span><br><span class="line">x = Flatten()(x)</span><br><span class="line">x = Dense(<span class="number">1</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)(x)</span><br><span class="line">model = Model(input_layer,x)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;rmsprop&#x27;</span>,loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line">model.summary()</span><br><span class="line">history = modhistory = modhistory = mod&gt; history = model.fit(x_train,y_train,epochs=<span class="number">10</span>,batch_size=<span class="number">32</span>,validation_split=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">input_4 (InputLayer)         (<span class="literal">None</span>, <span class="number">20</span>)                <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">embedding_5 (Embedding)      (<span class="literal">None</span>, <span class="number">20</span>, <span class="number">8</span>)             <span class="number">80000</span>     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_5 (Flatten)          (<span class="literal">None</span>, <span class="number">160</span>)               <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_5 (Dense)              (<span class="literal">None</span>, <span class="number">1</span>)                 <span class="number">161</span>       </span><br><span class="line">=================================================================</span><br><span class="line"></span><br><span class="line">Total params: <span class="number">80</span>,<span class="number">161</span></span><br><span class="line">Trainable params: <span class="number">80</span>,<span class="number">161</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Train on <span class="number">20000</span> samples, validate on <span class="number">5000</span> samples</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">10</span></span><br><span class="line"><span class="number">20000</span>/<span class="number">20000</span> [==============================] - 2s 105us/step - loss: <span class="number">0.6772</span> - acc: <span class="number">0.6006</span> - val_loss: <span class="number">0.6448</span> - val_acc: <span class="number">0.6704</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">10</span></span><br><span class="line"><span class="number">20000</span>/<span class="number">20000</span> [==============================] - 2s 93us/step - loss: <span class="number">0.5830</span> - acc: <span class="number">0.7188</span> - val_loss: <span class="number">0.5629</span> - val_acc: <span class="number">0.7046</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">10</span></span><br><span class="line"><span class="number">20000</span>/<span class="number">20000</span> [==============================] - 2s 95us/step - loss: <span class="number">0.5152</span> - acc: <span class="number">0.7464</span> - val_loss: <span class="number">0.5362</span> - val_acc: <span class="number">0.7208</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">10</span></span><br><span class="line"><span class="number">20000</span>/<span class="number">20000</span> [==============================] - 2s 93us/step - loss: <span class="number">0.4879</span> - acc: <span class="number">0.7607</span> - val_loss: <span class="number">0.5299</span> - val_acc: <span class="number">0.7292</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">10</span></span><br><span class="line"><span class="number">20000</span>/<span class="number">20000</span> [==============================] - 2s 97us/step - loss: <span class="number">0.4731</span> - acc: <span class="number">0.7694</span> - val_loss: <span class="number">0.5290</span> - val_acc: <span class="number">0.7334</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">10</span></span><br><span class="line"><span class="number">20000</span>/<span class="number">20000</span> [==============================] - 2s 98us/step - loss: <span class="number">0.4633</span> - acc: <span class="number">0.7773</span> - val_loss: <span class="number">0.5317</span> - val_acc: <span class="number">0.7344</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">10</span></span><br><span class="line"><span class="number">20000</span>/<span class="number">20000</span> [==============================] - 2s 96us/step - loss: <span class="number">0.4548</span> - acc: <span class="number">0.7819</span> - val_loss: <span class="number">0.5333</span> - val_acc: <span class="number">0.7318</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">10</span></span><br><span class="line"><span class="number">20000</span>/<span class="number">20000</span> [==============================] - 2s 93us/step - loss: <span class="number">0.4471</span> - acc: <span class="number">0.7870</span> - val_loss: <span class="number">0.5377</span> - val_acc: <span class="number">0.7288</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">10</span></span><br><span class="line"><span class="number">20000</span>/<span class="number">20000</span> [==============================] - 2s 95us/step - loss: <span class="number">0.4399</span> - acc: <span class="number">0.7924</span> - val_loss: <span class="number">0.5422</span> - val_acc: <span class="number">0.7278</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">10</span></span><br><span class="line"><span class="number">20000</span>/<span class="number">20000</span> [==============================] - 2s 90us/step - loss: <span class="number">0.4328</span> - acc: <span class="number">0.7957</span> - val_loss: <span class="number">0.5458</span> - val_acc: <span class="number">0.7290</span></span><br></pre></td></tr></table></figure>

<p>我们观察一下input的shape</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x_train[<span class="number">1</span>].shape</span><br><span class="line">x_train[<span class="number">1</span>]</span><br><span class="line">x_train[:<span class="number">1</span>].shape</span><br><span class="line">x_train[:<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">(<span class="number">20</span>,)</span><br><span class="line">array([ <span class="number">23</span>,   <span class="number">4</span>,   <span class="number">2</span>,  <span class="number">15</span>,  <span class="number">16</span>,   <span class="number">4</span>,   <span class="number">2</span>,   <span class="number">5</span>,  <span class="number">28</span>,   <span class="number">6</span>,  <span class="number">52</span>, <span class="number">154</span>, <span class="number">462</span>,</span><br><span class="line">        <span class="number">33</span>,  <span class="number">89</span>,  <span class="number">78</span>, <span class="number">285</span>,  <span class="number">16</span>, <span class="number">145</span>,  <span class="number">95</span>], dtype=int32)</span><br><span class="line">(<span class="number">1</span>, <span class="number">20</span>)</span><br><span class="line">array([[ <span class="number">65</span>,  <span class="number">16</span>,  <span class="number">38</span>,   <span class="number">2</span>,  <span class="number">88</span>,  <span class="number">12</span>,  <span class="number">16</span>, <span class="number">283</span>,   <span class="number">5</span>,  <span class="number">16</span>,   <span class="number">2</span>, <span class="number">113</span>, <span class="number">103</span>,</span><br><span class="line">         <span class="number">32</span>,  <span class="number">15</span>,  <span class="number">16</span>,   <span class="number">2</span>,  <span class="number">19</span>, <span class="number">178</span>,  <span class="number">32</span>]], dtype=int32)</span><br></pre></td></tr></table></figure>
<p>再看看embedding的输出，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">embedding.predict(x_train[:<span class="number">1</span>]).shape</span><br><span class="line">embedding.predict(x_train[:<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">(<span class="number">1</span>, <span class="number">20</span>, <span class="number">8</span>)</span><br><span class="line">array([[[-<span class="number">0.17401133</span>, -<span class="number">0.08743777</span>,  <span class="number">0.15631911</span>, -<span class="number">0.06831486</span>, -<span class="number">0.09105065</span>,</span><br><span class="line">          <span class="number">0.06253908</span>, -<span class="number">0.0798945</span> ,  <span class="number">0.07671431</span>],</span><br><span class="line">        [ <span class="number">0.18718374</span>,  <span class="number">0.10347525</span>, -<span class="number">0.06668846</span>,  <span class="number">0.25818944</span>,  <span class="number">0.07522523</span>,</span><br><span class="line">          <span class="number">0.07082067</span>,  <span class="number">0.05170904</span>,  <span class="number">0.22902426</span>],</span><br><span class="line">        [ <span class="number">0.06872956</span>, -<span class="number">0.00586612</span>,  <span class="number">0.07713806</span>, -<span class="number">0.00182899</span>,  <span class="number">0.00882899</span>,</span><br><span class="line">         -<span class="number">0.18892162</span>, -<span class="number">0.13580748</span>, -<span class="number">0.03166043</span>],</span><br><span class="line">        [-<span class="number">0.01912907</span>, -<span class="number">0.01732869</span>,  <span class="number">0.00391375</span>, -<span class="number">0.02338142</span>,  <span class="number">0.02787969</span>,</span><br><span class="line">         -<span class="number">0.02744135</span>,  <span class="number">0.0074541</span> ,  <span class="number">0.01806928</span>],</span><br><span class="line">        [ <span class="number">0.20604047</span>,  <span class="number">0.10910885</span>,  <span class="number">0.06304865</span>, -<span class="number">0.14038748</span>,  <span class="number">0.12123005</span>,</span><br><span class="line">          <span class="number">0.06124007</span>,  <span class="number">0.0532628</span> ,  <span class="number">0.17591232</span>],</span><br><span class="line">        [-<span class="number">0.19636872</span>, -<span class="number">0.0027669</span> ,  <span class="number">0.01087157</span>, -<span class="number">0.02332311</span>, -<span class="number">0.04321857</span>,</span><br><span class="line">         -<span class="number">0.09228673</span>, -<span class="number">0.03061322</span>, -<span class="number">0.13376454</span>],</span><br><span class="line">        [ <span class="number">0.18718374</span>,  <span class="number">0.10347525</span>, -<span class="number">0.06668846</span>,  <span class="number">0.25818944</span>,  <span class="number">0.07522523</span>,</span><br><span class="line">          <span class="number">0.07082067</span>,  <span class="number">0.05170904</span>,  <span class="number">0.22902426</span>],</span><br><span class="line">        [-<span class="number">0.27160701</span>, -<span class="number">0.29296583</span>,  <span class="number">0.1055108</span> ,  <span class="number">0.15896739</span>, -<span class="number">0.24833643</span>,</span><br><span class="line">         -<span class="number">0.17791845</span>, -<span class="number">0.27316946</span>, -<span class="number">0.241273</span>  ],</span><br><span class="line">        [-<span class="number">0.02175452</span>, -<span class="number">0.0839383</span> ,  <span class="number">0.04338101</span>,  <span class="number">0.01062139</span>, -<span class="number">0.11473208</span>,</span><br><span class="line">         -<span class="number">0.18394938</span>, -<span class="number">0.05141308</span>, -<span class="number">0.10405254</span>],</span><br><span class="line">        [ <span class="number">0.18718374</span>,  <span class="number">0.10347525</span>, -<span class="number">0.06668846</span>,  <span class="number">0.25818944</span>,  <span class="number">0.07522523</span>,</span><br><span class="line">          <span class="number">0.07082067</span>,  <span class="number">0.05170904</span>,  <span class="number">0.22902426</span>],</span><br><span class="line">        [-<span class="number">0.01912907</span>, -<span class="number">0.01732869</span>,  <span class="number">0.00391375</span>, -<span class="number">0.02338142</span>,  <span class="number">0.02787969</span>,</span><br><span class="line">         -<span class="number">0.02744135</span>,  <span class="number">0.0074541</span> ,  <span class="number">0.01806928</span>],</span><br><span class="line">        [-<span class="number">0.14751843</span>,  <span class="number">0.05572686</span>,  <span class="number">0.20332271</span>, -<span class="number">0.01759946</span>, -<span class="number">0.0946402</span> ,</span><br><span class="line">         -<span class="number">0.14416233</span>,  <span class="number">0.16961734</span>,  <span class="number">0.01381243</span>],</span><br><span class="line">        [ <span class="number">0.00282665</span>, -<span class="number">0.17532936</span>, -<span class="number">0.09342033</span>,  <span class="number">0.04514923</span>, -<span class="number">0.04684081</span>,</span><br><span class="line">          <span class="number">0.1748796</span> , -<span class="number">0.09669576</span>, -<span class="number">0.10699435</span>],</span><br><span class="line">        [ <span class="number">0.00225757</span>, -<span class="number">0.12751001</span>, -<span class="number">0.12703758</span>,  <span class="number">0.17167819</span>, -<span class="number">0.03712473</span>,</span><br><span class="line">          <span class="number">0.04252302</span>,  <span class="number">0.04741228</span>, -<span class="number">0.02731293</span>],</span><br><span class="line">        [ <span class="number">0.02198115</span>,  <span class="number">0.03989581</span>,  <span class="number">0.13165356</span>,  <span class="number">0.06523556</span>,  <span class="number">0.14900513</span>,</span><br><span class="line">          <span class="number">0.01858517</span>, -<span class="number">0.01644249</span>, -<span class="number">0.02377043</span>],</span><br><span class="line">        [ <span class="number">0.18718374</span>,  <span class="number">0.10347525</span>, -<span class="number">0.06668846</span>,  <span class="number">0.25818944</span>,  <span class="number">0.07522523</span>,</span><br><span class="line">          <span class="number">0.07082067</span>,  <span class="number">0.05170904</span>,  <span class="number">0.22902426</span>],</span><br><span class="line">        [-<span class="number">0.01912907</span>, -<span class="number">0.01732869</span>,  <span class="number">0.00391375</span>, -<span class="number">0.02338142</span>,  <span class="number">0.02787969</span>,</span><br><span class="line">         -<span class="number">0.02744135</span>,  <span class="number">0.0074541</span> ,  <span class="number">0.01806928</span>],</span><br><span class="line">        [-<span class="number">0.01993229</span>, -<span class="number">0.04436176</span>,  <span class="number">0.07624088</span>,  <span class="number">0.04268746</span>, -<span class="number">0.00883252</span>,</span><br><span class="line">          <span class="number">0.00789542</span>, -<span class="number">0.03039453</span>,  <span class="number">0.05851226</span>],</span><br><span class="line">        [-<span class="number">0.12873659</span>, -<span class="number">0.00083202</span>, -<span class="number">0.03246918</span>,  <span class="number">0.23910245</span>, -<span class="number">0.24635716</span>,</span><br><span class="line">          <span class="number">0.10966355</span>,  <span class="number">0.02079294</span>, -<span class="number">0.03829115</span>],</span><br><span class="line">        [ <span class="number">0.00225757</span>, -<span class="number">0.12751001</span>, -<span class="number">0.12703758</span>,  <span class="number">0.17167819</span>, -<span class="number">0.03712473</span>,</span><br><span class="line">          <span class="number">0.04252302</span>,  <span class="number">0.04741228</span>, -<span class="number">0.02731293</span>]]], dtype=float32)</span><br></pre></td></tr></table></figure>
<p>可以看出，embedding层将(1, 20)的一个输入sample（最长为20个单词的句子，其中每个单词表示为一个int数字），嵌入为一个(1, 20, 8)的向量，即将每个单词embed为一个8维的向量，而整个embedding层的参数就由神经网络学习得到，数据经过embedding层之后就方便地转换为了可以由CNN或者RNN进一步处理的格式。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/">How to Use Word Embedding Layers for Deep Learning with Keras - Machine Learning Mastery</a></li>
</ul>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://twitter.com/frankchen0130">
            <span class="icon">
              <i class="fa fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://t.me/frankchen0130">
            <span class="icon">
              <i class="fa fa-telegram"></i>
            </span>

            <span class="label">Telegram</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
              <a href="/tags/Keras/" rel="tag"># Keras</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/12/15/Neural-Network-Terms-Introduction-to-Optimization-Functions-Activation-Functions-Loss-Functions-and-Regular-Functions/" rel="prev" title="神经网络术语大百科：优化函数、激活函数、损失函数、正则方法的简介">
      <i class="fa fa-chevron-left"></i> 神经网络术语大百科：优化函数、激活函数、损失函数、正则方法的简介
    </a></div>
      <div class="post-nav-item">
    <a href="/2017/12/25/Remote-jupyter-notebook/" rel="next" title="DIY远程Jupyter Notebook服务器">
      DIY远程Jupyter Notebook服务器 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC8yODQ5NS81MDY2"></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Word-Embedding"><span class="nav-number">1.</span> <span class="nav-text">Word Embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keras-Embedding-Layer"><span class="nav-number">2.</span> <span class="nav-text">Keras Embedding Layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0-Embedding%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">3.</span> <span class="nav-text">学习 Embedding的例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83GloVE%E5%B5%8C%E5%85%A5%E7%9A%84%E7%A4%BA%E4%BE%8B"><span class="nav-number">4.</span> <span class="nav-text">使用预训练GloVE嵌入的示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IMDB-%E6%95%B0%E6%8D%AE%E9%9B%86Embedding%E5%AE%9E%E4%BE%8B"><span class="nav-number">5.</span> <span class="nav-text">IMDB 数据集Embedding实例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">6.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">frankchen0130</p>
  <div class="site-description" itemprop="description">Marketing/Python/Clojure/Game</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">106</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/frankchen0130" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;frankchen0130" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:znwindy@gmail.com" title="E-Mail → mailto:znwindy@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/znwindy" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;znwindy" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/frankchen0130" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;frankchen0130" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/5985526/frankchen0130" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;5985526&#x2F;frankchen0130" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i>StackOverflow</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://blog.topspeedsnail.com/" title="http:&#x2F;&#x2F;blog.topspeedsnail.com&#x2F;" rel="noopener" target="_blank">斗大的熊猫</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://hujiaweibujidao.github.io/" title="https:&#x2F;&#x2F;hujiaweibujidao.github.io&#x2F;" rel="noopener" target="_blank">胡家威</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://my.csdn.net/yywan1314520" title="http:&#x2F;&#x2F;my.csdn.net&#x2F;yywan1314520" rel="noopener" target="_blank">-dragon-</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://e1ias.github.io/" title="https:&#x2F;&#x2F;e1ias.github.io&#x2F;" rel="noopener" target="_blank">E1ias</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://cuiqingcai.com/" title="http:&#x2F;&#x2F;cuiqingcai.com&#x2F;" rel="noopener" target="_blank">静觅</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://hubojing.github.io/" title="https:&#x2F;&#x2F;hubojing.github.io&#x2F;" rel="noopener" target="_blank">胡博靖</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://gjoker.github.io/" title="https:&#x2F;&#x2F;gjoker.github.io&#x2F;" rel="noopener" target="_blank">GJoker</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://darrenliuwei.com/" title="https:&#x2F;&#x2F;darrenliuwei.com&#x2F;" rel="noopener" target="_blank">刘伟</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2015 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">frankchen0130</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
