<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>不正经数据科学家</title>
  
  <subtitle>Enjoy everything fun and challenging</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://frankchen.xyz/"/>
  <updated>2018-01-19T04:46:52.578Z</updated>
  <id>http://frankchen.xyz/</id>
  
  <author>
    <name>江南消夏</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Data Science Pipelines |  数据科学比赛工程中的管道和案例</title>
    <link href="http://frankchen.xyz/2018/01/12/Data-Science-Notes/"/>
    <id>http://frankchen.xyz/2018/01/12/Data-Science-Notes/</id>
    <published>2018-01-12T06:22:50.000Z</published>
    <updated>2018-01-19T04:46:52.578Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15161698020879.png" alt=""></p><p>暂定为记录各式数据科学项目、Kaggle竞赛里面常用、有用的代码片段、API、神操作等，通常是Numpy、Pandas、Matplotlib、Seaborn等相关，通常来说，项目基本步骤可以分为EDA、特征工程以及调参。</p><a id="more"></a><h2 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h2><ol><li><p>以一个Kaggle上的House Price为案例，机器学习流程分成两个大步骤 ：即<br>EDA与特征工程（只使用Pandas, StatsModel，scipy,numpy, seaborn等库）</p><ul><li><p>输入： 原始Train, Test 数据集，将原始Train和Test 合并成一个数据集combined</p></li><li><p>处理： Pandas Pipe</p><p>  根据各种可能和各种特征工程方法定义各种函数（输入combined, 输入pre_combined)</p><p>  用PandasPipe 将这个函数像搭积木一样连在一起。用列表按序存放这些函数）</p><p>  例如： pipe_basic = [pipe_basic_fillna,pipe_fillna_ascat,pipe_bypass,pipe_bypass,pipe_log_getdummies,pipe_export,pipe_r2test]</p><p>  这个列表就是，1. 基本的填充空值, 2. 转换数据类型， 3. 空白函数（为了对齐美观而以，啥事不做），4. log 转换，类别数据哑元处理， 5. 导出到hdf5文件， 6.检查R2值</p><p>  利用各种排列组合，或者各种参数组合，可以产生丰富的pipes，每一个pipes都可以产生一个预处理过的文件。</p></li><li><p>输出：某文件夹下 的N个预处理过的hdf5文件。 针对各种特征工程的排列组合，或者是Kaggle上面的各种新奇的特征工程方法。</p></li></ul></li><li><p>机器学习阶段（训练和产生模型，目标是尽可能获得尽可能低的RMSE值（针对训练数据），同时要具有范化的能力（针对测试数据））</p><ul><li>第一步，建立基准，筛选出最好的一个（几个）预处理文件（随机数设成固定值）</li><li>第二步，针对筛选出来的预处理文件，进行调参。找到最合适的几个算法（通常是RMSE值最低，且不同Kernel）（随机数设成固定值）    </li><li>第三步，用调好的参数来预处理文件中的Traing数据的做average 和stacking</li><li>第四部，生成csv文件，提交到Kaggle 看看得分如何。</li></ul></li></ol><h2 id="准备阶段-与-NoteBook-Head"><a href="#准备阶段-与-NoteBook-Head" class="headerlink" title="准备阶段 与 NoteBook Head"></a>准备阶段 与 NoteBook Head</h2><p>过滤warning：有句话说的好，在计算机科学里，我们只在意错误不在意warning</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> warnings</div><div class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</div></pre></td></tr></table></figure><hr><p>Notebook交互输出所有结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> IPython.core.interactiveshell <span class="keyword">import</span> InteractiveShell</div><div class="line">InteractiveShell.ast_node_interactivity=<span class="string">'all'</span></div></pre></td></tr></table></figure><p>结果如下<br><img src="/images/Screen%20Shot%202018-01-12%20at%2015.27.58.png" alt="Screen Shot 2018-01-12 at 15.27.58"></p><hr><p>一般对train以及test做一个concat，并记录train的条数ntrain<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">train = pd.read_csv(<span class="string">"train.csv.gz"</span>)</div><div class="line">test = pd.read_csv(<span class="string">"test.csv.gz"</span>)</div><div class="line"></div><div class="line">combined = pd.concat([train,test],axis =<span class="number">0</span>, ignore_index =<span class="keyword">True</span>)</div><div class="line">ntrain = train.shape[<span class="number">0</span>]</div><div class="line">Y_train = train[<span class="string">"SalePrice"</span>]</div><div class="line">X_train = train.drop([<span class="string">"Id"</span>,<span class="string">"SalePrice"</span>],axis=<span class="number">1</span>)</div><div class="line">print(<span class="string">"train data shape:\t "</span>,train.shape)</div><div class="line">print(<span class="string">"test data shape:\t "</span>,test.shape)</div><div class="line">print(<span class="string">"combined data shape:\t"</span>,combined.shape)</div></pre></td></tr></table></figure></p><h2 id="EDA相关"><a href="#EDA相关" class="headerlink" title="EDA相关"></a>EDA相关</h2><p>缺失值分析</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cols_missing_value = combined.isnull().sum()/combined.shape[<span class="number">0</span>]</div><div class="line">cols_missing_value = cols_missing_value[cols_missing_value&gt;<span class="number">0</span>]</div><div class="line">print(<span class="string">"How many features is bad/missing value? The answer is:"</span>,cols_missing_value.shape[<span class="number">0</span>])</div><div class="line">cols_missing_value.sort_values(ascending=<span class="keyword">False</span>).head(<span class="number">10</span>).plot.barh()</div></pre></td></tr></table></figure><p><img src="/images/15161679439549.jpg" alt=""></p><p>有缺失 - 需要填充或者删除，通常用均值或者中指，或者用人工分析（人工分析是提分关键）</p><hr><p>将若干个Dataframe画在同一个图里面相同坐标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">fig, ax = plt.subplots()</div><div class="line"><span class="comment"># desc, group 是一个Dataframe groupby desc 出的结果</span></div><div class="line"><span class="keyword">for</span> desc, group <span class="keyword">in</span> Energy_sources:</div><div class="line">    group.plot(x = group.index, y=<span class="string">'Value'</span>, label=desc,ax = ax, title=<span class="string">'Carbon Emissions per Energy Source'</span>, fontsize = <span class="number">20</span>)</div><div class="line">    ax.set_xlabel(<span class="string">'Time(Monthly)'</span>)</div><div class="line">    ax.set_ylabel(<span class="string">'Carbon Emissions in MMT'</span>)</div><div class="line">    ax.xaxis.label.set_size(<span class="number">20</span>)</div><div class="line">    ax.yaxis.label.set_size(<span class="number">20</span>)</div><div class="line">    ax.legend(fontsize = <span class="number">16</span>)</div></pre></td></tr></table></figure><p>结果如下图，<br><img src="/images/15157398435931.jpg" alt=""></p><hr><p>画a*b的子图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">fig, axes = plt.subplots(<span class="number">3</span>,<span class="number">3</span>, figsize = (<span class="number">30</span>, <span class="number">20</span>))</div><div class="line"><span class="comment"># desc, group 是一个Dataframe groupby desc 出的结果 也就是下面的Energy_sources</span></div><div class="line"><span class="keyword">for</span> (desc, group), ax <span class="keyword">in</span> zip(Energy_sources, axes.flatten()):</div><div class="line">    group.plot(x = group.index, y=<span class="string">'Value'</span>,ax = ax, title=desc, fontsize = <span class="number">18</span>)</div><div class="line">    ax.set_xlabel(<span class="string">'Time(Monthly)'</span>)</div><div class="line">    ax.set_ylabel(<span class="string">'Carbon Emissions in MMT'</span>)</div><div class="line">    ax.xaxis.label.set_size(<span class="number">18</span>)</div><div class="line">    ax.yaxis.label.set_size(<span class="number">18</span>)</div></pre></td></tr></table></figure><p><img src="/images/15157402388676.jpg" alt=""></p><hr><p>画柱状图</p><p><img src="/images/Screen%20Shot%202018-01-12%20at%2015.19.32.png" alt="Screen Shot 2018-01-12 at 15.19.32"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">fig = plt.figure(figsize = (<span class="number">16</span>,<span class="number">9</span>))</div><div class="line"><span class="comment"># CO2_per_source的来源与结构如上图</span></div><div class="line">x_label = map(<span class="keyword">lambda</span> x: x[:<span class="number">20</span>],CO2_per_source.index)</div><div class="line">x_tick = np.arange(len(cols))</div><div class="line">plt.bar(x_tick, CO2_per_source, align = <span class="string">'center'</span>, alpha = <span class="number">0.5</span>)</div><div class="line">fig.suptitle(<span class="string">"CO2 Emissions by Electric Power Sector"</span>, fontsize= <span class="number">25</span>)</div><div class="line">plt.xticks(x_tick, x_label, rotation = <span class="number">70</span>, fontsize = <span class="number">15</span>)</div><div class="line">plt.yticks(fontsize = <span class="number">20</span>)</div><div class="line">plt.xlabel(<span class="string">'Carbon Emissions in MMT'</span>, fontsize = <span class="number">20</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15157416530029.jpg" alt=""></p><hr><p>重叠图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> statsmodels.tsa.seasonal <span class="keyword">import</span> seasonal_decompose</div><div class="line">decomposition = seasonal_decompose(mte)</div><div class="line"></div><div class="line">trend = decomposition.trend</div><div class="line">seasonal = decomposition.seasonal</div><div class="line">residual = decomposition.resid</div><div class="line"></div><div class="line">plt.subplot(<span class="number">411</span>)</div><div class="line">plt.plot(mte, label=<span class="string">'Original'</span>)</div><div class="line">plt.legend(loc=<span class="string">'best'</span>)</div><div class="line">plt.subplot(<span class="number">412</span>)</div><div class="line">plt.plot(trend, label=<span class="string">'Trend'</span>)</div><div class="line">plt.legend(loc=<span class="string">'best'</span>)</div><div class="line">plt.subplot(<span class="number">413</span>)</div><div class="line">plt.plot(seasonal,label=<span class="string">'Seasonality'</span>)</div><div class="line">plt.legend(loc=<span class="string">'best'</span>)</div><div class="line">plt.subplot(<span class="number">414</span>)</div><div class="line">plt.plot(residual, label=<span class="string">'Residuals'</span>)</div><div class="line">plt.legend(loc=<span class="string">'best'</span>)</div><div class="line">plt.tight_layout()</div></pre></td></tr></table></figure><p><img src="/images/15157543822442.jpg" alt=""></p><hr><p>环形图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">plt.subplots(figsize=(<span class="number">15</span>,<span class="number">15</span>))</div><div class="line">data=response[<span class="string">'PublicDatasetsSelect'</span>].str.split(<span class="string">','</span>)</div><div class="line">dataset=[]</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data.dropna():</div><div class="line">    dataset.extend(i)</div><div class="line">pd.Series(dataset).value_counts().plot.pie(autopct=<span class="string">'%1.1f%%'</span>,colors=sns.color_palette(<span class="string">'Paired'</span>,<span class="number">10</span>),startangle=<span class="number">90</span>,wedgeprops = &#123; <span class="string">'linewidth'</span> : <span class="number">2</span>, <span class="string">'edgecolor'</span> : <span class="string">'white'</span> &#125;)</div><div class="line">plt.title(<span class="string">'Dataset Source'</span>)</div><div class="line">my_circle=plt.Circle( (<span class="number">0</span>,<span class="number">0</span>), <span class="number">0.7</span>, color=<span class="string">'white'</span>)</div><div class="line">p=plt.gcf()</div><div class="line">p.gca().add_artist(my_circle)</div><div class="line">plt.ylabel(<span class="string">''</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15159953413652.jpg" alt=""></p><hr><p>饼状图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">f,ax=plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">18</span>,<span class="number">8</span>))</div><div class="line">response[<span class="string">'JobSkillImportancePython'</span>].value_counts().plot.pie(ax=ax[<span class="number">0</span>],autopct=<span class="string">'%1.1f%%'</span>,explode=[<span class="number">0.1</span>,<span class="number">0</span>,<span class="number">0</span>],shadow=<span class="keyword">True</span>,colors=[<span class="string">'g'</span>,<span class="string">'lightblue'</span>,<span class="string">'r'</span>])</div><div class="line">ax[<span class="number">0</span>].set_title(<span class="string">'Python Necessity'</span>)</div><div class="line">ax[<span class="number">0</span>].set_ylabel(<span class="string">''</span>)</div><div class="line">response[<span class="string">'JobSkillImportanceR'</span>].value_counts().plot.pie(ax=ax[<span class="number">1</span>],autopct=<span class="string">'%1.1f%%'</span>,explode=[<span class="number">0</span>,<span class="number">0.1</span>,<span class="number">0</span>],shadow=<span class="keyword">True</span>,colors=[<span class="string">'lightblue'</span>,<span class="string">'g'</span>,<span class="string">'r'</span>])</div><div class="line">ax[<span class="number">1</span>].set_title(<span class="string">'R Necessity'</span>)</div><div class="line">ax[<span class="number">1</span>].set_ylabel(<span class="string">''</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15159954686370.jpg" alt=""></p><hr><p>维恩图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">f,ax=plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">18</span>,<span class="number">8</span>))</div><div class="line">pd.Series([python.shape[<span class="number">0</span>],R.shape[<span class="number">0</span>],both.shape[<span class="number">0</span>]],index=[<span class="string">'Python'</span>,<span class="string">'R'</span>,<span class="string">'Both'</span>]).plot.bar(ax=ax[<span class="number">0</span>])</div><div class="line">ax[<span class="number">0</span>].set_title(<span class="string">'Number of Users'</span>)</div><div class="line">venn2(subsets = (python.shape[<span class="number">0</span>],R.shape[<span class="number">0</span>],both.shape[<span class="number">0</span>]), set_labels = (<span class="string">'Python Users'</span>, <span class="string">'R Users'</span>))</div><div class="line">plt.title(<span class="string">'Venn Diagram for Users'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15159955616741.jpg" alt=""></p><h2 id="Seaborn"><a href="#Seaborn" class="headerlink" title="Seaborn"></a>Seaborn</h2><p>count plot</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plt.subplots(figsize=(<span class="number">22</span>,<span class="number">12</span>))</div><div class="line">sns.countplot(y=response[<span class="string">'GenderSelect'</span>],order=response[<span class="string">'GenderSelect'</span>].value_counts().index)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15159941639903.jpg" alt=""></p><hr><p>利用squarify画树形图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> squarify</div><div class="line">tree=response[<span class="string">'Country'</span>].value_counts().to_frame()</div><div class="line">squarify.plot(sizes=tree[<span class="string">'Country'</span>].values,label=tree.index,color=sns.color_palette(<span class="string">'RdYlGn_r'</span>,<span class="number">52</span>))</div><div class="line">plt.rcParams.update(&#123;<span class="string">'font.size'</span>:<span class="number">20</span>&#125;)</div><div class="line">fig=plt.gcf()</div><div class="line">fig.set_size_inches(<span class="number">40</span>,<span class="number">15</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15159945932245.jpg" alt=""></p><hr><p>sns画分布图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">plt.subplots(figsize=(<span class="number">15</span>,<span class="number">8</span>))</div><div class="line">salary=salary[salary[<span class="string">'Salary'</span>]&lt;<span class="number">1000000</span>]</div><div class="line">sns.distplot(salary[<span class="string">'Salary'</span>])</div><div class="line">plt.title(<span class="string">'Salary Distribution'</span>,size=<span class="number">15</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15159947979455.jpg" alt=""></p><hr><p>sns子图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">f,ax=plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">18</span>,<span class="number">8</span>))</div><div class="line">sal_coun=salary.groupby(<span class="string">'Country'</span>)[<span class="string">'Salary'</span>].median().sort_values(ascending=<span class="keyword">False</span>)[:<span class="number">15</span>].to_frame()</div><div class="line">sns.barplot(<span class="string">'Salary'</span>,sal_coun.index,data=sal_coun,palette=<span class="string">'RdYlGn'</span>,ax=ax[<span class="number">0</span>])</div><div class="line">ax[<span class="number">0</span>].axvline(salary[<span class="string">'Salary'</span>].median(),linestyle=<span class="string">'dashed'</span>)</div><div class="line">ax[<span class="number">0</span>].set_title(<span class="string">'Highest Salary Paying Countries'</span>)</div><div class="line">ax[<span class="number">0</span>].set_xlabel(<span class="string">''</span>)</div><div class="line">max_coun=salary.groupby(<span class="string">'Country'</span>)[<span class="string">'Salary'</span>].median().to_frame()</div><div class="line">max_coun=max_coun[max_coun.index.isin(resp_coun.index)]</div><div class="line">max_coun.sort_values(by=<span class="string">'Salary'</span>,ascending=<span class="keyword">True</span>).plot.barh(width=<span class="number">0.8</span>,ax=ax[<span class="number">1</span>],color=sns.color_palette(<span class="string">'RdYlGn'</span>))</div><div class="line">ax[<span class="number">1</span>].axvline(salary[<span class="string">'Salary'</span>].median(),linestyle=<span class="string">'dashed'</span>)</div><div class="line">ax[<span class="number">1</span>].set_title(<span class="string">'Compensation of Top 15 Respondent Countries'</span>)</div><div class="line">ax[<span class="number">1</span>].set_xlabel(<span class="string">''</span>)</div><div class="line">ax[<span class="number">1</span>].set_ylabel(<span class="string">''</span>)</div><div class="line">plt.subplots_adjust(wspace=<span class="number">0.8</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15159948678550.jpg" alt=""></p><hr><p>seaborn箱型图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">plt.subplots(figsize=(<span class="number">10</span>,<span class="number">8</span>))</div><div class="line">sns.boxplot(y=<span class="string">'GenderSelect'</span>,x=<span class="string">'Salary'</span>,data=salary)</div><div class="line">plt.ylabel(<span class="string">''</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15159949427978.jpg" alt=""></p><hr><p>seaborn count_plot</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">f,ax=plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">25</span>,<span class="number">15</span>))</div><div class="line">sns.countplot(y=response[<span class="string">'MajorSelect'</span>],ax=ax[<span class="number">0</span>],order=response[<span class="string">'MajorSelect'</span>].value_counts().index)</div><div class="line">ax[<span class="number">0</span>].set_title(<span class="string">'Major'</span>)</div><div class="line">ax[<span class="number">0</span>].set_ylabel(<span class="string">''</span>)</div><div class="line">sns.countplot(y=response[<span class="string">'CurrentJobTitleSelect'</span>],ax=ax[<span class="number">1</span>],order=response[<span class="string">'CurrentJobTitleSelect'</span>].value_counts().index)</div><div class="line">ax[<span class="number">1</span>].set_title(<span class="string">'Current Job'</span>)</div><div class="line">ax[<span class="number">1</span>].set_ylabel(<span class="string">''</span>)</div><div class="line">plt.subplots_adjust(wspace=<span class="number">0.8</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15159950249702.jpg" alt=""></p><hr><p>seaborn 图中添加文字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">sal_job=salary.groupby(<span class="string">'CurrentJobTitleSelect'</span>)[<span class="string">'Salary'</span>].median().to_frame().sort_values(by=<span class="string">'Salary'</span>,ascending=<span class="keyword">False</span>)</div><div class="line">ax=sns.barplot(sal_job.Salary,sal_job.index,palette=sns.color_palette(<span class="string">'inferno'</span>,<span class="number">20</span>))</div><div class="line">plt.title(<span class="string">'Compensation By Job Title'</span>,size=<span class="number">15</span>)</div><div class="line"><span class="keyword">for</span> i, v <span class="keyword">in</span> enumerate(sal_job.Salary): </div><div class="line">    ax.text(<span class="number">.5</span>, i, v,fontsize=<span class="number">10</span>,color=<span class="string">'white'</span>,weight=<span class="string">'bold'</span>)</div><div class="line">fig=plt.gcf()</div><div class="line">fig.set_size_inches(<span class="number">8</span>,<span class="number">8</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15159951024672.jpg" alt=""></p><hr><p>词云</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud, STOPWORDS</div><div class="line"><span class="keyword">import</span> nltk</div><div class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</div><div class="line">free=pd.read_csv(<span class="string">'../input/freeformResponses.csv'</span>)</div><div class="line">stop_words=set(stopwords.words(<span class="string">'english'</span>))</div><div class="line">stop_words.update(<span class="string">','</span>,<span class="string">';'</span>,<span class="string">'!'</span>,<span class="string">'?'</span>,<span class="string">'.'</span>,<span class="string">'('</span>,<span class="string">')'</span>,<span class="string">'$'</span>,<span class="string">'#'</span>,<span class="string">'+'</span>,<span class="string">':'</span>,<span class="string">'...'</span>)</div><div class="line">motivation=free[<span class="string">'KaggleMotivationFreeForm'</span>].dropna().apply(nltk.word_tokenize)</div><div class="line">motivate=[]</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> motivation:</div><div class="line">    motivate.extend(i)</div><div class="line">motivate=pd.Series(motivate)</div><div class="line">motivate=([i <span class="keyword">for</span> i <span class="keyword">in</span> motivate.str.lower() <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> stop_words])</div><div class="line">f1=open(<span class="string">"kaggle.png"</span>, <span class="string">"wb"</span>)</div><div class="line">f1.write(codecs.decode(kaggle,<span class="string">'base64'</span>))</div><div class="line">f1.close()</div><div class="line">img1 = imread(<span class="string">"kaggle.png"</span>)</div><div class="line">hcmask1 = img1</div><div class="line">wc = WordCloud(background_color=<span class="string">"black"</span>, max_words=<span class="number">4000</span>, mask=hcmask1, </div><div class="line">               stopwords=STOPWORDS, max_font_size= <span class="number">60</span>,width=<span class="number">1000</span>,height=<span class="number">1000</span>)</div><div class="line">wc.generate(<span class="string">" "</span>.join(motivate))</div><div class="line">plt.imshow(wc)</div><div class="line">plt.axis(<span class="string">'off'</span>)</div><div class="line">fig=plt.gcf()</div><div class="line">fig.set_size_inches(<span class="number">10</span>,<span class="number">10</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15159971204332.jpg" alt=""></p><hr><p>简单情况下的分类展示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</div><div class="line"></div><div class="line">h = <span class="number">0.01</span></div><div class="line">x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">1</span>, X[:, <span class="number">0</span>].max() + <span class="number">1</span></div><div class="line">y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">1</span>, X[:, <span class="number">1</span>].max() + <span class="number">1</span></div><div class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">(X, y, w, history)</span>:</span></div><div class="line">    <span class="string">"""draws classifier prediction with matplotlib magic"""</span></div><div class="line">    Z = probability(expand(np.c_[xx.ravel(), yy.ravel()]), w)</div><div class="line">    Z = Z.reshape(xx.shape)</div><div class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</div><div class="line">    plt.contourf(xx, yy, Z, alpha=<span class="number">0.8</span>)</div><div class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=plt.cm.Paired)</div><div class="line">    plt.xlim(xx.min(), xx.max())</div><div class="line">    plt.ylim(yy.min(), yy.max())</div><div class="line">    </div><div class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</div><div class="line">    plt.plot(history)</div><div class="line">    plt.grid()</div><div class="line">    ymin, ymax = plt.ylim()</div><div class="line">    plt.ylim(<span class="number">0</span>, ymax)</div><div class="line">    display.clear_output(wait=<span class="keyword">True</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/images/15163355887784.jpg" alt=""></p><h2 id="特征工程阶段"><a href="#特征工程阶段" class="headerlink" title="特征工程阶段"></a>特征工程阶段</h2><p>Numpy区间百分比切分异常值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cut off long distance trips</span></div><div class="line">lat_low, lat_hgh = np.percentile(latlong[:,<span class="number">0</span>], [<span class="number">2</span>, <span class="number">98</span>])</div><div class="line">lon_low, lon_hgh = np.percentile(latlong[:,<span class="number">1</span>], [<span class="number">2</span>, <span class="number">98</span>])</div></pre></td></tr></table></figure><hr><p>初始化同shape向量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">g2 = np.zeros_like(w)</div></pre></td></tr></table></figure></p><hr><p>Numpy 竖着叠放向量<br><code>np.column_stack</code></p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 用于查看Dataframe各列数据类型</span></div><div class="line">ts.dtypes</div></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#skew是单变量工具，用来监测数据是否有长尾，左偏或者右偏</span></div><div class="line">print(Y_train.skew())</div><div class="line">``` </div><div class="line"></div><div class="line">``` python</div><div class="line"><span class="comment">#np.abs 是绝对值函数，用来取整个向量绝对值</span></div><div class="line"><span class="comment"># 这里对所有train里的特征求偏度并排序</span></div><div class="line">np.abs(combined[:ntrain].skew()).sort_values(ascending = <span class="keyword">False</span> ).head(<span class="number">20</span>)</div></pre></td></tr></table></figure><p>有偏度 - 需要处理。通常是用log1p </p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 用于将Dataframe中被读取为object的数据转换为数值型，errors='coerce'代表错误将被置为NaN</span></div><div class="line">ts[<span class="string">'Value'</span>] = pd.to_numeric(ts[<span class="string">'Value'</span>] , errors=<span class="string">'coerce'</span>)</div></pre></td></tr></table></figure><hr><p>过滤index 里面的NaN值，推广也可以过滤其他列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ts = df.loc[pd.Series(pd.to_datetime(df.index, errors=<span class="string">'coerce'</span>)).notnull().values]</div></pre></td></tr></table></figure><hr><p>按月groupby，以及unstack解构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Emissions.groupby([<span class="string">'Description'</span>, pd.TimeGrouper(<span class="string">'M'</span>)])[<span class="string">'Value'</span>].sum().unstack(level = <span class="number">0</span>)</div></pre></td></tr></table></figure><p><img src="/images/Screen%20Shot%202018-01-12%20at%2016.16.30.png" alt="Screen Shot 2018-01-12 at 16.16.30"></p><hr><p>将value_counts、groupby等Series转换为Dataframe<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tree=response[<span class="string">'Country'</span>].value_counts().to_frame()</div></pre></td></tr></table></figure></p><hr><p>特征工程大杀器，<a href="http://pandas.pydata.org/pandas-docs/stable/basics.html?highlight=pipe#tablewise-function-application" target="_blank" rel="external">Pandas Pipe</a><br>这里有个简单的例子，,每个pipes里面都有若干个特征处理函数和一个快速测试的函数，其中为了对齐美观，用bypass函数来填充空白的地方（无用但是为了强行让pipes长度相同）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">pipe_basic = [pipe_basic_fillna,pipe_bypass,\</div><div class="line">              pipe_bypass,pipe_bypass,\</div><div class="line">              pipe_bypass,pipe_bypass,\</div><div class="line">              pipe_log_getdummies,pipe_bypass, \</div><div class="line">              pipe_export,pipe_r2test]</div><div class="line"></div><div class="line"></div><div class="line">pipe_ascat = [pipe_fillna_ascat,pipe_drop_cols,\</div><div class="line">              pipe_drop4cols,pipe_outliersdrop,\</div><div class="line">              pipe_extract,pipe_bypass,\</div><div class="line">              pipe_log_getdummies,pipe_drop_dummycols, \</div><div class="line">              pipe_export,pipe_r2test]</div><div class="line"></div><div class="line">pipe_ascat_unitprice = [pipe_fillna_ascat,pipe_drop_cols,\</div><div class="line">              pipe_drop4cols,pipe_outliersdrop,\</div><div class="line">              pipe_extract,pipe_unitprice,\</div><div class="line">              pipe_log_getdummies,pipe_drop_dummycols, \</div><div class="line">              pipe_export,pipe_r2test]</div><div class="line"></div><div class="line">pipes = [pipe_basic,pipe_ascat,pipe_ascat_unitprice ]</div></pre></td></tr></table></figure><p>跑的代码为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(pipes)):</div><div class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"\n"</span>)</div><div class="line">    pipe_output=pipes[i]</div><div class="line">    output_name =<span class="string">"_"</span>.join([x.__name__[<span class="number">5</span>:] <span class="keyword">for</span> x <span class="keyword">in</span> pipe_output <span class="keyword">if</span> x.__name__ <span class="keyword">is</span> <span class="keyword">not</span> <span class="string">"pipe_bypass"</span>])</div><div class="line">    output_name = <span class="string">"PIPE_"</span> +output_name</div><div class="line">    print(output_name)</div><div class="line">    (combined.pipe(pipe_output[<span class="number">0</span>])</div><div class="line">             .pipe(pipe_output[<span class="number">1</span>])</div><div class="line">             .pipe(pipe_output[<span class="number">2</span>])</div><div class="line">             .pipe(pipe_output[<span class="number">3</span>])</div><div class="line">             .pipe(pipe_output[<span class="number">4</span>])</div><div class="line">             .pipe(pipe_output[<span class="number">5</span>])          </div><div class="line">             .pipe(pipe_output[<span class="number">6</span>])</div><div class="line">             .pipe(pipe_output[<span class="number">7</span>])</div><div class="line">             .pipe(pipe_output[<span class="number">8</span>],name=output_name)</div><div class="line">             .pipe(pipe_output[<span class="number">9</span>])</div><div class="line">             ）</div></pre></td></tr></table></figure><p>在这一步，我们可以初步看到三个特征工程的性能。并且文件已经输出到hd5格式文件。后期在训练和预测时，直接取出预处理的文件就可以。各个pipe代码可见<a href="https://gist.github.com/frankchen0130/5950eaa4d98ea4f93deed707b027b517" target="_blank" rel="external">此处</a>。</p><h2 id="调参阶段"><a href="#调参阶段" class="headerlink" title="调参阶段"></a>调参阶段</h2><p>在数据准备好后训练时，最基本的就是要调整超参（Hyperparameter）耗时耗力，并且和发生错误和遗漏情况。<br>Stackoverflow上常见的算法训练错误有：</p><ul><li>算法预测的结果差异非常大。 其中一个可能就是训练时的标准化步骤，在预测时遗漏了。</li><li>算法的调参结果差异非常大。（有的是0.01,有的就是10）。其中的一个可能就是不同的训练步骤中采用的标准化算法不同（例如,一次用了StandardScaler, 另一次用了RobustScaler)</li><li>此外，繁多的超参数调整起来异常繁琐。比较容易错误或者写错。</li></ul><p><strong>解决方法：Pipeline + Gridsearch + 参数字典 + 容器。</strong><br>使用Pipeline的例子</p><p>针对线形回归问题，Sklearn提供了超过15种回归算法。利用Pipeline 大法可以综合测试所有算法，找到最合适的算法。 具体步骤如下：</p><ol><li><p>初始化所有希望调测线形回归。</p></li><li><p>建立一个字典容器。{“算法名称”:[初始算法对象，参数字典，训练好的Pipeline模型对象，CV的成绩}</p></li><li><p>在调参步骤，将初始算法用Pipeline包装起来，利用Gridsearch进行调参。调参完成后可以得到针对相应的CV而获得的最后模型对象。 例如： lasso 算法的步骤如下：</p></li></ol><ul><li>包装 pipe=Pipeline([(“scaler”:None),(“selector”:None),(“clf”:Lasso())<ul><li>Pipe就是刚刚包装好的算法。可以直接用于 训练(fit)和预测(predict)</li><li>使用Pipe来处理训练集和测试集可以避免错误和遗漏，提高效率。</li><li>但是Pipe中算法是默认的参数，直接训练出的模型RMSE不太理想。（例如：local CV, 0.12~0.14左右）。这时可以考虑调参。</li></ul></li><li>调参第一步：准备参数字典：<br>  Params_lasso ={<br>  “Scaler”:[RobustScaler(),StandardScaler()], #两个标准化算法供调模型<br>  “selector<strong>threshold”:np.logspace(-5,-4,3), #3个选择门限供选特征<br>  “clf</strong>alpha”:np.logspace(-5,-1,10) }， #10个alpha指供调参</li><li>调参第二步：暴力调参和生成模型 rsearch = GridSearchCV(pipe, param_grid=Params_lasso,scoring =’neg_mean_squared_error’,verbose=verbose,cv=10,refit =True)<ul><li>GridSearch 是暴力调参。遍历所有参数组合，另外有一个RandomedSearch 可以随机选择参数组合，缩短调参时间，并且获得近似的调参性能</li><li>Pipe就是刚刚包装好的算法。GridSearch把可选的参数和算法（放入，或者更好的组合。</li><li>调参的训练标准是“’neg_mean_squared_error”, RMSE的负数。 这种处理方法，让最大值称为最小的MSE指。只需要对结果做一次np.sqrt( 结果负数）就能获得RMSE值。</li><li>cv=10. Cross Validate 数据集为9：1。数据集小的情况，例如House Price. 3折和10折结果甚至比调参差异还大。</li><li>refit =True. 在调参完成后，再需要做一次所有数据集的fit. 生成完整的训练模型</li></ul></li></ul><hr><p>Sklearn 流程图<br><img src="/images/15161696298310.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15161698020879.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;暂定为记录各式数据科学项目、Kaggle竞赛里面常用、有用的代码片段、API、神操作等，通常是Numpy、Pandas、Matplotlib、Seaborn等相关，通常来说，项目基本步骤可以分为EDA、特征工程以及调参。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Data Science" scheme="http://frankchen.xyz/tags/Data-Science/"/>
    
      <category term="Python" scheme="http://frankchen.xyz/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>DIY远程Jupyter Notebook服务器</title>
    <link href="http://frankchen.xyz/2017/12/25/Remote-jupyter-notebook/"/>
    <id>http://frankchen.xyz/2017/12/25/Remote-jupyter-notebook/</id>
    <published>2017-12-25T11:43:17.000Z</published>
    <updated>2017-12-25T11:48:42.351Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15142020075328.jpg" alt="Screen Shot 2017-07-18 at 14.16.18"><br>构建自己的远程Jupyter Notebook服务器，添加system开机自启，让Jupyter Notebook支持跨网络访问的方法。<br><a id="more"></a></p><h2 id="完全开放，不需密码"><a href="#完全开放，不需密码" class="headerlink" title="完全开放，不需密码"></a>完全开放，不需密码</h2><h3 id="1-登陆远程服务器"><a href="#1-登陆远程服务器" class="headerlink" title="1.  登陆远程服务器"></a>1.  登陆远程服务器</h3><h3 id="2-生成配置文件"><a href="#2-生成配置文件" class="headerlink" title="2.生成配置文件"></a>2.生成配置文件</h3><p><code>$jupyter notebook --generate-config</code></p><h3 id="3-修改默认配置文件"><a href="#3-修改默认配置文件" class="headerlink" title="3. 修改默认配置文件"></a>3. 修改默认配置文件</h3><p><code>$vim ~/.jupyter/jupyter_notebook_config.py</code><br>进行如下修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">c.NotebookApp.ip = <span class="string">'0.0.0.0'</span>      <span class="comment">#支持其它IP访问，关键</span></div><div class="line">c.NotebookApp.port = <span class="number">10000</span> <span class="comment">#随便指定一个端口</span></div></pre></td></tr></table></figure><h3 id="4-启动jupyter-notebook："><a href="#4-启动jupyter-notebook：" class="headerlink" title="4. 启动jupyter notebook："></a>4. 启动jupyter notebook：</h3><p><code>jupyter notebook</code></p><h3 id="5-远程访问"><a href="#5-远程访问" class="headerlink" title="5. 远程访问"></a>5. 远程访问</h3><p>此时应该可以直接从本地浏览器直接访问<code>http://address_of_remote:10000</code>就可以看到jupyter的登陆界面，输入密码即可。</p><h2 id="需要密码"><a href="#需要密码" class="headerlink" title="需要密码"></a>需要密码</h2><h3 id="1-生成密码"><a href="#1-生成密码" class="headerlink" title="1. 生成密码"></a>1. 生成密码</h3><p>打开ipython，创建一个密文的密码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> notebook.auth <span class="keyword">import</span> passwd</div><div class="line">In [<span class="number">2</span>]: passwd()</div><div class="line">Enter password: </div><div class="line">Verify password: </div><div class="line">Out[<span class="number">2</span>]: <span class="string">'sha1:ce23d945972f:34769685a7ccd3d08c84a18c63968a41f1140274'</span></div></pre></td></tr></table></figure><h3 id="2-添加密码"><a href="#2-添加密码" class="headerlink" title="2. 添加密码"></a>2. 添加密码</h3><p><code>$vim ~/.jupyter/jupyter_notebook_config.py</code><br>进行如下修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">c.NotebookApp.password = <span class="string">u'sha:ce...刚才复制的那个密文'</span></div></pre></td></tr></table></figure><p><img src="/images/Screen%20Shot%202017-07-18%20at%2014.16.18.png" alt="Screen Shot 2017-07-18 at 14.16.18"></p><h3 id="3-建立ssh通道"><a href="#3-建立ssh通道" class="headerlink" title="3. 建立ssh通道"></a>3. 建立ssh通道</h3><p>若还是无法登录，也可用</p><p><code>ssh username@address_of_remote -L 127.0.0.1:10000:127.0.0.1:10000</code></p><p>建立ssh通道，便可以在localhost:10000直接访问远程的jupyter了。</p><h2 id="添加system开机自启"><a href="#添加system开机自启" class="headerlink" title="添加system开机自启"></a>添加system开机自启</h2><p>将 Jupyter Notebook 设定为系统服务并且开机自动启动，这里以 systemd 下的设定为例，创建文件 <code>sudo vim /etc/systemd/system/jupyter.service</code>文件，内容是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=Jupyter Notebook</div><div class="line">After=network.target</div><div class="line"></div><div class="line">[Service]</div><div class="line">Type=simple</div><div class="line">ExecStart=/home/frank/anaconda3/bin/jupyter-notebook  --config=/home/frank/.jupyter/jupyter_notebook_config.py --no-browser</div><div class="line">User=frank</div><div class="line">Group=frank</div><div class="line">WorkingDirectory=/home/frank/</div><div class="line">Restart=always</div><div class="line">RestartSec=10</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div></pre></td></tr></table></figure><p>上面你需要把我的用户名frank替换掉，保存文件之后执行<br><code>systemctl enable jupyter</code><br>再执行<br><code>systemctl start jupyter</code><br>即可，需要输入几次密码，之后重启Notebook会自启。</p><p><img src="/images/Screen%20Shot%202017-12-25%20at%2019.32.28.png" alt="Screen Shot 2017-12-25 at 19.32.28"></p><h2 id="内网穿透"><a href="#内网穿透" class="headerlink" title="内网穿透"></a>内网穿透</h2><p>结合下文的方法，用ftp即可做到</p><ul><li><a href="http://frankchen.xyz/2017/11/12/ftp-using/">frp的内网穿透及外网访问内网jupyter-notebook的实现 | 不正经数据科学家</a></li></ul><h2 id="参考自"><a href="#参考自" class="headerlink" title="参考自"></a>参考自</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/23110830" target="_blank" rel="external">Jupyter (IPython notebook)用于服务器的配置方法(Windows) - 知乎专栏</a></li><li><a href="http://blog.leanote.com/post/jevonswang/%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEjupyter-notebook" target="_blank" rel="external">远程访问jupyter notebook</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15142020075328.jpg&quot; alt=&quot;Screen Shot 2017-07-18 at 14.16.18&quot;&gt;&lt;br&gt;构建自己的远程Jupyter Notebook服务器，添加system开机自启，让Jupyter Notebook支持跨网络访问的方法。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://frankchen.xyz/tags/python/"/>
    
      <category term="Linux" scheme="http://frankchen.xyz/tags/Linux/"/>
    
      <category term="Jupyter Notebook" scheme="http://frankchen.xyz/tags/Jupyter-Notebook/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中Keras中的Embedding层的理解与使用</title>
    <link href="http://frankchen.xyz/2017/12/18/How-to-Use-Word-Embedding-Layers-for-Deep-Learning-with-Keras/"/>
    <id>http://frankchen.xyz/2017/12/18/How-to-Use-Word-Embedding-Layers-for-Deep-Learning-with-Keras/</id>
    <published>2017-12-18T07:59:41.000Z</published>
    <updated>2017-12-18T10:09:40.657Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15135840798621.jpg" alt=""><br>单词嵌入提供了单词的密集表示及其相对含义，它们是对简单包模型表示中使用的稀疏表示的改进，可以从文本数据中学习字嵌入，并在项目之间重复使用。它们也可以作为拟合文本数据的神经网络的一部分来学习。<br><a id="more"></a></p><h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><p>单词嵌入是使用密集的矢量表示来表示单词和文档的一类方法。</p><p>词嵌入是对传统的词袋模型编码方案的改进，传统方法使用大而稀疏的矢量来表示每个单词或者在矢量内对每个单词进行评分以表示整个词汇表，这些表示是稀疏的，因为每个词汇的表示是巨大的，给定的词或文档主要由零值组成的大向量表示。</p><p>相反，在嵌入中，单词由密集向量表示，其中向量表示将单词投影到连续向量空间中。</p><p>向量空间中的单词的位置是从文本中学习的，并且基于在使用单词时围绕单词的单词。</p><p>学习到的向量空间中的单词的位置被称为它的嵌入：Embedding。</p><p>从文本学习单词嵌入方法的两个流行例子包括：</p><ul><li>Word2Vec.</li><li>GloVe.</li></ul><p>除了这些精心设计的方法之外，还可以将词嵌入学习作为深度学习模型的一部分。这可能是一个较慢的方法，但可以通过这样为特定数据集定制模型。</p><h2 id="Keras-Embedding-Layer"><a href="#Keras-Embedding-Layer" class="headerlink" title="Keras Embedding Layer"></a>Keras Embedding Layer</h2><p>Keras提供了一个嵌入层，适用于文本数据的神经网络。</p><p>它要求输入数据是整数编码的，所以每个字都用一个唯一的整数表示。这个数据准备步骤可以使用Keras提供的Tokenizer API来执行。</p><p>嵌入层用随机权重进行初始化，并将学习训练数据集中所有单词的嵌入。</p><p>它是一个灵活的图层，可以以多种方式使用，例如：</p><ul><li>它可以单独使用来学习一个单词嵌入，以后可以保存并在另一个模型中使用。</li><li>它可以用作深度学习模型的一部分，其中嵌入与模型本身一起学习。</li><li>它可以用来加载预先训练的词嵌入模型，这是一种迁移学习。</li></ul><p>嵌入层被定义为网络的第一个隐藏层。它必须指定3个参数：</p><ul><li>input_dim：这是文本数据中词汇的取值可能数。例如，如果您的数据是整数编码为0-9之间的值，那么词汇的大小就是10个单词；</li><li>output_dim：这是嵌入单词的向量空间的大小。它为每个单词定义了这个层的输出向量的大小。例如，它可能是32或100甚至更大，可以视为具体问题的超参数；</li><li>input_length：这是输入序列的长度，就像您为Keras模型的任何输入层所定义的一样，也就是一次输入带有的词汇个数。例如，如果您的所有输入文档都由1000个字组成，那么input_length就是1000。</li></ul><p>例如，下面我们定义一个词汇表为200的嵌入层（例如从0到199的整数编码的字，包括0到199），一个32维的向量空间，其中将嵌入单词，以及输入文档，每个单词有50个单词。</p><p><code>e = Embedding(input_dim=200, output_dim=32, input_length=50)</code></p><p>嵌入层自带学习的权重，如果将模型保存到文件中，则将包含嵌入图层的权重。</p><p>嵌入层的输出是一个二维向量，每个单词在输入文本（输入文档）序列中嵌入一个。</p><p>如果您希望直接将Dense层接到Embedding层后面，则必须先使用Flatten层将Embedding层的2D输出矩阵平铺为一维矢量。</p><p>现在，让我们看看我们如何在实践中使用嵌入层。</p><h2 id="学习-Embedding的例子"><a href="#学习-Embedding的例子" class="headerlink" title="学习 Embedding的例子"></a>学习 Embedding的例子</h2><p>在本节中，我们将看看如何在文本分类问题上拟合神经网络的同时学习单词嵌入。</p><p>我们将定义一个小问题，我们有10个文本文档，每个文档都有一个学生提交的工作评论。每个文本文档被分类为正的“1”或负的“0”。这是一个简单的情感分析问题。</p><p>首先，我们将定义文档及其类别标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># define documents 定义文档</span></div><div class="line">docs = [<span class="string">'Well done!'</span>,</div><div class="line"><span class="string">'Good work'</span>,</div><div class="line"><span class="string">'Great effort'</span>,</div><div class="line"><span class="string">'nice work'</span>,</div><div class="line"><span class="string">'Excellent!'</span>,</div><div class="line"><span class="string">'Weak'</span>,</div><div class="line"><span class="string">'Poor effort!'</span>,</div><div class="line"><span class="string">'not good'</span>,</div><div class="line"><span class="string">'poor work'</span>,</div><div class="line"><span class="string">'Could have done better.'</span>]</div><div class="line"><span class="comment"># define class labels 定义分类标签</span></div><div class="line">labels = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</div></pre></td></tr></table></figure><p>接下来，我们来整数编码每个文件。这意味着把输入，嵌入层将具有整数序列。我们可以尝试其他更复杂的bag of word 模型比如计数或TF-IDF。</p><p>Keras提供<a href="https://keras.io/preprocessing/text/#one_hot" target="_blank" rel="external">one_hot()</a>函数来创建每个单词的散列作为一个有效的整数编码。我们用估计50的词汇表大小，这大大减少了hash函数的冲突概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># integer encode the documents 独热编码</span></div><div class="line">vocab_size = <span class="number">50</span></div><div class="line">encoded_docs = [one_hot(d, vocab_size) <span class="keyword">for</span> d <span class="keyword">in</span> docs]</div><div class="line">print(encoded_docs)</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[[6, 16], [42, 24], [2, 17], [42, 24], [18], [17], [22, 17], [27, 42], [22, 24], [49, 46, 16, 34]]</div></pre></td></tr></table></figure><p>这样以后序列具有不同的长度，但是Keras更喜欢输入矢量化和所有输入具有相同的长度。我们将填充所有输入序列的长度为4，同样，我们可以使用内置的Keras函数（在这种情况下为<a href="https://keras.io/preprocessing/sequence/#pad_sequences" target="_blank" rel="external">pad_sequences()</a>函数）执行此操作,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># pad documents to a max length of 4 words 将不足长度的用0填充为长度4</span></div><div class="line">max_length = <span class="number">4</span></div><div class="line">padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=<span class="string">'post'</span>)</div><div class="line">print(padded_docs)</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[[ 6 16  0  0]</div><div class="line"> [42 24  0  0]</div><div class="line"> [ 2 17  0  0]</div><div class="line"> [42 24  0  0]</div><div class="line"> [18  0  0  0]</div><div class="line"> [17  0  0  0]</div><div class="line"> [22 17  0  0]</div><div class="line"> [27 42  0  0]</div><div class="line"> [22 24  0  0]</div><div class="line"> [49 46 16 34]]</div></pre></td></tr></table></figure><p>我们现在准备将我们的嵌入层定义为我们的神经网络模型的一部分。</p><p>嵌入的词汇量为50，输入长度为4，我们将选择一个8维的嵌入空间。</p><p>该模型是一个简单的二元分类模型。重要的是，嵌入层的输出将是每个8维的4个矢量，每个单词一个。我们将其平铺到一个32个元素的向量上以传递到密集输出层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># define the model 定义模型</span></div><div class="line">model = Sequential()</div><div class="line">model.add(Embedding(vocab_size, <span class="number">8</span>, input_length=max_length))</div><div class="line">model.add(Flatten())</div><div class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</div><div class="line"><span class="comment"># compile the model 编译</span></div><div class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'acc'</span>])</div><div class="line"><span class="comment"># summarize the model 打印模型信息</span></div><div class="line">print(model.summary())</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">_________________________________________________________________</div><div class="line">Layer (type)                 Output Shape              Param #</div><div class="line">=================================================================</div><div class="line">embedding_1 (Embedding)      (None, 4, 8)              400</div><div class="line">_________________________________________________________________</div><div class="line">flatten_1 (Flatten)          (None, 32)                0</div><div class="line">_________________________________________________________________</div><div class="line">dense_1 (Dense)              (None, 1)                 33</div><div class="line">=================================================================</div><div class="line">Total params: 433</div><div class="line">Trainable params: 433</div><div class="line">Non-trainable params: 0</div><div class="line">_________________________________________________________________</div></pre></td></tr></table></figure><p>最后，我们可以拟合和评估分类模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># fit the model 拟合</span></div><div class="line">model.fit(padded_docs, labels, epochs=<span class="number">50</span>, verbose=<span class="number">0</span>)</div><div class="line"><span class="comment"># evaluate the model 评估</span></div><div class="line">loss, accuracy = model.evaluate(padded_docs, labels, verbose=<span class="number">0</span>)</div><div class="line">print(<span class="string">'Accuracy: %f'</span> % (accuracy*<span class="number">100</span>))</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Accuracy: 100.000000</div></pre></td></tr></table></figure><p>下面是完整的代码，这里我们用函数式API改写了模型定义，不过结构和上面是完全一样的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Flatten, Input</div><div class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div><div class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</div><div class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> one_hot</div><div class="line"></div><div class="line"><span class="comment"># define documents</span></div><div class="line">docs = [<span class="string">'Well done!'</span>,</div><div class="line">        <span class="string">'Good work'</span>,</div><div class="line">        <span class="string">'Great effort'</span>,</div><div class="line">        <span class="string">'nice work'</span>,</div><div class="line">        <span class="string">'Excellent!'</span>,</div><div class="line">        <span class="string">'Weak'</span>,</div><div class="line">        <span class="string">'Poor effort!'</span>,</div><div class="line">        <span class="string">'not good'</span>,</div><div class="line">        <span class="string">'poor work'</span>,</div><div class="line">        <span class="string">'Could have done better.'</span>]</div><div class="line"><span class="comment"># define class labels</span></div><div class="line">labels = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line"><span class="comment"># integer encode the documents</span></div><div class="line">vocab_size = <span class="number">50</span></div><div class="line">encoded_docs = [one_hot(d, vocab_size) <span class="keyword">for</span> d <span class="keyword">in</span> docs]</div><div class="line">print(encoded_docs)</div><div class="line"><span class="comment"># pad documents to a max length of 4 words</span></div><div class="line">max_length = <span class="number">4</span></div><div class="line">padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=<span class="string">'post'</span>)</div><div class="line">print(padded_docs)</div><div class="line"><span class="comment"># define the model</span></div><div class="line">input = Input(shape=(<span class="number">4</span>, ))</div><div class="line">x = Embedding(vocab_size, <span class="number">8</span>, input_length=max_length)(input)</div><div class="line">x = Flatten()(x)</div><div class="line">x = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(x)</div><div class="line">model = Model(inputs=input, outputs=x)</div><div class="line"><span class="comment"># compile the model</span></div><div class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'acc'</span>])</div><div class="line"><span class="comment"># summarize the model</span></div><div class="line">print(model.summary())</div><div class="line"><span class="comment"># fit the model</span></div><div class="line">model.fit(padded_docs, labels, epochs=<span class="number">50</span>, verbose=<span class="number">0</span>)</div><div class="line"><span class="comment"># evaluate the model</span></div><div class="line">loss, accuracy = model.evaluate(padded_docs, labels, verbose=<span class="number">0</span>)</div><div class="line">print(<span class="string">'Accuracy: %f'</span> % (accuracy * <span class="number">100</span>))</div></pre></td></tr></table></figure><p>之后，我们可以将嵌入图层中学习的权重保存到文件中，以便以后在其他模型中使用。</p><p>通常也可以使用这个模型来分类在测试数据集中看到的同类词汇的其他文档。</p><p>接下来，让我们看看在Keras中加载预先训练的词嵌入。</p><h2 id="使用预训练GloVE嵌入的示例"><a href="#使用预训练GloVE嵌入的示例" class="headerlink" title="使用预训练GloVE嵌入的示例"></a>使用预训练GloVE嵌入的示例</h2><p>Keras嵌入层也可以使用在其他地方学习的嵌入字。</p><p>在自然语言处理领域，学习，保存和分享提供词嵌入是很常见的。</p><p>例如，GloVe方法背后的研究人员提供了一套在公共领域许可下发布的预先训练的词嵌入。看到：</p><ul><li><a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="external">GloVe: Global Vectors for Word Representation</a></li></ul><p>最小的包是822Mb，叫做“glove.6B.zip”。它训练了10亿个词汇（单词）的数据集，词汇量为40万字，有几种不同的嵌入矢量尺寸，包括50,100,200和300size。</p><p>您可以下载这个嵌入的集合，可以作为Keras嵌入层中训练数据集中的单词预先训练嵌入的权重。</p><p>这个例子受Keras项目中的一个例子的启发：<a href="https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py" target="_blank" rel="external">pretrained_word_embeddings.py</a>。</p><p>下载并解压缩后，您将看到几个文件，其中一个是“glove.6B.100d.txt”，其中包含一个100维版本的嵌入。</p><p>如果你在文件内部偷看，你会看到一个token（单词），后面是每行的权重（100个数字）。例如，下面是嵌入的ASCII文本文件的第一行，显示“the”的嵌入。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062</div></pre></td></tr></table></figure><p>如前一节所述，第一步是定义这些示例，将它们编码为整数，然后将这些序列填充为相同的长度。</p><p>在这种情况下，我们需要能够将单词映射到整数以及整数到单词。</p><p>Keras提供了一个<a href="https://keras.io/preprocessing/text/#tokenizer" target="_blank" rel="external">Tokenizer</a>类，可以适应训练数据，通过调用Tokenizer类的texts_to_sequences（）方法，可以一致地将文本转换为序列，并且可以访问单词在word_index属性中的整数字典映射。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># define documents</span></div><div class="line">docs = [<span class="string">'Well done!'</span>,</div><div class="line"><span class="string">'Good work'</span>,</div><div class="line"><span class="string">'Great effort'</span>,</div><div class="line"><span class="string">'nice work'</span>,</div><div class="line"><span class="string">'Excellent!'</span>,</div><div class="line"><span class="string">'Weak'</span>,</div><div class="line"><span class="string">'Poor effort!'</span>,</div><div class="line"><span class="string">'not good'</span>,</div><div class="line"><span class="string">'poor work'</span>,</div><div class="line"><span class="string">'Could have done better.'</span>]</div><div class="line"><span class="comment"># define class labels</span></div><div class="line">labels = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</div><div class="line"><span class="comment"># prepare tokenizer</span></div><div class="line">t = Tokenizer()</div><div class="line">t.fit_on_texts(docs)</div><div class="line">vocab_size = len(t.word_index) + <span class="number">1</span></div><div class="line"><span class="comment"># integer encode the documents</span></div><div class="line">encoded_docs = t.texts_to_sequences(docs)</div><div class="line">print(encoded_docs)</div><div class="line"><span class="comment"># pad documents to a max length of 4 words</span></div><div class="line">max_length = <span class="number">4</span></div><div class="line">padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=<span class="string">'post'</span>)</div><div class="line">print(padded_docs)</div></pre></td></tr></table></figure><p>接下来，我们需要将整个Glove字嵌入文件作为字的字典加载到内存中以嵌入数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># load the whole embedding into memory</span></div><div class="line">embeddings_index = dict()</div><div class="line">f = open(<span class="string">'glove.6B.100d.txt'</span>)</div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</div><div class="line">values = line.split()</div><div class="line">word = values[<span class="number">0</span>]</div><div class="line">coefs = asarray(values[<span class="number">1</span>:], dtype=<span class="string">'float32'</span>)</div><div class="line">embeddings_index[word] = coefs</div><div class="line">f.close()</div><div class="line">print(<span class="string">'Loaded %s word vectors.'</span> % len(embeddings_index))</div></pre></td></tr></table></figure><p>这很慢。在训练数据中过滤特殊字词的嵌入可能会更好。</p><p>接下来，我们需要为训练数据集中的每个单词创建一个嵌入矩阵。我们可以通过枚举Tokenizer.word_index中的所有唯一单词并从加载的GloVe嵌入中找到嵌入权重向量来实现这一点。</p><p>结果是一个仅用于训练期间将会看到的单词的权重矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># create a weight matrix for words in training docs</span></div><div class="line">embedding_matrix = zeros((vocab_size, <span class="number">100</span>))</div><div class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> t.word_index.items():</div><div class="line">embedding_vector = embeddings_index.get(word)</div><div class="line"><span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">embedding_matrix[i] = embedding_vector</div></pre></td></tr></table></figure><p>现在我们可以像以前一样定义我们的模型，并进行评估。</p><p>关键的区别是嵌入层可以用GloVe字嵌入权重来播种。我们选择了100维版本，因此必须使用output_dim将其设置为100来定义嵌入层。最后，我们不希望更新此模型中的学习单词权重，因此我们将设置模型的可训练属性为False 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)</div></pre></td></tr></table></figure><p>下面列出了完整的工作示例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> asarray</div><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> zeros</div><div class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</div><div class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Flatten</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Embedding</div><div class="line"><span class="comment"># define documents</span></div><div class="line">docs = [<span class="string">'Well done!'</span>,</div><div class="line"><span class="string">'Good work'</span>,</div><div class="line"><span class="string">'Great effort'</span>,</div><div class="line"><span class="string">'nice work'</span>,</div><div class="line"><span class="string">'Excellent!'</span>,</div><div class="line"><span class="string">'Weak'</span>,</div><div class="line"><span class="string">'Poor effort!'</span>,</div><div class="line"><span class="string">'not good'</span>,</div><div class="line"><span class="string">'poor work'</span>,</div><div class="line"><span class="string">'Could have done better.'</span>]</div><div class="line"><span class="comment"># define class labels</span></div><div class="line">labels = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</div><div class="line"><span class="comment"># prepare tokenizer</span></div><div class="line">t = Tokenizer()</div><div class="line">t.fit_on_texts(docs)</div><div class="line">vocab_size = len(t.word_index) + <span class="number">1</span></div><div class="line"><span class="comment"># integer encode the documents</span></div><div class="line">encoded_docs = t.texts_to_sequences(docs)</div><div class="line">print(encoded_docs)</div><div class="line"><span class="comment"># pad documents to a max length of 4 words</span></div><div class="line">max_length = <span class="number">4</span></div><div class="line">padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=<span class="string">'post'</span>)</div><div class="line">print(padded_docs)</div><div class="line"><span class="comment"># load the whole embedding into memory</span></div><div class="line">embeddings_index = dict()</div><div class="line">f = open(<span class="string">'../glove_data/glove.6B/glove.6B.100d.txt'</span>)</div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</div><div class="line">values = line.split()</div><div class="line">word = values[<span class="number">0</span>]</div><div class="line">coefs = asarray(values[<span class="number">1</span>:], dtype=<span class="string">'float32'</span>)</div><div class="line">embeddings_index[word] = coefs</div><div class="line">f.close()</div><div class="line">print(<span class="string">'Loaded %s word vectors.'</span> % len(embeddings_index))</div><div class="line"><span class="comment"># create a weight matrix for words in training docs</span></div><div class="line">embedding_matrix = zeros((vocab_size, <span class="number">100</span>))</div><div class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> t.word_index.items():</div><div class="line">embedding_vector = embeddings_index.get(word)</div><div class="line"><span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">embedding_matrix[i] = embedding_vector</div><div class="line"><span class="comment"># define model</span></div><div class="line">model = Sequential()</div><div class="line">e = Embedding(vocab_size, <span class="number">100</span>, weights=[embedding_matrix], input_length=<span class="number">4</span>, trainable=<span class="keyword">False</span>)</div><div class="line">model.add(e)</div><div class="line">model.add(Flatten())</div><div class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</div><div class="line"><span class="comment"># compile the model</span></div><div class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'acc'</span>])</div><div class="line"><span class="comment"># summarize the model</span></div><div class="line">print(model.summary())</div><div class="line"><span class="comment"># fit the model</span></div><div class="line">model.fit(padded_docs, labels, epochs=<span class="number">50</span>, verbose=<span class="number">0</span>)</div><div class="line"><span class="comment"># evaluate the model</span></div><div class="line">loss, accuracy = model.evaluate(padded_docs, labels, verbose=<span class="number">0</span>)</div><div class="line">print(<span class="string">'Accuracy: %f'</span> % (accuracy*<span class="number">100</span>))</div></pre></td></tr></table></figure><p>运行这个例子可能需要更长的时间，但是这表明它能够适应这个简单的问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]</div><div class="line"></div><div class="line">[[ 6  2  0  0]</div><div class="line"> [ 3  1  0  0]</div><div class="line"> [ 7  4  0  0]</div><div class="line"> [ 8  1  0  0]</div><div class="line"> [ 9  0  0  0]</div><div class="line"> [10  0  0  0]</div><div class="line"> [ 5  4  0  0]</div><div class="line"> [11  3  0  0]</div><div class="line"> [ 5  1  0  0]</div><div class="line"> [12 13  2 14]]</div><div class="line"></div><div class="line">Loaded 400000 word vectors.</div><div class="line"></div><div class="line">_________________________________________________________________</div><div class="line">Layer (type)                 Output Shape              Param #</div><div class="line">=================================================================</div><div class="line">embedding_1 (Embedding)      (None, 4, 100)            1500</div><div class="line">_________________________________________________________________</div><div class="line">flatten_1 (Flatten)          (None, 400)               0</div><div class="line">_________________________________________________________________</div><div class="line">dense_1 (Dense)              (None, 1)                 401</div><div class="line">=================================================================</div><div class="line">Total params: 1,901</div><div class="line">Trainable params: 401</div><div class="line">Non-trainable params: 1,500</div><div class="line">_________________________________________________________________</div><div class="line"></div><div class="line"></div><div class="line">Accuracy: 100.000000</div></pre></td></tr></table></figure><p>在实践中，最好还是尝试使用预先训练好的嵌入来学习单词嵌入，因为它是固定的，并尝试在预先训练好的嵌入之上进行学习，这就类似于计算机视觉里面用预训练的VGG或者res-net迁移具体问题那样。</p><p>不过这取决于什么最适合你的具体问题。</p><h2 id="参考-翻译自"><a href="#参考-翻译自" class="headerlink" title="参考/翻译自"></a>参考/翻译自</h2><ul><li><a href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/" target="_blank" rel="external">How to Use Word Embedding Layers for Deep Learning with Keras - Machine Learning Mastery</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15135840798621.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;单词嵌入提供了单词的密集表示及其相对含义，它们是对简单包模型表示中使用的稀疏表示的改进，可以从文本数据中学习字嵌入，并在项目之间重复使用。它们也可以作为拟合文本数据的神经网络的一部分来学习。&lt;br&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://frankchen.xyz/tags/Deep-Learning/"/>
    
      <category term="Keras" scheme="http://frankchen.xyz/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>神经网络术语大百科：优化函数、激活函数、损失函数、正则方法的简介</title>
    <link href="http://frankchen.xyz/2017/12/15/Neural-Network-Terms-Introduction-to-Optimization-Functions-Activation-Functions-Loss-Functions-and-Regular-Functions/"/>
    <id>http://frankchen.xyz/2017/12/15/Neural-Network-Terms-Introduction-to-Optimization-Functions-Activation-Functions-Loss-Functions-and-Regular-Functions/</id>
    <published>2017-12-15T04:45:44.000Z</published>
    <updated>2017-12-16T09:21:29.262Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/neuralnetworks.png" alt="neuralnetworks"></p><p>简述关于神经网络的各种优化函数（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）、各种激活函数（Sigmoid，Tanh、Hard Sigmoid、Softplus、ReLU、ElU、PReLU、RReLU）、各种损失函数以及正则方法的简述，并附带代码实现例子。</p><a id="more"></a><h1 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h1><p>先上两张图</p><figure class="three"><br>   <img src="/images/2017/05/contours_evaluation_optimizers.gif" title="Logo" width="300"><br>   <img src="/images/2017/05/saddle_point_evaluation_optimizers.gif" title="Logo" width="300"><br></figure><h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p>没有激活函数，神经元就只是一个线性函数，那么无论多少层的神经元叠加是没有意义的。而主流激活函数也随着神经网络、深度学习的发展迭代进化了许多次代。</p><h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p><img src="/images/15134144452960.jpg" alt=""><br>Sigmoid是S形状的意思，又因为它是逻辑回归的激活函数又叫logistic函数，函数式为$<code>y = 1 / (1 + exp(-x))</code>$是很早以前最常用的激活函数，其实也是有一些优点的，比如，</p><ul><li>值域位于0-1，那么对于逻辑回归，这是对于二分类的一个很自然的表达，也就是概率</li><li>处处连续可导</li></ul><p>不过呢，我们观察它的形状，可以得出，Sigmoid函数在两端（靠近0和1的部分）梯度很小，这也意味着，如果神经元的输出落到了这个地方，那么它几乎没什么梯度可以传到后面，而随着神经网络的层层削弱，后面的层（靠近输入的层）没有多少梯度能传过来，几乎就“学不到什么”了。这叫做梯度消失问题，一度是阻碍神经网络往更深的层进化的主要困难，导致深度学习专家们绞尽脑汁想了许多方法来对抗这个问题，比如“Xavier and He Initialization”，比如我们要把weight随机初始化为如下的范围，<br><img src="/images/Screen%20Shot%202017-12-16%20at%2017.03.18.png" alt="Screen Shot 2017-12-16 at 17.03.18"></p><p>sigmoid的另一个问题是它不是0均值的，Sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢。举例来讲，对，如果所有均为正数或负数，那么其对的导数总是正数或负数，这会导致如下图红色箭头所示的阶梯式更新，这显然并非一个好的优化路径。深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。</p><p><img src="/images/15134157378274.jpg" alt=""></p><p>如今，sigmoid函数应用最广泛的在于其变种softmax在多元分类中，比如手写数字识别，经过卷积神经网络的处理，最后我们需要网络输出每个预测的概率值，最后预测为某一个数字，这里就需要用到softmax，<br><img src="/images/15134154076033.jpg" alt=""><br>以下是softmax的Keras代码，注意其中一个trick，<code>e = K.exp(x - K.max(x, axis=axis, keepdims=True))</code>这里每个分量减去最大值是为了减少计算量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x, axis=<span class="number">-1</span>)</span>:</span></div><div class="line">    <span class="string">"""Softmax activation function.</span></div><div class="line"></div><div class="line">    # Arguments</div><div class="line">        x : Tensor.</div><div class="line">        axis: Integer, axis along which the softmax normalization is applied.</div><div class="line"></div><div class="line">    # Returns</div><div class="line">        Tensor, output of softmax transformation.</div><div class="line"></div><div class="line">    # Raises</div><div class="line">        ValueError: In case `dim(x) == 1`.</div><div class="line">    """</div><div class="line">    ndim = K.ndim(x)</div><div class="line">    <span class="keyword">if</span> ndim == <span class="number">2</span>:</div><div class="line">        <span class="keyword">return</span> K.softmax(x)</div><div class="line">    <span class="keyword">elif</span> ndim &gt; <span class="number">2</span>:</div><div class="line">        e = K.exp(x - K.max(x, axis=axis, keepdims=<span class="keyword">True</span>))</div><div class="line">        s = K.sum(e, axis=axis, keepdims=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> e / s</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Cannot apply softmax to a tensor that is 1D'</span>)</div></pre></td></tr></table></figure></p><h2 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h2><p><img src="/images/15134156685588.jpg" alt=""></p><p>tanh 是sigmoid的变形： $tanh(x)=2sigmoid(2x)-1$，与 sigmoid 不同的是，tanh 是0均值的。因此，实际应用中，tanh 会比 sigmoid 更好一些，</p><p><img src="/images/15134156615077.jpg" alt=""></p><h2 id="ReLU家族"><a href="#ReLU家族" class="headerlink" title="ReLU家族"></a>ReLU家族</h2><p>然而标准ReLU不是完美的，比如因为ReLU在小于0的坐标梯度都是0，那么会造成“死亡”的神经元的问题：一旦神经元的输入与权重之乘积是负的，那么经过ReLU的激活，输出就是0，而ReLU的0梯度让“死亡”的神经元无法“复活”：没办法回到输出不是0的状态，这样就出现了许多在ReLU的变种，一般都是对标准ReLU坐标轴左边的部分做文章，比如<strong>leaky ReLU</strong>。其公式就是$LeakyReLU_ α (z) = max(\alpha z,z)$。如图，<br><img src="/images/15134123600394.jpg" alt=""></p><p>这篇文章<a href="https://arxiv.org/pdf/1505.00853.pdf" target="_blank" rel="external">Empirical Evaluation of Rectified Activations in Convolution Network</a>对比了几种leaky ReLU，比如把$\alpha$设置为0.2效果总是好过0.01，并且，对于randomized leaky ReLU (RReLU)（其中$\alpha$设置为一个在指定范围内的随机数），效果也不错，而且还具有一定的正则作用。另外，对于parametric leaky ReLU (PReLU)（其中$\alpha$作为网络的一个参数，被反向传播学习出来，之前的$\alpha$都是超参数，不能学只能调节），这种变种对于大数据集不错，但是数据量过小就有过拟合的风险。以下是Keras里面relu的代码，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x, alpha=<span class="number">0.</span>, max_value=None)</span>:</span></div><div class="line">    <span class="string">"""Rectified linear unit.</span></div><div class="line"></div><div class="line">    With default values, it returns element-wise `max(x, 0)`.</div><div class="line"></div><div class="line">    # Arguments</div><div class="line">        x: A tensor or variable.</div><div class="line">        alpha: A scalar, slope of negative section (default=`0.`).</div><div class="line">        max_value: Saturation threshold.</div><div class="line"></div><div class="line">    # Returns</div><div class="line">        A tensor.</div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> alpha != <span class="number">0.</span>:</div><div class="line">        negative_part = tf.nn.relu(-x)</div><div class="line">    x = tf.nn.relu(x)</div><div class="line">    <span class="keyword">if</span> max_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        max_value = _to_tensor(max_value, x.dtype.base_dtype)</div><div class="line">        zero = _to_tensor(<span class="number">0.</span>, x.dtype.base_dtype)</div><div class="line">        x = tf.clip_by_value(x, zero, max_value)</div><div class="line">    <span class="keyword">if</span> alpha != <span class="number">0.</span>:</div><div class="line">        alpha = _to_tensor(alpha, x.dtype.base_dtype)</div><div class="line">        x -= alpha * negative_part</div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure><p>另外，在这篇文章里面<a href="https://arxiv.org/pdf/1511.07289v5.pdf" target="_blank" rel="external">FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)</a>，引入了一种新的ReLU，exponential linear unit (ELU)，公式如下，<br>$$<br>ELU_{\alpha}(z) = \alpha (\exp(z)-1) \ if \ z \lt 0 ; \ z \ if \ z \gt 0;<br>$$<br><img src="/images/15134129990682.jpg" alt=""></p><p>与标准ReLU最大的区别在于它处处连续可导，这使得梯度下降得到加速，收敛得到了加速，而使用了指数函数使得其测试阶段的计算代价更高。Keras里elu的实现，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">elu</span><span class="params">(x, alpha=<span class="number">1.</span>)</span>:</span></div><div class="line">    <span class="string">"""Exponential linear unit.</span></div><div class="line"></div><div class="line">    # Arguments</div><div class="line">        x: A tenor or variable to compute the activation function for.</div><div class="line">        alpha: A scalar, slope of positive section.</div><div class="line"></div><div class="line">    # Returns</div><div class="line">        A tensor.</div><div class="line">    """</div><div class="line">    res = tf.nn.elu(x)</div><div class="line">    <span class="keyword">if</span> alpha == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> res</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> tf.where(x &gt; <span class="number">0</span>, res, alpha * res)</div></pre></td></tr></table></figure><h2 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a>激活函数的选择</h2><p>一般来说，我们的选择顺序可以理解为：<br>ELU &gt; leaky ReLU (以及其变种) &gt; ReLU &gt; tanh &gt; logistic。但是，</p><ul><li>如果我们更顾虑模型运行速度，那么leaky ReLU可能比ELU更好；</li><li>如果我们不想调节超参数，那么用默认的$\alpha$就行，ReLU和ELU的分别是0.01和1； </li><li>如果算力足够可以用来调参，那么如果网络过拟合我们会选择RReLU，如果训练集数据足够多，那可以用PReLU。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/neuralnetworks.png&quot; alt=&quot;neuralnetworks&quot;&gt;&lt;/p&gt;
&lt;p&gt;简述关于神经网络的各种优化函数（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）、各种激活函数（Sigmoid，Tanh、Hard Sigmoid、Softplus、ReLU、ElU、PReLU、RReLU）、各种损失函数以及正则方法的简述，并附带代码实现例子。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://frankchen.xyz/tags/Deep-Learning/"/>
    
      <category term="Algorithm" scheme="http://frankchen.xyz/tags/Algorithm/"/>
    
      <category term="Data Science" scheme="http://frankchen.xyz/tags/Data-Science/"/>
    
      <category term="Python" scheme="http://frankchen.xyz/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>如何理解Pandas 和 Numpy里的axis</title>
    <link href="http://frankchen.xyz/2017/12/12/Understanding-the-axis-parameter-in-Pandas-and-Numpy/"/>
    <id>http://frankchen.xyz/2017/12/12/Understanding-the-axis-parameter-in-Pandas-and-Numpy/</id>
    <published>2017-12-12T10:36:04.000Z</published>
    <updated>2017-12-12T11:07:29.828Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15130766716183.jpg" alt=""></p><p>简述一种如何直观的理解Pandas 和 Numpy里面的axis参数的方法。<br><a id="more"></a></p><p>Numpy 和 Pandas里的sort、mean、drop等操作，不是分行或者列分别用一个method来定义，而是一个method里面用户指定axis来操作的，举例来说：</p><p>我们先在<a href="https://raw.githubusercontent.com/justmarkham/pandas-videos/master/data/drinks.csv" target="_blank" rel="external">此处</a>下载了一份各国酒类消费的csv文件为例。<br><img src="/images/Screen%20Shot%202017-12-12%20at%2018.46.14.png" alt="Screen Shot 2017-12-12 at 18.46.14"><br>如下是pandas里按axis 0和1进行drop的操作示例，我们很容易看出，axis 0是按行drop，而axis 1是按列drop：<br><img src="/images/Screen%20Shot%202017-12-12%20at%2018.46.22.png" alt="Screen Shot 2017-12-12 at 18.46.22"></p><p>但是，mean操作呢？<br><img src="/images/Screen%20Shot%202017-12-12%20at%2018.49.18.png" alt="Screen Shot 2017-12-12 at 18.49.18"></p><p>容易看出，axis 0得出了每一列的均值，而axis 1得出了则是每一行的均值。<br>那么，在Numpy里呢？</p><p><img src="/images/Screen%20Shot%202017-12-12%20at%2019.06.17.png" alt="Screen Shot 2017-12-12 at 19.06.17"></p><p>容易看出，axis为1的时候得出的是每行的sum，axis为0的时候得出了每列的sum。</p><p>由上面的例子，我们似乎可以看出，axis为1代表水平方向上的操作，axis为0代表垂直方向上的操作，比如axis为1的sum得出的就是每一行的和。</p><p><img src="/images/15130760734631.jpg" alt=""></p><p>但是，在Pandas的Dataframe里面，为什么axis=1代表的是drop整个列呢？以下这个例子也可以说明一些情况：</p><p><img src="/images/Screen%20Shot%202017-12-12%20at%2018.56.53.png" alt="Screen Shot 2017-12-12 at 18.56.53"></p><p>联系这个视频<a href="https://www.youtube.com/watch?v=PtO3t6ynH-8" target="_blank" rel="external">How do I use the “axis” parameter in pandas? - YouTube</a>，大家也可以得到一些结论，作者说：</p><blockquote><p>0 is the row axis, and 1 is the column axis. When you drop with axis=1, that means drop a column. When you take the mean with axis=1, that means the operation should “move across” the column axis, which produces row means.<br>指的就是一种更加容易理解的方式，“0就是行的axis，1就是列的axis，当以axis=1来drop，那么就是drop一个column，而axis=1 来取mean，那么就是这个操作‘穿越’了列的axis，产生了行上的mean”。</p></blockquote><p>另外，其实我们也可以这样来操作，<br><img src="/images/Screen%20Shot%202017-12-12%20at%2019.01.27.png" alt="Screen Shot 2017-12-12 at 19.01.27"><br><img src="/images/Screen%20Shot%202017-12-12%20at%2019.01.45.png" alt="Screen Shot 2017-12-12 at 19.01.45"></p><p>可以看出，axis=0与axis=’rows’是一样的（在Pandas里），是不是更加容易理解了？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15130766716183.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;简述一种如何直观的理解Pandas 和 Numpy里面的axis参数的方法。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://frankchen.xyz/tags/python/"/>
    
      <category term="Data Science" scheme="http://frankchen.xyz/tags/Data-Science/"/>
    
      <category term="Numpy" scheme="http://frankchen.xyz/tags/Numpy/"/>
    
      <category term="Pandas" scheme="http://frankchen.xyz/tags/Pandas/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Triplet Loss and Two example code</title>
    <link href="http://frankchen.xyz/2017/12/01/understanding-triplet-loss-and-example-code/"/>
    <id>http://frankchen.xyz/2017/12/01/understanding-triplet-loss-and-example-code/</id>
    <published>2017-12-01T09:19:05.000Z</published>
    <updated>2017-12-01T11:10:15.444Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15121200710041.jpg" alt=""><br>理解triplet loss，与给出TensorFlow和numpy两种形式的example code。<br><a id="more"></a></p><p>Triplet Loss 是当前应用的很广泛的一种损失函数，在人脸识别和聚类领域，这是一种很自然的映射与计算损失的方式，比如<a href="https://arxiv.org/abs/1503.03832" target="_blank" rel="external">FaceNet</a>里，通过构建一种embedding 方式，将人脸图像直接映射到欧式空间，而优化这种embedding的方法可以概括为，构建许多组三元组（Anchor，Positive，Negative），其中Anchor与Positive同label，Anchor与Negative不同label（在人脸识别里面，就是Anchor与Positive是同一个个体，而与Negative是不同个体），通过学习优化这个embedding，使得欧式空间内的Anchor与Positive 的距离比与Negative的距离要近。</p><h2 id="公式表示"><a href="#公式表示" class="headerlink" title="公式表示"></a>公式表示</h2><p>用公式表示就是，我们希望：</p><p>$$<br>\left\lVert  f(x^a_i) - f(x^p_i) \right\rVert ^2_2  +<br>\alpha \lt \left\lVert  f(x^a_i) - f(x^n_i) \right\rVert ^2_2 , \<br>\forall (f(x^a_i) , f(x^p_i) , f(x^n_i))  \in \mathscr T<br>$$</p><p>其中$\alpha$ 是强制的正例和负例之间的margin，$\mathscr T$是具有基数为$N$的训练集中的三元组的集合。</p><p>那么，损失函数很自然的可以写为：</p><p>$$<br>\sum^N_i<br>\Bigl [<br>\left\lVert  f(x^a_i) - f(x^p_i) \right\rVert ^2_2   +<br> \left\lVert  f(x^a_i) - f(x^n_i) \right\rVert ^2_2 + \alpha<br> \Bigr ] _ +<br>$$</p><p>其中加号指的，如果中括号内部分大于0，则没有损失（Anchor与Positive的距离加上margin小于与Negative的距离），否则计算这个距离为损失。</p><h2 id="代码表示"><a href="#代码表示" class="headerlink" title="代码表示"></a>代码表示</h2><p>Numpy 实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">batch_size = <span class="number">3</span>*<span class="number">12</span></div><div class="line">embedding_size = <span class="number">16</span></div><div class="line"></div><div class="line"><span class="comment"># 构造batch_size * embedding_size 维度的随机矩阵</span></div><div class="line">emb = np.random.uniform(size=[])</div><div class="line"></div><div class="line"><span class="comment"># 对emb逢三取1、2、3行分别为Anchor、Positive、Negative</span></div><div class="line"><span class="comment"># 计算其2范数的距离即欧氏距离</span></div><div class="line">pos_dist_sqr = np.sum(np.square(emb[<span class="number">0</span>::<span class="number">3</span>,:]-emb[<span class="number">1</span>::<span class="number">3</span>,:]), axis=<span class="number">1</span>)        </div><div class="line">neg_dist_sqr = np.sum(np.square(emb[<span class="number">0</span>::<span class="number">3</span>,:]-emb[<span class="number">2</span>::<span class="number">3</span>,:]), axis=<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 这里就是照抄公式了，注意mean和sum是一样的</span></div><div class="line">np_triplet_loss = np.mean(np.maximum(<span class="number">0.</span>, pos_dist_sqr-neg_dist_sqr+alpha))</div></pre></td></tr></table></figure><p>TensorFlow 实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">batch_size = <span class="number">3</span>*<span class="number">12</span></div><div class="line">embedding_size = <span class="number">16</span></div><div class="line">alpha = <span class="number">0.2</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">triplet_loss</span><span class="params">(anchor, positive, negative, alpha)</span>:</span>   </div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'triplet_loss'</span>):</div><div class="line">        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), <span class="number">1</span>)</div><div class="line">        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), <span class="number">1</span>)</div><div class="line">        basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)</div><div class="line">        loss = tf.reduce_mean(tf.maximum(basic_loss, <span class="number">0.0</span>), <span class="keyword">None</span>)</div><div class="line">    <span class="keyword">return</span> loss</div><div class="line"></div><div class="line"><span class="comment"># 构建矩阵</span></div><div class="line">embeddings = tf.placeholder(np.float64, shape=(batch_size, embedding_size), name=<span class="string">'embeddings'</span>)</div><div class="line"><span class="comment"># 先将embeddings矩阵第0维resize为(?, 3)维，第1维不变，变为三维矩阵(-1, 3, embedding_size)，再在其第二维度为3上unstack为三份</span></div><div class="line">anchor, positive, negative = tf.unstack(tf.reshape(embeddings, shape=(<span class="number">-1</span>, <span class="number">3</span>, embedding_size)), axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><p>完整代码如下，这里测试对比了两种实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">batch_size = <span class="number">3</span>*<span class="number">12</span></div><div class="line">embedding_size = <span class="number">16</span></div><div class="line">alpha = <span class="number">0.2</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">triplet_loss</span><span class="params">(anchor, positive, negative, alpha)</span>:</span>   </div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'triplet_loss'</span>):</div><div class="line">        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), <span class="number">1</span>)</div><div class="line">        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), <span class="number">1</span>)</div><div class="line">        basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)</div><div class="line">        loss = tf.reduce_mean(tf.maximum(basic_loss, <span class="number">0.0</span>), <span class="keyword">None</span>)</div><div class="line">    <span class="keyword">return</span> loss</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Graph().as_default():</div><div class="line">    embeddings = tf.placeholder(np.float64, shape=(batch_size, embedding_size), name=<span class="string">'embeddings'</span>)</div><div class="line">    anchor, positive, negative = tf.unstack(tf.reshape(embeddings, shape=(<span class="number">-1</span>, <span class="number">3</span>, embedding_size)), axis=<span class="number">1</span>)</div><div class="line">    triplet_loss = triplet_loss(anchor, positive, negative, alpha)</div><div class="line">    </div><div class="line">    sess = tf.Session()</div><div class="line">    <span class="keyword">with</span> sess.as_default():</div><div class="line">        np.random.seed(<span class="number">666</span>)</div><div class="line">        emb = np.random.uniform(size=[batch_size, embedding_size])</div><div class="line">        tf_triplet_loss = sess.run(triplet_loss, feed_dict=&#123;embeddings:emb&#125;)</div><div class="line">        </div><div class="line">        pos_dist_sqr = np.sum(np.square(emb[<span class="number">0</span>::<span class="number">3</span>,:]-emb[<span class="number">1</span>::<span class="number">3</span>,:]), axis=<span class="number">1</span>)        </div><div class="line">        neg_dist_sqr = np.sum(np.square(emb[<span class="number">0</span>::<span class="number">3</span>,:]-emb[<span class="number">2</span>::<span class="number">3</span>,:]), axis=<span class="number">1</span>)</div><div class="line">        np_triplet_loss = np.mean(np.maximum(<span class="number">0.</span>, pos_dist_sqr-neg_dist_sqr+alpha))</div><div class="line">        </div><div class="line">        np.testing.assert_almost_equal(tf_triplet_loss, np_triplet_loss, decimal=<span class="number">5</span>, err_msg=<span class="string">'Triplet loss is incorrect'</span>)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15121200710041.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;理解triplet loss，与给出TensorFlow和numpy两种形式的example code。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://frankchen.xyz/tags/python/"/>
    
      <category term="Algorithm" scheme="http://frankchen.xyz/tags/Algorithm/"/>
    
      <category term="TensorFlow" scheme="http://frankchen.xyz/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>数据的标准化与归一化</title>
    <link href="http://frankchen.xyz/2017/11/29/Data-Normalization-and-Standardization/"/>
    <id>http://frankchen.xyz/2017/11/29/Data-Normalization-and-Standardization/</id>
    <published>2017-11-29T03:53:57.000Z</published>
    <updated>2017-12-01T11:10:13.685Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15119359203487.png" alt=""><br>聊一聊Normalization and Standardization<br><a id="more"></a></p><h2 id="什么是"><a href="#什么是" class="headerlink" title="什么是"></a>什么是</h2><p>Normalization就是归一化，是最小-最大缩放(min-max scaling)的特例，指的是将数据缩放到指定range，这个range通常是0~1或者-1~+1，直观来讲就是下图，在数据不包含离群点时很有用，<br><img src="/images/Screen%20Shot%202017-11-29%20at%2012.35.41.png" alt="Screen Shot 2017-11-29 at 12.35.41"></p><p>公式则是</p><p>$$<br>x^{(i)}_{norm} = \frac {x^{(i)} - x_{min}} {x_{max} - x_{min}}<br>$$</p><p>若要缩放至-1~+1，则是<br>$$<br>x’ = \frac{x - min}{max - min}<br>$$</p><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#导入数据预处理库</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</div><div class="line"></div><div class="line"><span class="comment">#范围缩放标准化</span></div><div class="line">min_max_scaler = preprocessing.MinMaxScaler()</div><div class="line"></div><div class="line"><span class="comment">#训练集缩放标准化</span></div><div class="line">min_max_scaler.fit_transform(X_train)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#测试集缩放标准化</span></div><div class="line">min_max_scaler.fit_transform(X_test)</div></pre></td></tr></table></figure><p>Z-score 标准化指的是，通过缩放让数据的均值为0（移除均值），标准差为固定值（比如1）。在许多模型里，如SVM的RBF、线性模型的 L1 &amp; L2 正则项对于所有的feature都有这样的假设。<br>$$<br>x^{(i)}_{std} = \frac{x^{(i)} - \mu_x}{\sigma_x}<br>$$</p><p>以下是一个简单的例子展示了两者的区别：</p><p><img src="/images/Screen%20Shot%202017-11-29%20at%2012.41.13.png" alt="Screen Shot 2017-11-29 at 12.41.13"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#导入数据预处理库</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</div><div class="line"></div><div class="line"><span class="comment">#数据标准化</span></div><div class="line">scaler = preprocessing.StandardScaler().fit(X_train)</div><div class="line"></div><div class="line"><span class="comment">#训练集数据标准化</span></div><div class="line">scaler.transform(X_train)</div></pre></td></tr></table></figure><p>同时对测试集的数据进行标准化处理，以保证训练集和测试集的变换方式相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#测试集数据标准化</span></div><div class="line">scaler.transform(X_test)</div></pre></td></tr></table></figure><h2 id="值得注意"><a href="#值得注意" class="headerlink" title="值得注意"></a>值得注意</h2><p>从流程上讲，标准化和归一化应该在读入数据、处理缺失值，切分训练测试集之后，而且我们要做的是在切分之后，在训练集fit，再去transform测试集，而不是在整个数据上转换以后再切分，因为<strong>无论是标准化还是归一化，我们要么利用到了数据的max min值，要么利用到了数据的均值和标准差，这些数值在训练之前是不能被测试集所影响的。</strong></p><p>类似于缺失值的填充，举个例子，我们使用均值填充以下数据，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#使用均值填充缺失值</span></div><div class="line">imp = Imputer(missing_values=<span class="string">"NaN"</span>, strategy=<span class="string">'mean'</span>, axis=<span class="number">0</span>)</div><div class="line">imp.fit(X_train)</div><div class="line"></div><div class="line"><span class="comment">#填充训练集</span></div><div class="line">X_train=imp.transform(X_train)</div></pre></td></tr></table></figure><p>以同样的方式填充测试集，以保证测试集和训练集的数据填充方式保持一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#填充测试集</span></div><div class="line">X_test=imp.transform(X_test)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15119359203487.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;聊一聊Normalization and Standardization&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://frankchen.xyz/tags/python/"/>
    
      <category term="Data Science" scheme="http://frankchen.xyz/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow on a GTX 1080</title>
    <link href="http://frankchen.xyz/2017/11/13/TensorFlow-on-a-GTX-1080/"/>
    <id>http://frankchen.xyz/2017/11/13/TensorFlow-on-a-GTX-1080/</id>
    <published>2017-11-13T10:23:28.000Z</published>
    <updated>2017-11-27T06:31:34.109Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15105686803158.jpg" alt=""><br><img src="/images/15105687114770.jpg" alt=""></p><p>Ubuntu 16.03 安装 CUDA、NVIDIA驱动，CUDNN及GPU版TensorFlow。<br><a id="more"></a><br>GPU 支持的TensorFlow让算力大幅提升，但是安装好一切支持却不那么容易！其实主要是三个东西：</p><ol><li>Nvidia 驱动：显卡驱动</li><li>CUDA Toolkit CUDA工具箱</li><li>CUDNN：CUDA Deep Neural Network library  神经网络库函数<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install \</div><div class="line">    freeglut3-dev \</div><div class="line">    g++-4.9 \</div><div class="line">    gcc-4.9 \</div><div class="line">    libglu1-mesa-dev \</div><div class="line">    libx11-dev \</div><div class="line">    libxi-dev \</div><div class="line">    libxmu-dev \</div><div class="line">    nvidia-modprobe \</div><div class="line">    python-dev \</div><div class="line">    python-pip \</div><div class="line">    python-virtualenv</div></pre></td></tr></table></figure><h2 id="安装Nvidia驱动"><a href="#安装Nvidia驱动" class="headerlink" title="安装Nvidia驱动"></a>安装Nvidia驱动</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get purge nvidia-* 删除nvidia 之前的</div><div class="line">$ sudo add-apt-repository ppa:graphics-drivers/ppa</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install nvidia-384</div></pre></td></tr></table></figure><p>可在<a href="https://launchpad.net/~graphics-drivers/+archive/ubuntu/ppa" target="_blank" rel="external">Proprietary GPU Drivers : “Graphics Drivers” team</a>查看当前稳定版本Nvidia驱动，如笔者当前（2017-11-13）版本是‘nvidia-384’。</p><p>接下来重启<code>$ sudo reboot</code>。<br>重启后，检测Nvidia驱动安装情况，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cat /proc/driver/nvidia/version</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">NVRM version: NVIDIA UNIX x86_64 Kernel Module  384.98  Thu Oct 26 15:16:01 PDT 2017</div><div class="line">GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)</div></pre></td></tr></table></figure><p>显示Nvidia’s system management interface：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo nvidia-smi</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| NVIDIA-SMI 384.98                 Driver Version: 384.98                    |</div><div class="line">|-------------------------------+----------------------+----------------------+</div><div class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</div><div class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</div><div class="line">|===============================+======================+======================|</div><div class="line">|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |</div><div class="line">|  0%   47C    P8    12W / 215W |   7992MiB /  8112MiB |      2%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line"></div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| Processes:                                                       GPU Memory |</div><div class="line">|  GPU       PID   Type   Process name                             Usage      |</div><div class="line">|=============================================================================|</div><div class="line">|    0       994      G   /usr/lib/xorg/Xorg                           193MiB |</div><div class="line">|    0      1889      G   compiz                                       151MiB |</div><div class="line">|    0      5068      C   /home/frank/anaconda3/bin/python            7643MiB |</div><div class="line">+-----------------------------------------------------------------------------+</div></pre></td></tr></table></figure><p>设置GCC 4.9为默认</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 10</div><div class="line">$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 20</div><div class="line"></div><div class="line">$ sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 10</div><div class="line">$ sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 20</div></pre></td></tr></table></figure><h2 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h2><p>当前虽然CUDA-9.0已经发布，但是TensorFlow默认编译版本还是基于CUDA-8.0的，我们在这里<a href="https://developer.nvidia.com/cuda-80-ga2-download-archive" target="_blank" rel="external">CUDA Toolkit 8.0 - Feb 2017 | NVIDIA Developer</a>下载runfile<br><img src="/images/Screen%20Shot%202017-11-13%20at%2018.35.28.png" alt="Screen Shot 2017-11-13 at 18.35.28"></p><p>使用如下安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo cuda_8.0.61_375.26_linux.run --override</div></pre></td></tr></table></figure></p><p>安装时记得</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">Do you accept the previously <span class="built_in">read</span> EULA? (accept/decline/quit): accept</div><div class="line">You are attempting to install on an unsupported configuration. Do you wish to <span class="built_in">continue</span>? ((y)es/(n)o) [ default is no ]: yes</div><div class="line">Install NVIDIA Accelerated Graphics Driver <span class="keyword">for</span> Linux-x86_64 352.39? ((y)es/(n)o/(q)uit): no</div><div class="line">Install the CUDA 8.0 Toolkit? ((y)es/(n)o/(q)uit): yes</div><div class="line">Enter Toolkit Location [ default is /usr/<span class="built_in">local</span>/cuda-8.0 ]:</div><div class="line">Do you want to install a symbolic link at /usr/<span class="built_in">local</span>/cuda? ((y)es/(n)o/(q)uit): yes</div><div class="line">Install the CUDA 8.0 Samples? ((y)es/(n)o/(q)uit): no</div><div class="line">Installing the CUDA Toolkit <span class="keyword">in</span> /usr/<span class="built_in">local</span>/cuda-8.0 ...</div><div class="line"></div><div class="line">===========</div><div class="line">= Summary =8.0</div><div class="line">===========</div><div class="line"></div><div class="line">Driver:   Not Selected</div><div class="line">Toolkit:  Installed <span class="keyword">in</span> /usr/<span class="built_in">local</span>/cuda-8.0</div><div class="line">Samples:  Not Selected</div><div class="line"></div><div class="line">Please make sure that</div><div class="line"> -   PATH includes /usr/<span class="built_in">local</span>/cuda-8.0/bin</div><div class="line"> -   LD_LIBRARY_PATH includes /usr/<span class="built_in">local</span>/cuda-8.0/lib64, or, add /usr/<span class="built_in">local</span>/cuda-8.0/lib64 to /etc/ld.so.conf and run ldconfig as root</div><div class="line"></div><div class="line">To uninstall the CUDA Toolkit, run the uninstall script <span class="keyword">in</span> /usr/<span class="built_in">local</span>/cuda-8.0/bin</div><div class="line">To uninstall the NVIDIA Driver, run nvidia-uninstall</div><div class="line"></div><div class="line">Please see CUDA_Installation_Guide_Linux.pdf <span class="keyword">in</span> /usr/<span class="built_in">local</span>/cuda-8.0/doc/pdf <span class="keyword">for</span> detailed information on setting up CUDA.</div><div class="line"></div><div class="line">***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 352.00 is required <span class="keyword">for</span> CUDA 8.0 functionality to work.</div><div class="line">To install the driver using this installer, run the following <span class="built_in">command</span>, replacing &lt;CudaInstaller&gt; with the name of this run file:</div><div class="line">    sudo &lt;CudaInstaller&gt;.run -silent -driver</div><div class="line"></div><div class="line">Logfile is /tmp/cuda_install_14557.log</div></pre></td></tr></table></figure><p>记得上面这里也有个询问你是否安装Nvidia驱动的地方，因为我们前面已经安装了最新的版本，这里当然选择no。</p><p>添加环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">echo</span> <span class="string">'export PATH=/usr/local/cuda/bin:$PATH'</span> &gt;&gt; ~/.bashrc</div><div class="line">$ <span class="built_in">echo</span> <span class="string">'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH'</span> &gt;&gt; ~/.bashrc</div><div class="line">$ <span class="built_in">source</span> ~/.bashrc</div></pre></td></tr></table></figure><p>查看CUDA compiler</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ nvcc -V</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">nvcc: NVIDIA (R) Cuda compiler driver</div><div class="line">Copyright (c) 2005-2016 NVIDIA Corporation</div><div class="line">Built on Tue_Jan_10_13:22:03_CST_2017</div><div class="line">Cuda compilation tools, release 8.0, V8.0.61</div></pre></td></tr></table></figure><h2 id="安装CUDA-Deep-Neural-Network-library-：CUDNN"><a href="#安装CUDA-Deep-Neural-Network-library-：CUDNN" class="headerlink" title="安装CUDA Deep Neural Network library ：CUDNN"></a>安装CUDA Deep Neural Network library ：CUDNN</h2><p>在此处下载<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="external">cuDNN Download | NVIDIA Developer</a>，可能需要我们注册账号登录。<br>选择适配CUDA的版本，以及cuDNN v7.0 Library for Linux，这个就是个targz文件。<br><img src="/images/Screen%20Shot%202017-11-13%20at%2018.42.51.png" alt="Screen Shot 2017-11-13 at 18.42.51"></p><p>接下来操作就是把cudnn的几个库放到cuda里面：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ tar xvf cudnn-8.0-linux-x64-v7.tgz</div><div class="line">$ sudo cp cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda/include/</div><div class="line">$ sudo cp cuda/lib64/libcudnn* /usr/<span class="built_in">local</span>/cuda/lib64/</div><div class="line">$ sudo chmod a+r /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn*</div></pre></td></tr></table></figure></p><h2 id="TensorFlow安装"><a href="#TensorFlow安装" class="headerlink" title="TensorFlow安装"></a>TensorFlow安装</h2><p><code>pip  install --upgrade tfBinaryURL</code>即可，这里的<code>tfBinaryURL</code>可在<a href="https://www.tensorflow.org/install/install_linux#the_url_of_the_tensorflow_python_package" target="_blank" rel="external">Installing TensorFlow on Ubuntu  |  TensorFlow</a>选取，例如我这里选取Python3.6的GPU Support：</p><p><img src="/images/Screen%20Shot%202017-11-13%20at%2018.47.45.png" alt="Screen Shot 2017-11-13 at 18.47.45"></p><h2 id="验证TensorFlow安装"><a href="#验证TensorFlow安装" class="headerlink" title="验证TensorFlow安装"></a>验证TensorFlow安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">In [1]: import tensorflow as tf</div><div class="line"></div><div class="line">In [2]: sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</div><div class="line">2017-11-13 18:54:59.081831: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA</div><div class="line">2017-11-13 18:54:59.186280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node <span class="built_in">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</div><div class="line">2017-11-13 18:54:59.186604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:</div><div class="line">name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.86</div><div class="line">pciBusID: 0000:01:00.0</div><div class="line">totalMemory: 7.92GiB freeMemory: 7.46GiB</div><div class="line">2017-11-13 18:54:59.186617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)</div><div class="line">Device mapping:</div><div class="line">/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1</div><div class="line">2017-11-13 18:54:59.216573: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:</div><div class="line">/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1</div></pre></td></tr></table></figure><p>如上，打印出这些信息就证明安装成功啦！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15105686803158.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/15105687114770.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Ubuntu 16.03 安装 CUDA、NVIDIA驱动，CUDNN及GPU版TensorFlow。&lt;br&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="http://frankchen.xyz/tags/Linux/"/>
    
      <category term="TensorFlow" scheme="http://frankchen.xyz/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>frp的内网穿透及外网访问内网jupyter-notebook的实现</title>
    <link href="http://frankchen.xyz/2017/11/12/ftp-using/"/>
    <id>http://frankchen.xyz/2017/11/12/ftp-using/</id>
    <published>2017-11-11T18:20:38.000Z</published>
    <updated>2017-11-13T08:02:04.255Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15104246654723.png" alt=""></p><a id="more"></a><p><a href="https://github.com/fatedier/frp" target="_blank" rel="external">fatedier/frp: A fast reverse proxy to help you expose a local server behind a NAT or firewall to the internet.</a>是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp, http, https 协议， 这里我们用它来搭建外网访问内网jupyter-notebook的服务。<br>我们基于Ubuntu16.04 选用amd64版本，<br><img src="/images/Screen%20Shot%202017-11-12%20at%2002.25.46.png" alt="Screen Shot 2017-11-12 at 02.25.46"></p><h3 id="server设置"><a href="#server设置" class="headerlink" title="server设置"></a>server设置</h3><p>server就是你拥有外网IP的服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[common]</div><div class="line">bind_port = 7000</div><div class="line">vhost_http_port = 8888</div></pre></td></tr></table></figure></p><p>用<code>./frps -c ./frps.ini</code>启动，注意服务端需要先启动。</p><h3 id="client设置"><a href="#client设置" class="headerlink" title="client设置"></a>client设置</h3><p>client就是没有外网IP，但是你想在外网访问的机器，</p><p>XXXXXXXXX就是上面的server的外网IP。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[common]</div><div class="line">server_addr = XXXXXXXXX</div><div class="line">server_port = 7000</div><div class="line"></div><div class="line">[web]</div><div class="line"><span class="built_in">type</span> = http</div><div class="line">local_port = 8888</div><div class="line">custom_domains = XXXXXXXXX</div></pre></td></tr></table></figure><p>以<code>./frpc -c ./frpc.ini</code>启动。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>接下来，在client端，在8888段端口启动jupyter-notebook即可在XXXXXXXXX:8888访问内网机器上的Notebook了。</p><p><img src="/images/Screen%20Shot%202017-11-12%20at%2002.37.55.png" alt="Screen Shot 2017-11-12 at 02.37.55"></p><p>另外，由于jupyter-notebook自带终端，这也一举两得，也是一个内网穿透ssh的方案。<br>当然，必须使用一些运维工具来保证服务的稳定性，如supervisor，可参考<a href="http://frankchen.xyz/2017/07/06/Use-supervisor-support-Python3-program/">使用supervisor支持Python3程序 | 不正经数据科学家</a>。<br><img src="/images/Screen%20Shot%202017-11-12%20at%2002.39.36.png" alt="Screen Shot 2017-11-12 at 02.39.36"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15104246654723.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="http://frankchen.xyz/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Pycharm Pro</title>
    <link href="http://frankchen.xyz/2017/11/08/pycharm-pro/"/>
    <id>http://frankchen.xyz/2017/11/08/pycharm-pro/</id>
    <published>2017-11-08T10:44:13.000Z</published>
    <updated>2017-11-09T08:31:53.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15101362178541.jpg" alt=""><br>Pycharm，只为提高python开发者的生产力！<br><a id="more"></a></p><h2 id="版本管理"><a href="#版本管理" class="headerlink" title="版本管理"></a>版本管理</h2><p><img src="/images/Screen%20Shot%202017-11-08%20at%2018.17.59.png" alt="Screen Shot 2017-11-08 at 18.17.59"><br>版本管理，主要是依靠这两个按钮，左边是pull，右边是commit。一般我们开发，打开项目，先pull下代码仓库的变更，开始开发，然后commit，再pull，合并冲突，再push。<br><img src="/images/Screen%20Shot%202017-11-08%20at%2018.21.55.png" alt="Screen Shot 2017-11-08 at 18.21.55"><br>pycharm非常人性化地为我们标出了，黑色则是没有变更，蓝色是有变更，绿色是新add的文件。<br><img src="/images/Screen%20Shot%202017-11-08%20at%2018.23.21.png" alt="Screen Shot 2017-11-08 at 18.23.21"><br>commit时，可以很方便地看出变更对比，对于需要回滚的零时操作文件可以用紫色的revert按钮回退变更，总之填写commit message之后就可以commit了。<br>然后，为防止在此期间，代码仓库又有人push了新变更，在push之前，我们需要再次pull，如果没有变更，push即可。<br><img src="/images/Screen%20Shot%202017-11-08%20at%2018.26.24.png" alt="Screen Shot 2017-11-08 at 18.26.24"><br>如果有冲突呢，pycharm有非常human的解决冲突界面，<br><img src="/images/15101368989889.jpg" alt=""><br>总之，选择修改的、丢弃的、保留的，就可以push了，当然，这次push会有两条message，第二条是解决冲突的。</p><h2 id="远程调试"><a href="#远程调试" class="headerlink" title="远程调试"></a>远程调试</h2><p>利用pycharm我们可以在服务器直接run、debug，非常的便捷。<br>首先要设置deployment，<br><img src="/images/Screen%20Shot%202017-11-08%20at%2018.31.20.png" alt="Screen Shot 2017-11-08 at 18.31.20"><br>选择SFTP，也就是ssh，这里用密码或者私钥都是ok的。设置好映射目录，<br><img src="/images/Screen%20Shot%202017-11-08%20at%2018.31.56.png" alt="Screen Shot 2017-11-08 at 18.31.56"><br>接下来添加远程解释器，<br><img src="/images/Screen%20Shot%202017-11-08%20at%2018.33.05.png" alt="Screen Shot 2017-11-08 at 18.33.05"><br>，并勾选auto upload，<br><img src="/images/Screen%20Shot%202017-11-08%20at%2018.33.46.png" alt="Screen Shot 2017-11-08 at 18.33.46"><br>那么，每次本地的更改都会同步到服务器，直接run或者debug都是获取服务器的结果，非常方便。</p><p>## </p><h2 id="DEBUG"><a href="#DEBUG" class="headerlink" title="DEBUG"></a>DEBUG</h2><p>DEBUG可谓是开发中最最重要的技能了，也许我们在初级的开发的时候，还可以依靠各种print变量来查看，那么工程一旦变复杂，debug就必不可少了，；举个栗子，我们用TensorFlow写深度学习工程的时候，如果出现矩阵维度不匹配的情况，这个时候用debug去观察各个维度就相当高效了。<br>以下图为例，我们做debug，当然先要在我们希望观察的代码处打断点（Break Point），就是左边的红色小点，第一个断点debug停下来的地方。接下来就是debug按钮了，也就是红框内的这些箭头，依次是</p><ul><li>Step Over 直接从当前断点步进到下一个断点，也就是我们不希望看到中间的任何调用，直接跳过</li><li>Step Into，从当前断点，每一次调用都进入，如果调用比较多，可能很繁因为会一步步深入</li><li>Step Into Mycode，简单明了，一行一行的步进，不跳转到调用</li><li>Force Step Into，强制步进进入</li><li>Step Out 跳出当前调用，和Step Into结合，一个进入、一个跳出</li><li>Run to Cursor 不需打断点，直接步进到光标所在处，这个也很方便，只需把光标放在某处点击即可<br><img src="/images/Screen%20Shot%202017-11-09%20at%2015.49.12.png" alt="Screen Shot 2017-11-09 at 15.49.12"><br>此处类似堆栈，后面调用的进程在上面。<br><img src="/images/Screen%20Shot%202017-11-09%20at%2016.19.42.png" alt="Screen Shot 2017-11-09 at 16.19.42"><br>当然，最主要的，上面这些步骤按钮的最终目的就是观察Variable的变化，通过步进观察各个变量的信息，以找出bug等等。<br><img src="/images/Screen%20Shot%202017-11-09%20at%2016.21.15.png" alt="Screen Shot 2017-11-09 at 16.21.15"></li></ul><h2 id="Python-Console"><a href="#Python-Console" class="headerlink" title="Python Console"></a>Python Console</h2><p>相比iPython与Notebook，Pycharm自带的Python Console有其独到的优势，比如Special Variables与Code History ，如图，分别用一个两个三个下划线代表上一次上两次上三次的变量，在这里可以直接像debug一样查看各个变量的值；Code History则是使用户方便地使用之前的代码。<br><img src="/images/Screen%20Shot%202017-11-09%20at%2015.23.52.png" alt="Screen Shot 2017-11-09 at 15.23.52"></p><h2 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h2><h3 id="调整代码行次序"><a href="#调整代码行次序" class="headerlink" title="调整代码行次序"></a>调整代码行次序</h3><p>Command + Shift + ⬆️/⬇️<br>有时我们需要调整两行代码的上下次序，那么用剪切、粘贴的方法不如这个方法简洁自然。</p><h3 id="查看源码及跳转"><a href="#查看源码及跳转" class="headerlink" title="查看源码及跳转"></a>查看源码及跳转</h3><p>摁住Command键去点击即可跳转到源码处，而查看源码时我们可以通过Command + [/]来方便的前进或后退。</p><h3 id="批量展开收缩代码"><a href="#批量展开收缩代码" class="headerlink" title="批量展开收缩代码"></a>批量展开收缩代码</h3><p>Command + Shift +/-</p><p>当项目写到一定规模的时候，难免方法/函数会很多，这个时候我们可以使用此命令来收缩代码，这个主要是为了方便查看。</p><h3 id="快速插入常用代码"><a href="#快速插入常用代码" class="headerlink" title="快速插入常用代码"></a>快速插入常用代码</h3><p>Command + J 是弹出插入常用代码块的快捷键，比如Dict/List/Set 的comprehension都有，之前我只会‘main’然后跳出<code>if __name__ == &#39;__main__&#39;:</code>😜</p><p><img src="/images/2017/06/05.png" alt=""></p><h3 id="一键-PEP8"><a href="#一键-PEP8" class="headerlink" title="一键 PEP8"></a>一键 PEP8</h3><p>其实在了解这个tips之前我都是点击函数名，等待一个黄色的小灯泡再去点击灯泡。。。其实只需要<code>Command+Option+L</code>即可！</p><h3 id="cheat-sheet"><a href="#cheat-sheet" class="headerlink" title="cheat sheet"></a>cheat sheet</h3><p>顺便更新两张cheat sheet</p><p><img src="/images/2017/06/06.png" alt=""><br><img src="/images/2017/06/07.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15101362178541.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;Pycharm，只为提高python开发者的生产力！&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://frankchen.xyz/tags/python/"/>
    
      <category term="Pycharm" scheme="http://frankchen.xyz/tags/Pycharm/"/>
    
  </entry>
  
  <entry>
    <title>Go 语言操作与扫描 Hbase 实例</title>
    <link href="http://frankchen.xyz/2017/11/08/go-hbase/"/>
    <id>http://frankchen.xyz/2017/11/08/go-hbase/</id>
    <published>2017-11-08T03:10:21.000Z</published>
    <updated>2017-11-09T02:01:07.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15101107329028.jpg" alt=""><br><img src="/images/15101107462857.jpg" alt=""><br>记录纯go语言的gohbase客户端的扫描操作。<br><a id="more"></a></p><figure class="highlight go"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> main</div><div class="line"></div><div class="line"><span class="keyword">import</span> (</div><div class="line"><span class="string">"github.com/tsuna/gohbase"</span></div><div class="line"><span class="string">"github.com/tsuna/gohbase/hrpc"</span></div><div class="line"><span class="string">"context"</span></div><div class="line"><span class="string">"io"</span></div><div class="line"><span class="string">"fmt"</span></div><div class="line"><span class="string">"github.com/tsuna/gohbase/filter"</span></div><div class="line"><span class="string">"strconv"</span></div><div class="line"><span class="string">"time"</span></div><div class="line">)</div><div class="line"></div><div class="line"><span class="keyword">const</span> table = <span class="string">"user"</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">func</span> <span class="title">beforeMiniTimeStamps</span><span class="params">(beforeMini <span class="keyword">int</span>)</span> <span class="title">string</span></span> &#123;</div><div class="line"><span class="comment">//当前时刻某分钟之前的时间戳</span></div><div class="line"><span class="keyword">return</span> strconv.Itoa(<span class="keyword">int</span>(time.Now().Add(- time.Duration(beforeMini) * time.Minute).UnixNano() / <span class="number">1000000</span>))</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">func</span> <span class="title">fetch</span><span class="params">()</span> []*<span class="title">hrpc</span>.<span class="title">Result</span></span> &#123;</div><div class="line">client := gohbase.NewClient(<span class="string">"hmaster.shise.com,rm.shise.com,nn.shise.com"</span>)</div><div class="line"><span class="comment">//client := gohbase.NewClient("wwj.shise.com,czn.shise.com,czn.shise.com")</span></div><div class="line"><span class="comment">// 列族</span></div><div class="line">family := hrpc.Families(<span class="keyword">map</span>[<span class="keyword">string</span>][]<span class="keyword">string</span>&#123;<span class="string">"c"</span>: <span class="literal">nil</span>&#125;)</div><div class="line"></div><div class="line"><span class="comment">// 全局hbase filter时间间隔</span></div><div class="line"><span class="comment">//timeRange := hrpc.TimeRange(time.Now().Add(- time.Duration(minute)*time.Minute), time.Now())</span></div><div class="line"></div><div class="line"><span class="comment">// 某列value的filter</span></div><div class="line">notRecommendFilter := filter.NewSingleColumnValueFilter([]<span class="keyword">byte</span>(<span class="string">"c"</span>),</div><div class="line">[]<span class="keyword">byte</span>(<span class="string">"notRecommend"</span>),</div><div class="line">filter.NotEqual,</div><div class="line">filter.NewBinaryComparator(filter.NewByteArrayComparable([]<span class="keyword">byte</span>(<span class="string">"true"</span>))),</div><div class="line"><span class="literal">true</span>,</div><div class="line"><span class="literal">true</span>)</div><div class="line">violationFilter := filter.NewSingleColumnValueFilter([]<span class="keyword">byte</span>(<span class="string">"c"</span>),</div><div class="line">[]<span class="keyword">byte</span>(<span class="string">"violation"</span>),</div><div class="line">filter.NotEqual,</div><div class="line">filter.NewBinaryComparator(filter.NewByteArrayComparable([]<span class="keyword">byte</span>(<span class="string">"true"</span>))),</div><div class="line"><span class="literal">true</span>,</div><div class="line"><span class="literal">true</span>)</div><div class="line"><span class="comment">//filter某列value的时间戳</span></div><div class="line">timeStartFilter := filter.NewSingleColumnValueFilter([]<span class="keyword">byte</span>(<span class="string">"c"</span>),</div><div class="line">[]<span class="keyword">byte</span>(<span class="string">"createDate"</span>),</div><div class="line">filter.Greater,</div><div class="line">filter.NewBinaryComparator(filter.NewByteArrayComparable([]<span class="keyword">byte</span>(beforeMiniTimeStamps(<span class="number">4</span>*<span class="number">60</span>)))),</div><div class="line"><span class="literal">true</span>,</div><div class="line"><span class="literal">true</span>)</div><div class="line">timeEndFilter := filter.NewSingleColumnValueFilter([]<span class="keyword">byte</span>(<span class="string">"c"</span>),</div><div class="line">[]<span class="keyword">byte</span>(<span class="string">"createDate"</span>),</div><div class="line">filter.Less,</div><div class="line">filter.NewBinaryComparator(filter.NewByteArrayComparable([]<span class="keyword">byte</span>(beforeMiniTimeStamps(<span class="number">2</span>*<span class="number">60</span>)))),</div><div class="line"><span class="literal">true</span>,</div><div class="line"><span class="literal">true</span>)</div><div class="line"></div><div class="line"><span class="comment">//filter 列表</span></div><div class="line">filters := filter.NewList(filter.MustPassAll, notRecommendFilter, violationFilter, timeStartFilter, timeEndFilter)</div><div class="line"><span class="comment">//创建scan对象</span></div><div class="line">scan, _ := hrpc.NewScanStr(context.Background(), table, family, hrpc.Filters(filters))</div><div class="line"></div><div class="line"><span class="keyword">var</span> rsp []*hrpc.Result</div><div class="line">scanner := client.Scan(scan)</div><div class="line"><span class="keyword">for</span> &#123;</div><div class="line">res, err := scanner.Next()</div><div class="line"><span class="keyword">if</span> err == io.EOF &#123;</div><div class="line"><span class="keyword">break</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</div><div class="line"><span class="built_in">print</span>(err)</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> hasHeadImage(res) &#123;</div><div class="line">rsp = <span class="built_in">append</span>(rsp, res)</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> rsp</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">func</span> <span class="title">hasHeadImage</span><span class="params">(res *hrpc.Result)</span> <span class="title">bool</span></span> &#123;</div><div class="line"><span class="keyword">return</span> <span class="literal">true</span></div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</div><div class="line">rsp := fetch()</div><div class="line"><span class="keyword">for</span> _, item := <span class="keyword">range</span> rsp &#123;</div><div class="line">fmt.Println(*item)</div><div class="line"><span class="keyword">break</span></div><div class="line">&#125;</div><div class="line">fmt.Println(<span class="built_in">len</span>(rsp))</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15101107329028.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/15101107462857.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;记录纯go语言的gohbase客户端的扫描操作。&lt;br&gt;
    
    </summary>
    
    
      <category term="Go" scheme="http://frankchen.xyz/tags/Go/"/>
    
      <category term="Hbase" scheme="http://frankchen.xyz/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04 安装IKEV2 VPN 并在Mac上使用以解决Google Drive 同步问题</title>
    <link href="http://frankchen.xyz/2017/11/01/Google-Drive-Sync/"/>
    <id>http://frankchen.xyz/2017/11/01/Google-Drive-Sync/</id>
    <published>2017-11-01T01:56:43.000Z</published>
    <updated>2017-11-01T02:21:02.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15095018008987.jpg" alt=""><br><img src="/images/15095018930547.jpg" alt=""></p><a id="more"></a><p>Google Drive无法识别shadowsocks所用的socks5代理，故这边有需求在VPS上部署http代理的VPN。</p><h2 id="服务端安装说明"><a href="#服务端安装说明" class="headerlink" title="服务端安装说明"></a>服务端安装说明</h2><ul><li><p>下载脚本:</p><p>   <code>wget --no-check-certificate https://raw.githubusercontent.com/quericy/one-key-ikev2-vpn/master/one-key-ikev2.sh</code></p></li><li><p>运行脚本：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">chmod +x one-key-ikev2.sh</div><div class="line">bash one-key-ikev2.sh</div></pre></td></tr></table></figure></li><li><p>等待自动配置部分内容后，选择vps类型（OpenVZ还是Xen、KVM），选错将无法成功连接，请务必核实服务器的类型。输入服务器ip或者绑定的域名(连接vpn时服务器地址将需要与此保持一致,如果是导入泛域名证书这里需要写*.域名的形式),这里推荐直接输入域名。</p></li><li>选择使用使用证书颁发机构签发的SSL证书还是生成自签名证书，这里我们选择自签名即选择no：‘’<ul><li>如果选择no,使用自签名证书（客户端如果使用IkeV2方式连接，将需要导入生成的证书并信任）则需要填写证书的相关信息(C,O,CN)，为空将使用默认值(default value)，确认无误后按任意键继续,后续安装过程中会出现输入两次pkcs12证书的密码的提示(可以设置为空)    </li></ul></li><li>接下来一直空格即可。<br>*看到install Complete字样即表示安装完成。默认用户名密码将以黄字显示，可根据提示自行修改配置文件中的用户名密码,多用户则在配置文件中按格式一行一个(多用户时用户名不能使用%any),保存并重启服务生效。</li><li>将提示信息中的证书文件ca.cert.pem拷贝到客户端，修改后缀名为.cer后导入。ios设备使用Ikev1无需导入证书，而是需要在连接时输入共享密钥，共享密钥即是提示信息中的黄字PSK.<ul><li><img src="/images/15095022245212.jpg" alt=""><h2 id="客户端配置说明"><a href="#客户端配置说明" class="headerlink" title="客户端配置说明"></a>客户端配置说明</h2></li></ul></li><li>iOS/OSX/Windows7+/WindowsPhone8.1+/Linux 均可使用IkeV2,认证方式为用户名+密码。使用SSL证书则无需导入证书；使用自签名证书则需要先导入证书才能连接,可将ca.cert.pem更改后缀名作为邮件附件发送给客户端,手机端也可通过浏览器导入,其中:<ul><li>iOS/OSX 的远程ID和服务器地址保持一致,用户鉴定选择”用户名”.如果通过浏览器导入,将证书放在可访问的远程外链上,并在系统浏览器(Safari)中访问外链地址;</li></ul></li><li>注意OSX导入后需要在钥匙串内设置信任，如：<ul><li><img src="/images/15095023464796.png" alt=""></li></ul></li><li>设置连接成功后Google Drive就会连接成功开始同步<br>😎😎😎😎😎😎<ul><li><img src="/images/Screen%20Shot%202017-11-01%20at%2010.13.03.png" alt="Screen Shot 2017-11-01 at 10.13.03"></li></ul></li></ul><h2 id="参考自"><a href="#参考自" class="headerlink" title="参考自"></a>参考自</h2><ul><li><a href="https://quericy.me/blog/699/" target="_blank" rel="external">CentOS/Ubuntu一键安装IPSEC/IKEV2 VPN服务器 | Quericy Eden*</a></li><li><a href="https://github.com/quericy/one-key-ikev2-vpn/issues/58" target="_blank" rel="external">安装完成后，mac不能连接 · Issue #58 · quericy/one-key-ikev2-vpn</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15095018008987.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/15095018930547.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="VPN" scheme="http://frankchen.xyz/tags/VPN/"/>
    
  </entry>
  
  <entry>
    <title>Install Opencv3.2 on Ununtu 16.04</title>
    <link href="http://frankchen.xyz/2017/10/25/Install-Opencv3-2-on-Ununtu-16-04/"/>
    <id>http://frankchen.xyz/2017/10/25/Install-Opencv3-2-on-Ununtu-16-04/</id>
    <published>2017-10-25T04:34:28.000Z</published>
    <updated>2017-11-09T02:00:24.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15101928106001.jpg" alt=""></p><p>Opencv3.2 在 Ununtu 16.04 上的编译安装<br><a id="more"></a></p><p>参考自<a href="http://blog.topspeedsnail.com/archives/4755" target="_blank" rel="external">Ubuntu 16.04编译安装OpenCV（Python） – WTF Daily Blog</a>，不过这位博主装的是3.1版本，而且有些问题。</p><h2 id="安装OpenCV依赖"><a href="#安装OpenCV依赖" class="headerlink" title="安装OpenCV依赖"></a>安装OpenCV依赖</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get upgrade</div><div class="line">sudo apt-get install build-essential cmake pkg-config</div><div class="line"> sudo apt-get install libjpeg8-dev libtiff5-dev libjasper-dev libpng12-dev</div><div class="line"> sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev</div><div class="line"> sudo apt-get install libxvidcore-dev libx264-dev</div><div class="line">sudo apt-get install libgtk-3-dev</div><div class="line">sudo apt-get install libatlas-base-dev gfortran</div><div class="line">sudo apt-get install python2.7-dev python3.5-dev</div></pre></td></tr></table></figure><h2 id="下载OpenCV源码"><a href="#下载OpenCV源码" class="headerlink" title="下载OpenCV源码"></a>下载OpenCV源码</h2><p>这里下载 3.2.0</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> ~</div><div class="line">$ wget -O opencv.zip https://github.com/Itseez/opencv/archive/3.2.0.zip</div><div class="line">$ unzip opencv.zip</div></pre></td></tr></table></figure><p>下载和OpenCV版本对应的opencv_contrib（一些扩展功能和non-free代码）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ wget -O opencv_contrib.zip https://github.com/Itseez/opencv_contrib/archive/3.2.0.zip</div><div class="line">$ unzip opencv_contrib.zip</div></pre></td></tr></table></figure></p><h2 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> ~/opencv-3.2.0/</div><div class="line">$ mkdir build</div><div class="line">$ <span class="built_in">cd</span> build</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE \</div><div class="line">    -D CMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> \</div><div class="line">    -D INSTALL_PYTHON_EXAMPLES=ON \</div><div class="line">    -D INSTALL_C_EXAMPLES=OFF \</div><div class="line">    -D OPENCV_EXTRA_MODULES_PATH=/root/Downloads/opencv_contrib-3.2.0/modules  -D PYTHON_EXECUTABLE=/root/miniconda3/bin/python  ..</div></pre></td></tr></table></figure><p>其中<code>OPENCV_EXTRA_MODULES_PATH</code>是opencv_contrib的解压后的地址，<code>PYTHON_EXECUTABLE</code>是# 你的python 解释器地址 可用<code>witch python</code> 查看。<br>若出现，需要下载<code>ippicv_linux_20151201.tgz</code>的长时间等待，可在此<a href="https://github.com/opencv/opencv_3rdparty/tree/ippicv/master_20151201/ippicv" target="_blank" rel="external">opencv_3rdparty/ippicv at ippicv/master_20151201 · opencv/opencv_3rdparty</a>手动下载对应文件，并放在对应位置如Put the ippicv_linux…tgz under<br>&lt;…&gt;/opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e/， 参考自<a href="https://github.com/opencv/opencv/issues/5973" target="_blank" rel="external">incorrect hash in cmake ippicv when installing · Issue #5973 · opencv/opencv</a>。</p><p>编译：</p><p><code>$ make</code></p><p>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo make install</div><div class="line">$ sudo ldconfig</div></pre></td></tr></table></figure><p>再<code>pip install opencv</code>即可😎</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15101928106001.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Opencv3.2 在 Ununtu 16.04 上的编译安装&lt;br&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="http://frankchen.xyz/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>git notes in action | 生产环境各场景下git常用命令</title>
    <link href="http://frankchen.xyz/2017/10/21/git-notes-in-action/"/>
    <id>http://frankchen.xyz/2017/10/21/git-notes-in-action/</id>
    <published>2017-10-21T06:03:47.000Z</published>
    <updated>2017-11-09T01:59:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15085663573989.jpg" alt=""></p><a id="more"></a><ol><li>更改远程链接 <a href="https://help.github.com/articles/changing-a-remote-s-url/" target="_blank" rel="external">Changing a remote’s URL - User Documentation</a></li><li>redo add单文件操作 <a href="http://data.agaric.com/undo-git-add-remove-files-staged-git-commit" target="_blank" rel="external">Undo a git add - remove files staged for a git commit | Open Data</a> </li><li>移除所有已经add的文件，场景例如你刚刚add了all，才发现有许多是可以ignore的，那么就运行<code>git rm --cached -r .</code>移除也就是撤回刚刚add的所有文件，再去管理ignore，再add就好了。</li><li>revert当前文件变更，场景例如比如你上传本地的一个config上服务器调试，调试结束，需要将这个文件回滚变更，那么<code>git checkout -- &lt;file-you-want-revert&gt;</code>即可。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15085663573989.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="git" scheme="http://frankchen.xyz/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>服务器负载GUI 神器sargraph 的安装</title>
    <link href="http://frankchen.xyz/2017/10/11/sargraph-install/"/>
    <id>http://frankchen.xyz/2017/10/11/sargraph-install/</id>
    <published>2017-10-11T03:40:21.000Z</published>
    <updated>2017-10-12T09:25:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>通过Linux 的<code>sar</code>命令可以很容易知道服务器的负载，那么如何通过网页等更好地可视化呢？本文介绍实现此功能的神器<a href="http://www.sargraph.com/sargraph/chart.php?id=dbserv" target="_blank" rel="external">SARGRAPH-Graphical front-end for sar</a>的使用及安装。</p><h2 id="sar的配置"><a href="#sar的配置" class="headerlink" title="sar的配置"></a>sar的配置</h2><p>通过这里我们可以看到<br><a href="http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/sar.html" target="_blank" rel="external">sar 找出系统瓶颈的利器 — Linux Tools Quick Tutorial</a><br>安装sar</p><blockquote><ol><li>有的linux系统下，默认可能没有安装这个包，使用apt-get install sysstat 来安装；</li><li>安装完毕，将性能收集工具的开关打开： vi /etc/default/sysstat</li><li>设置 ENABLED=”true”</li><li>启动这个工具来收集系统性能数据： /etc/init.d/sysstat start<a id="more"></a>可使用命令<code>vi /etc/cron.d/sysstat</code>调整报告频率，例如下面就将默认的十分钟修改为隔两分钟报告一次。<br><img src="/images/Screen%20Shot%202017-10-12%20at%2017.23.55.png" alt="Screen Shot 2017-10-12 at 17.23.55"></li></ol></blockquote><h2 id="sargraph-的安装"><a href="#sargraph-的安装" class="headerlink" title="sargraph 的安装"></a>sargraph 的安装</h2><h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><p>根据<a href="http://sargraph.com/index.php?option=com_content&amp;view=article&amp;id=48&amp;Itemid=13" target="_blank" rel="external">Documentation</a>，<br><img src="/images/Screen%20Shot%202017-10-11%20at%2011.52.07-1.png" alt="Screen Shot 2017-10-11 at 11.52.07"></p><ul><li>安装php</li></ul><p><code>sudo apt-get install php libapache2-mod-php</code></p><ul><li>安装apache2 </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install apache2</div></pre></td></tr></table></figure><h2 id="配置apache2"><a href="#配置apache2" class="headerlink" title="配置apache2"></a>配置apache2</h2><p><code>systemctl status apache2</code>查看apache2的情况，若发现其不是active，可能是由于与nginx监听默认端口冲突，那么需要在<code>vim /etc/apache2/ports.conf</code>，把80修改为81即可。<br><img src="/images/Screen%20Shot%202017-10-11%20at%2011.57.39.png" alt="Screen Shot 2017-10-11 at 11.57.39"><br>再‘service apache2 restart’，查看<code>systemctl status apache2</code>为active即成功，</p><p><img src="/images/Screen%20Shot%202017-10-11%20at%2011.55.59.png" alt="Screen Shot 2017-10-11 at 11.55.59"></p><h2 id="sargraph安装及配置"><a href="#sargraph安装及配置" class="headerlink" title="sargraph安装及配置"></a>sargraph安装及配置</h2><p>参考<a href="http://www.sargraph.com/index.php?option=com_content&amp;view=article&amp;id=47&amp;Itemid=14" target="_blank" rel="external">Download Sargraph</a>安装即可，</p><blockquote><p>Download sargraph_version3.tgz to /tmp. Unzip and untar it. And run the INSTALLER</p><p>tar xzf sargraph_version3.tgz</p><p>cd sargraph_version3</p><p> ./INSTALLER</p></blockquote><p>注意需要修改其config，<code>vim /etc/sargraph.conf</code>，</p><p><img src="/images/Screen%20Shot%202017-10-11%20at%2012.00.55.png" alt="Screen Shot 2017-10-11 at 12.00.55"></p><p>注意是SARUSER修改为需要监听的服务器的用户名，KEY修改为sargraph服务器当前用户公钥文件即可。</p><h3 id="添加server"><a href="#添加server" class="headerlink" title="添加server"></a>添加server</h3><p>使用<code>/var/www/html/sargraph/scripts/addserver datalab</code>添加server，比如之前我们把config里user改为root，那么这里我们添加的server就是<code>root@datalab</code>，再可用<code>/var/www/html/sargraph/scripts</code>里的脚本添加删除用户修改密码等等。</p><p>最后在如<a href="http://datalab:81/sargraph/" target="_blank" rel="external">Sargraph Login</a>访问即可，负载等信息可视化出现！<img src="/images/Screen%20Shot%202017-10-11%20at%2012.05.12.png" alt="Screen Shot 2017-10-11 at 12.05.12"></p><p>😎</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过Linux 的&lt;code&gt;sar&lt;/code&gt;命令可以很容易知道服务器的负载，那么如何通过网页等更好地可视化呢？本文介绍实现此功能的神器&lt;a href=&quot;http://www.sargraph.com/sargraph/chart.php?id=dbserv&quot;&gt;SARGRAPH-Graphical front-end for sar&lt;/a&gt;的使用及安装。&lt;/p&gt;
&lt;h2 id=&quot;sar的配置&quot;&gt;&lt;a href=&quot;#sar的配置&quot; class=&quot;headerlink&quot; title=&quot;sar的配置&quot;&gt;&lt;/a&gt;sar的配置&lt;/h2&gt;&lt;p&gt;通过这里我们可以看到&lt;br&gt;&lt;a href=&quot;http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/sar.html&quot;&gt;sar 找出系统瓶颈的利器 — Linux Tools Quick Tutorial&lt;/a&gt;&lt;br&gt;安装sar&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;有的linux系统下，默认可能没有安装这个包，使用apt-get install sysstat 来安装；&lt;/li&gt;
&lt;li&gt;安装完毕，将性能收集工具的开关打开： vi /etc/default/sysstat&lt;/li&gt;
&lt;li&gt;设置 ENABLED=”true”&lt;/li&gt;
&lt;li&gt;启动这个工具来收集系统性能数据： /etc/init.d/sysstat start
    
    </summary>
    
    
      <category term="Linux" scheme="http://frankchen.xyz/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>避免 spark 提交 上传自带 jar包解决办法</title>
    <link href="http://frankchen.xyz/2017/09/01/spark-submit-avoid-upload-jars/"/>
    <id>http://frankchen.xyz/2017/09/01/spark-submit-avoid-upload-jars/</id>
    <published>2017-09-01T08:15:04.000Z</published>
    <updated>2017-09-01T08:25:21.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">17/09/01 15:38:59 INFO yarn.Client: Uploading resource file:/usr/<span class="built_in">local</span>/spark-2.1.1-bin-without-hadoop/spark-46d1bd70-b346-4027-bce4-9540f4b6035a/__spark_libs__4051900056689219834.zip -&gt; hdfs://wwj.shise.com:9000/user/hadoop/.sparkStaging/application_1504148698505_0021/__spark_libs__4051900056689219834.zip</div><div class="line">17/09/01 15:41:45 INFO yarn.Client: Uploading resource file:/Users/frank/IdeaProjects/simpleApp/target/scala-2.11/simpleApp-assembly-1.0.jar -&gt; hdfs://wwj.shise.com:9000/user/hadoop/.sparkStaging/application_1504148698505_0021/simpleApp-assembly-1.0.jar</div></pre></td></tr></table></figure><p>可以看到，上传花费约3分钟，这段时间是为了将$SPARK_HOME/jar下的所有jar包上传到yarn，实际上可以完全避免。<br><img src="/images/Screen%20Shot%202017-09-01%20at%2016.16.35.png" alt="Screen Shot 2017-09-01 at 16.16.35"></p><p>实际上这部分文件完全可以就放在hdfs上，<br><img src="/images/Screen%20Shot%202017-09-01%20at%2016.21.32.png" alt="Screen Shot 2017-09-01 at 16.21.32"></p><p>先将这部分jar包复制到hdfs：<br>hadoop fs -mkdir /tmp/spark/lib_jars/<br>hadoop fs -put  $SPARK_HOME/jars/* /tmp/spark/lib_jars/</p><p>设置<code>vim $SPARK_HOME/conf/spark-defaults.conf</code>：<br>添加这行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">spark.yarn.jars                  /tmp/spark/lib_jars/* <span class="comment">##这里用hdfs相对路径即可</span></div></pre></td></tr></table></figure></p><p>再submit不会出现将jar文件打包成zip文件上传的信息了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;co
      
    
    </summary>
    
    
      <category term="spark" scheme="http://frankchen.xyz/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>hbase rest 绑定到内网localhost</title>
    <link href="http://frankchen.xyz/2017/08/29/hbase-rest-bindAddress-to-localhost/"/>
    <id>http://frankchen.xyz/2017/08/29/hbase-rest-bindAddress-to-localhost/</id>
    <published>2017-08-29T08:34:08.000Z</published>
    <updated>2017-08-29T08:42:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>通过开启Hbase 的REST 服务我们可以很方便的以API的形式访问Hbase，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Foreground</span></div><div class="line">$ bin/hbase rest start -p &lt;port&gt;</div><div class="line"></div><div class="line"><span class="comment"># Background, logging to a file in $HBASE_LOGS_DIR</span></div><div class="line">$ bin/hbase-daemon.sh start rest -p &lt;port&gt;</div></pre></td></tr></table></figure><p>但是其默认是绑定<code>0.0.0.0</code>地址的，也就是对外网开放，而通过REST 服务别有用心的人是可以删表的。。。如何只对内网开放呢？</p><p>查了无数中英文网页不得，最后决定：看源码！最后在这里发现如下片段，<a href="https://github.com/apache/hbase/blob/master/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java" target="_blank" rel="external">hbase/RESTServer.java at master · apache/hbase</a><br><img src="/images/Screen%20Shot%202017-08-29%20at%2016.30.16.png" alt="Screen Shot 2017-08-29 at 16.30.16"><br>那么解决方法就显而易见了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># sudo vim /usr/local/hbase/conf/hbase-site.xml</span></div><div class="line">&lt;configuration&gt;</div><div class="line">&lt;property&gt;</div><div class="line">         &lt;name&gt;hbase.rest.host&lt;/name&gt;</div><div class="line">         &lt;value&gt;127.0.0.1&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;通过开启Hbase 的REST 服务我们可以很方便的以API的形式访问Hbase，&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;
      
    
    </summary>
    
    
      <category term="Hbase" scheme="http://frankchen.xyz/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>scala-notes</title>
    <link href="http://frankchen.xyz/2017/08/22/scala-note/"/>
    <id>http://frankchen.xyz/2017/08/22/scala-note/</id>
    <published>2017-08-22T03:06:48.000Z</published>
    <updated>2017-08-22T03:06:48.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="call-by-name-与-call-by-value的区别"><a href="#call-by-name-与-call-by-value的区别" class="headerlink" title="call by name 与 call by value的区别"></a>call by name 与 call by value的区别</h2><p>两者的区别就是调用之前需不需要evaluation，前者不需要，后者需要。例如一个函数$f(x, y) = x$，我们分别调用$f(1+1, 2 )$，call by name 直接引用1+1，再计算出为2，而 call by value是先算出函数参数的值，再去调用$f(2, 2 )$，scala默认是call by value，但是可以在需要call by name 的参数加箭头如<code>=&gt;</code>。</p><p>scala里面定义变量<code>def</code>和<code>val</code>的区别即在此，前者是call by name，例如我们分别定义两个函数：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span></span>: <span class="type">Boolean</span> = loop</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">x</span> </span>= loop</div><div class="line"></div><div class="line"><span class="keyword">val</span> y = loop</div></pre></td></tr></table></figure><p>函数x可以被成功定义，而后者不行，因为在call by name 的参数evaluation的时候就进入死循环了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;call-by-name-与-call-by-value的区别&quot;&gt;&lt;a href=&quot;#call-by-name-与-call-by-value的区别&quot; class=&quot;headerlink&quot; title=&quot;call by name 与 call by value的区
      
    
    </summary>
    
    
      <category term="scala" scheme="http://frankchen.xyz/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>详解Coursera 奖学金申请步骤</title>
    <link href="http://frankchen.xyz/2017/08/21/coursera-scholarship/"/>
    <id>http://frankchen.xyz/2017/08/21/coursera-scholarship/</id>
    <published>2017-08-21T04:06:29.000Z</published>
    <updated>2017-09-05T09:30:38.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/15046037829976.jpg" alt=""><br>最新Coursera 奖学金申请步骤！成功率100%🏆</p><a id="more"></a><p>以这门<a href="https://www.coursera.org/learn/progfun1" target="_blank" rel="external">Scala 函数式程序设计原理 | Coursera</a>为例：</p><p><img src="/images/Screen%20Shot%202017-08-21%20at%2012.09.20.png" alt="Screen Shot 2017-08-21 at 12.09.20"><br><img src="/images/Screen%20Shot%202017-08-21%20at%2012.16.21.png" alt="Screen Shot 2017-08-21 at 12.16.21"><br><img src="/images/Screen%20Shot%202017-08-21%20at%2012.17.47.png" alt="Screen Shot 2017-08-21 at 12.17.47"><br>以下问题必须填写英文，必须大于等于150字，从我的申请记录来看，基本不会审核你究竟填写了什么，这只是为免费用户设置一个门槛，吸引你氪金😂😎<br><img src="/images/Screen%20Shot%202017-08-21%20at%2012.19.16.png" alt="Screen Shot 2017-08-21 at 12.19.16"><br><img src="/images/Screen%20Shot%202017-08-21%20at%2012.19.26.png" alt="Screen Shot 2017-08-21 at 12.19.26"><br><img src="/images/Screen%20Shot%202017-08-21%20at%2012.19.30.png" alt="Screen Shot 2017-08-21 at 12.19.30"></p><p>之前是立即可以获得证书，现在需要等待十五天即可。<br><img src="/images/Screen%20Shot%202017-08-21%20at%2012.20.21.png" alt="Screen Shot 2017-08-21 at 12.20.21"></p><p>关于上面的三个问题回复提供一模板：</p><blockquote><p>I am a graduate student in mainland China, was born in a peasant family, the family has four people, my father, my mother, my brother and I, we four and grandparents living in the same home. Mom and Dad did not work, can only rely on two acres of the family to be barely subsistence income is very meager, good for my brother and I can learn, the family live frugally money supply year my brother and I go to school Reading used. The family also owe a lot of money, so my brother and I grew to know two people not to spend money.</p><p>Dad, Mom no cultural knowledge, they know the importance of knowledge, so small they are strict requirements of our brothers and both learn to be good in the future to test a good university, find a good job, do not like them, did not work in the countryside. We two brothers are also very competitive, it has been among the best in school, in our view, only with honors in order to make my parents happy, to return to their pains. Monthly income of around one hundred US dollars to pay tuition for this course certificate will be spent half of my cost of living, it will bring a lot of economic pressure to my normal life, and this course is the first door I finished on the site class, have a special meaning for me, I wanted to get this certificate course, this will inspire my passion for learning and motivation.</p><p>1, many people have recommended this course for the learning experience to enhance learning method above, I think learning is a lifelong, this course I will gain the knowledge of my lifetime. All the copyright.</p><p>This door exercise logic and thinking about Coursera course is one of the most popular courses, course descriptions and practical reasoning methods and common logical fallacies, teach you how to properly reasoning, learning a few simple but critical general rules apply to all topics, while avoiding prone to problems when reasoning. 2, almost all individuals can enhance the quality of life of the curriculum to enhance areas of interest I have, I will continue to follow up. 3. Harvest course certificate for me is a recognition of my pay, my motivation for future learning enhance the effect is self-evident, thank you!</p><p>1, I guarantee independence to complete a full course, to ensure that all academic tasks independently myself by myself.</p><p>2, actively participate in discussions and course work, upload your own achievement, strive to contribute their efforts for curriculum community.</p><p>3, actively publicize the site to friends and relatives, for future expansion of community development programs and make a contribution.</p><p>4, to participate in and complete the course more sites to learn more new knowledge.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/15046037829976.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;最新Coursera 奖学金申请步骤！成功率100%🏆&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coursera" scheme="http://frankchen.xyz/tags/Coursera/"/>
    
  </entry>
  
  <entry>
    <title>server certificate verification failed solution</title>
    <link href="http://frankchen.xyz/2017/08/15/server-certificate-verification-failed-solution/"/>
    <id>http://frankchen.xyz/2017/08/15/server-certificate-verification-failed-solution/</id>
    <published>2017-08-15T10:23:03.000Z</published>
    <updated>2017-08-15T10:26:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>server certificate verification failed. CAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none 的正确解决方法。</p><p>出现此错误时，问题出在证书的缺失，不可用如<code>export GIT_SSL_NO_VERIFY=1</code>方法去解除安全限制，正确方法是下载证书，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hostname=XXX</div><div class="line">port=443</div><div class="line">trust_cert_file_location=`curl-config --ca`</div><div class="line">sudo bash -c <span class="string">"echo -n | openssl s_client -showcerts -connect <span class="variable">$hostname</span>:<span class="variable">$port</span> 2&gt;/dev/null  | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt;&gt; <span class="variable">$trust_cert_file_location</span>"</span></div></pre></td></tr></table></figure><p>若不起作用，可用IP代替真实hostname。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;server certificate verification failed. CAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none 的正确解决方法。&lt;/p&gt;
&lt;p&gt;出现此错误时，问题出在证书的缺失，不可用如&lt;co
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://frankchen.xyz/tags/Linux/"/>
    
  </entry>
  
</feed>
