{"meta":{"title":"不正经数据科学家","subtitle":"Enjoy everything fun and challenging","description":"当你的才华还撑不起你的野心时，你就应该静下心来学习。","author":"江南消夏","url":"http://frankchen.xyz"},"pages":[{"title":"404","date":"2016-11-12T17:28:54.000Z","updated":"2016-11-12T17:28:54.000Z","comments":true,"path":"404/index.html","permalink":"http://frankchen.xyz/404/index.html","excerpt":"","text":""},{"title":"categories","date":"2017-10-27T03:51:19.000Z","updated":"2017-10-27T06:20:09.000Z","comments":true,"path":"categories/index.html","permalink":"http://frankchen.xyz/categories/index.html","excerpt":"","text":""},{"title":"About Me","date":"2016-11-12T17:00:21.000Z","updated":"2018-03-20T08:56:46.176Z","comments":true,"path":"about/index.html","permalink":"http://frankchen.xyz/about/index.html","excerpt":"","text":"本科毕业于西安电子科技大学的通信工程专业，现为深圳大学信息工程学院的研三学生。 原误入完全不感兴趣的通信坑，现在迈入数据科学世界，为了成为独当一面的算法工程师！ 冒昧问一句Witcher 3是不是最好玩的游戏？😘我想现在我找到答案了。。。"},{"title":"标签","date":"2016-11-12T17:00:37.000Z","updated":"2017-10-27T06:20:25.000Z","comments":true,"path":"tags/index.html","permalink":"http://frankchen.xyz/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"处理大数据集的建议","slug":"handle-big-datasets","date":"2018-04-10T07:07:34.000Z","updated":"2018-04-10T08:15:51.972Z","comments":true,"path":"2018/04/10/handle-big-datasets/","link":"","permalink":"http://frankchen.xyz/2018/04/10/handle-big-datasets/","excerpt":"最近的一些比赛如TalkingData AdTracking Fraud Detection Challenge | Kaggle提供了很大的数据集，一般来说，只有16G的内存的“小”电脑都无法直接处理这种数据集了，本文收集了一些关于处理这种数据的建议，供大家参考。","text":"最近的一些比赛如TalkingData AdTracking Fraud Detection Challenge | Kaggle提供了很大的数据集，一般来说，只有16G的内存的“小”电脑都无法直接处理这种数据集了，本文收集了一些关于处理这种数据的建议，供大家参考。 1.及时删除无用变量并垃圾回收通常我们在特征工程中会涉及大量的转换操作，产生很多的中间变量等，除了使用del以外，使用gc.collect()也是个不错的选择。 12345678temp = pd.read_csv('../input/train_sample.csv')#do something to the filetemp['os'] = temp['os'].astype('str')#delete when no longer neededdel temp#collect residual garbagegc.collect() 2.预定义数据类型pandas一般会自己推断数据类型，不过倾向于使用耗费空间大的，如下面例子所示，预定义数据类型节省了超过一半的空间。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475dtypes = &#123; 'ip' : 'uint32', 'app' : 'uint16', 'device' : 'uint16', 'os' : 'uint16', 'channel' : 'uint16', 'is_attributed' : 'uint8', &#125;dtypes2 = &#123; 'ip' : 'int32', 'app' : 'int16', 'device' : 'int16', 'os' : 'int16', 'channel' : 'int16', 'is_attributed' : 'int8', &#125;train = pd.read_csv(train_sample_file,parse_dates=['click_time'])#check datatypes:train.info()train = pd.read_csv(train_sample_file,dtype=dtypes,parse_dates=['click_time'])#check datatypes:train.info()train = pd.read_csv(train_sample_file,dtype=dtypes2,parse_dates=['click_time'])#check datatypes:train.info()'''&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 100000 entries, 0 to 99999Data columns (total 8 columns):ip 100000 non-null int64app 100000 non-null int64device 100000 non-null int64os 100000 non-null int64channel 100000 non-null int64click_time 100000 non-null datetime64[ns]attributed_time 227 non-null objectis_attributed 100000 non-null int64dtypes: datetime64[ns](1), int64(6), object(1)memory usage: 6.1+ MB&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 100000 entries, 0 to 99999Data columns (total 8 columns):ip 100000 non-null uint32app 100000 non-null uint16device 100000 non-null uint16os 100000 non-null uint16channel 100000 non-null uint16click_time 100000 non-null datetime64[ns]attributed_time 227 non-null objectis_attributed 100000 non-null uint8dtypes: datetime64[ns](1), object(1), uint16(4), uint32(1), uint8(1)memory usage: 2.8+ MB&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 100000 entries, 0 to 99999Data columns (total 8 columns):ip 100000 non-null int32app 100000 non-null int16device 100000 non-null int16os 100000 non-null int16channel 100000 non-null int16click_time 100000 non-null datetime64[ns]attributed_time 227 non-null objectis_attributed 100000 non-null int8dtypes: datetime64[ns](1), int16(4), int32(1), int8(1), object(1)memory usage: 2.8+ MB''' 3.只使用csv文件内的指定行a) 指定行数直接使用nrows指定 1train = pd.read_csv('../input/train.csv', nrows=1e5, dtype=dtypes) b) 跳过行数比如我们跳过前500w取100w下面保留了head， 12train = pd.read_csv('../input/train.csv', skiprows=range(1, 5000000), nrows=1000000, dtype=dtypes) c) sampling1234567891011import subprocessprint('# Line count:')for file in ['train.csv', 'test.csv', 'train_sample.csv']: lines = subprocess.run(['wc', '-l', '../input/&#123;&#125;'.format(file)], stdout=subprocess.PIPE).stdout.decode('utf-8') print(lines, end='', flush=True)'''# Line count:184903891 ../input/train.csv18790470 ../input/test.csv100001 ../input/train_sample.csv''' train一共有lines=184903891 行，那么假设我们需要采样出100w行，那么我们需要跳过lines - 1 - 1000000行，即 123456789101112131415161718192021#generate list of lines to skipskiplines = np.random.choice(np.arange(1, lines), size=lines-1-1000000, replace=False)#sort the listskiplines=np.sort(skiplines)#check our listprint('lines to skip:', len(skiplines))print('remaining lines in sample:', lines-len(skiplines), '(remember that it includes the heading!)')###################SANITY CHECK####################find lines that weren't skipped by checking difference between each consecutive line#how many out of first 100000 will be imported into the csv?diff = skiplines[1:100000]-skiplines[2:100001]remain = sum(diff!=-1)print('Ratio of lines from first 100000 lines:', '&#123;0:.5f&#125;'.format(remain/100000) ) print('Ratio imported from all lines:', '&#123;0:.5f&#125;'.format((lines-len(skiplines))/lines) )train = pd.read_csv('../input/train.csv', skiprows=skiplines, dtype=dtypes)train.head()del skiplinesgc.collect() 4.使用pandas 的生成器，用chunk处理这里我们使用np.where过滤掉‘is_attributed’为0的部分（例如[xv if c else yv for (c,xv,yv) in zip(condition,x,y)]）12345678910#set up an empty dataframedf_converted = pd.DataFrame()#we are going to work with chunks of size 1 million rowschunksize = 10 ** 6#in each chunk, filter for values that have 'is_attributed'==1, and merge these values into one dataframefor chunk in pd.read_csv('../input/train.csv', chunksize=chunksize, dtype=dtypes): filtered = (chunk[(np.where(chunk['is_attributed']==1, True, False))]) df_converted = pd.concat([df_converted, filtered], ignore_index=True, ) 5.只载入若干列1234567891011121314151617181920#wanted columnscolumns = ['ip', 'click_time', 'is_attributed']dtypes = &#123; 'ip' : 'uint32', 'is_attributed' : 'uint8', &#125;ips_df = pd.read_csv('../input/train.csv', usecols=columns, dtype=dtypes)print(ips_df.info())ips_df.head()'''&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 184903890 entries, 0 to 184903889Data columns (total 3 columns):ip uint32click_time objectis_attributed uint8dtypes: object(1), uint32(1), uint8(1)memory usage: 2.2+ GBNone''' 6.结合多种方法创意性的处理数据例如无法使用整个数据来groupby那么可以分块来做， 1234567891011121314151617181920size=100000all_rows = len(ips_df)num_parts = all_rows//size#generate the first batchip_sums = ips_df[0:size][['ip', 'is_attributed']].groupby('ip', as_index=False).sum()#add remaining batchesfor p in range(1,num_parts): start = p*size end = p*size + size if end &lt; all_rows: group = ips_df[start:end][['ip', 'is_attributed']].groupby('ip', as_index=False).sum() else: group = ips_df[start:][['ip', 'is_attributed']].groupby('ip', as_index=False).sum() ip_sums = ip_sums.merge(group, on='ip', how='outer') ip_sums.columns = ['ip', 'sum1','sum2'] ip_sums['conversions_per_ip'] = np.nansum((ip_sums['sum1'], ip_sums['sum2']), axis = 0) ip_sums.drop(columns=['sum1', 'sum2'], axis = 0, inplace=True) 7.使用dask代替pandas12import daskimport dask.dataframe as dd","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"Data Science","slug":"Data-Science","permalink":"http://frankchen.xyz/tags/Data-Science/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://frankchen.xyz/tags/Kaggle/"}]},{"title":"机器学习之 sklearn中的pipeline","slug":"pipeline-in-machine-learning","date":"2018-04-08T08:13:42.000Z","updated":"2018-04-10T06:04:14.946Z","comments":true,"path":"2018/04/08/pipeline-in-machine-learning/","link":"","permalink":"http://frankchen.xyz/2018/04/08/pipeline-in-machine-learning/","excerpt":"","text":"如图所示，利用pipeline我们可以方便的减少代码量同时让机器学习的流程变得直观， 例如我们需要做如下操作，容易看出，训练测试集重复了代码， 123456789101112vect = CountVectorizer()tfidf = TfidfTransformer()clf = SGDClassifier()vX = vect.fit_transform(Xtrain)tfidfX = tfidf.fit_transform(vX)predicted = clf.fit_predict(tfidfX)# Now evaluate all steps on test setvX = vect.fit_transform(Xtest)tfidfX = tfidf.fit_transform(vX)predicted = clf.fit_predict(tfidfX) 利用pipeline，上面代码可以抽象为， 12345678pipeline = Pipeline([ ('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier()),])predicted = pipeline.fit(Xtrain).predict(Xtrain)# Now evaluate all steps on test setpredicted = pipeline.predict(Xtest) 注意，pipeline最后一步如果有predict()方法我们才可以对pipeline使用fit_predict()，同理，最后一步如果有transform()方法我们才可以对pipeline使用fit_transform()方法。 使用pipeline做cross validation看如下案例，即先对输入手写数字的数据进行PCA降维，再通过逻辑回归预测标签。其中我们通过pipeline对PCA的降维维数n_components和逻辑回归的正则项C大小做交叉验证，主要步骤有： 依次实例化各成分对象如pca = decomposition.PCA() 以(name, object)的tuble为元素组装pipeline如Pipeline(steps=[(&#39;pca&#39;, pca), (&#39;logistic&#39;, logistic)]) 初始化CV参数如n_components = [20, 40, 64] 实例化CV对象如estimator = GridSearchCV(pipe, dict(pca__n_components=n_components, logistic__C=Cs))，其中注意参数的传递方式，即key为pipeline元素名+函数参数 123456789101112131415161718192021222324252627282930313233343536import numpy as npimport matplotlib.pyplot as pltfrom sklearn import linear_model, decomposition, datasetsfrom sklearn.pipeline import Pipelinefrom sklearn.model_selection import GridSearchCVlogistic = linear_model.LogisticRegression()pca = decomposition.PCA()pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])digits = datasets.load_digits()X_digits = digits.datay_digits = digits.target# Predictionn_components = [20, 40, 64]Cs = np.logspace(-4, 4, 3)pca.fit(X_digits)estimator = GridSearchCV(pipe, dict(pca__n_components=n_components, logistic__C=Cs))estimator.fit(X_digits, y_digits)plt.figure(1, figsize=(4, 3))plt.clf()plt.axes([.2, .2, .7, .7])plt.plot(pca.explained_variance_, linewidth=2)plt.axis('tight')plt.xlabel('n_components')plt.ylabel('explained_variance_')plt.axvline( estimator.best_estimator_.named_steps['pca'].n_components, linestyle=':', label='n_components chosen')plt.legend(prop=dict(size=12))plt.show() 自定义transformerFeatureUnionsklearn.pipeline.FeatureUnion — scikit-learn 0.19.1 documentation 和pipeline的序列执行不同，FeatureUnion指的是并行地应用许多transformer在input上，再将结果合并，所以自然地适合特征工程中的增加特征，而FeatureUnion与pipeline组合可以方便的完成许多复杂的操作，例如如下的例子， 123456789101112pipeline = Pipeline([ ('extract_essays', EssayExractor()), ('features', FeatureUnion([ ('ngram_tf_idf', Pipeline([ ('counts', CountVectorizer()), ('tf_idf', TfidfTransformer()) ])), ('essay_length', LengthTransformer()), ('misspellings', MispellingCountTransformer()) ])), ('classifier', MultinomialNB())]) 整个features是一个FeatureUnion，而其中的ngram_tf_idf又是一个包括两步的pipeline。 下面的例子中，使用FeatureUnion结合PCA降维后特征以及选择原特征中的几个作为特征组合再喂给SVM分类，最后用grid_search 做了 pca的n_components、SelectKBest的k以及SVM的C的CV。 123456789101112131415161718192021222324252627282930313233343536373839from sklearn.pipeline import Pipeline, FeatureUnionfrom sklearn.model_selection import GridSearchCVfrom sklearn.svm import SVCfrom sklearn.datasets import load_irisfrom sklearn.decomposition import PCAfrom sklearn.feature_selection import SelectKBestiris = load_iris()X, y = iris.data, iris.targetprint(X.shape, y.shape)# This dataset is way too high-dimensional. Better do PCA:pca = PCA()# Maybe some original features where good, too?selection = SelectKBest()# Build estimator from PCA and Univariate selection:svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])), (\"svm\", svm)])param_grid = dict( features__pca__n_components=[1, 2, 3], features__univ_select__k=[1, 2], svm__C=[0.1, 1, 10])grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_)","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://frankchen.xyz/tags/Data-Science/"}]},{"title":"VPS搭建私人BT离线服务器","slug":"private-BT-server","date":"2018-04-08T03:13:51.000Z","updated":"2018-04-08T03:47:02.802Z","comments":true,"path":"2018/04/08/private-BT-server/","link":"","permalink":"http://frankchen.xyz/2018/04/08/private-BT-server/","excerpt":"使用闲置的VPS搭建私人BT离线服务器的方法，亦或者推广至树莓派或者家用路由器亦可。","text":"使用闲置的VPS搭建私人BT离线服务器的方法，亦或者推广至树莓派或者家用路由器亦可。 安装及配置 Transmission 安装 sudo apt-get install transmission-daemon 配置 停止服务（否则配置文件锁定，无法修改）sudo service transmission-daemon stop 编辑配置文件 sudo vim /etc/transmission-daemon/settings.json 1234567&#123; \"ratio-limit\": 0.0100, \"ratio-limit-enabled\": true, \"rpc-password\": \"*******\", \"rpc-username\": \"frank\", \"download-dir\": \"/var/www/html/Downloads\", &#125; 我只列出了我修改过且无法在 Transmission Web-GUI 中无法完成修改的几项，四项依次是下载完成做种率，开启限制做种率，Web-GUI 密码，Web-GUI 用户名。像保存路径，下载/ 上传速度限制，都可以在 Web-GUI 中直接设定，为了方便之后对下载文件的 Web 管理，我直接将保存路径改到了 Web 发布路径下的一个子目录。 重启服务 sudo service transmission-daemon start 此时在浏览器打开VPS的IP地址/域名:9091并输入刚刚设置的用户名及密码应该就可以访问 Transmission 的 Web-GUI了。 可是在添加了第一个任务后出现保存路径写入权限的问题。解决办法如Permission denied when downloading with transmission deamon - Ask Ubuntu所示： 我们的下载地址是 /var/www/html/Downloads 用户名是znwindy:那么 1234567891011121314# 将本用户加入 `debian-transmission`组sudo usermod -a -G debian-transmission znwindy# 文件夹所有者sudo chgrp debian-transmission /var/www/html/Downloads# 组添加写权限sudo chmod -R 755 /var/www# 停止后台deamon sudo service transmission-daemon stop# 更改 file creation masksudo vim /etc/transmission-daemon/settings.json# 把\"umask\": 18 改为 \"umask\": 2# 重启服务sudo service transmission-daemon start 即可解决写的问题。 配置 Apache 加密区域安装apache2 12sudo apt-get updatesudo apt-get install apache2 Adjust the Firewall 1234sudo ufw app listsudo ufw allow 'Apache Full'sudo ufw statussudo systemctl status apache2 密码生成 1sudo htpasswd -c /etc/apache2/.htpasswd 用户名 然后会被提示输入两次该 “用户名” 的密码。 修改虚拟 host 的配置文件sudo vim /etc/apache2/sites-enabled/000-default.conf 123456&lt;Directory \"/var/www/html\"&gt; AuthType Basic AuthName \"Restricted Content\" AuthUserFile /etc/apache2/.htpasswd Require valid-user&lt;/Directory&gt; 保存后重启 sudo service apache2 restart 总结通过 HTTP 将下载的任务取回本地，速度也很快！这样，通过访问 Transmission Web-GUI “投喂” 种子，磁力链，然后在下载完成后通过 HTTP 方式从 VPS 将资源取回本地，甚至直接对 .mp3、.mp4 等文件格式进行在线播放，实现了一个简化版的迅雷离线下载，可是它却在下载某些特定资源时远比迅雷离线管用。 参考自 在 VPS 上搭建私人离线下载","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"},{"name":"Old Driver","slug":"Old-Driver","permalink":"http://frankchen.xyz/tags/Old-Driver/"}]},{"title":"numpy 中增加channel的方法","slug":"numpy-add-channel","date":"2018-03-29T12:15:30.000Z","updated":"2018-03-29T12:48:49.127Z","comments":true,"path":"2018/03/29/numpy-add-channel/","link":"","permalink":"http://frankchen.xyz/2018/03/29/numpy-add-channel/","excerpt":"numpy 数组中一维怎么转二维和多维？简述 numpy 中增加channel的方法。","text":"numpy 数组中一维怎么转二维和多维？简述 numpy 中增加channel的方法。 在机器学习中，所有的数据都是向量和矩阵，而怎么根据我们所要解决的问题来调整模型以及数据的格式，也就是矩阵的维度和大小是一项重要的基本功，那么本文就具体介绍下numpy中数组的转换，也就是增加channel的方法。 一维转二维例如我们有一个一维的numpy array，有如下方法可以转为二维 123456789101112131415161718192021a = np.arange(10)aa.shapeb = a[:,None]bb.shape'''array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])(10,)array([[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]])(10, 1)''' 可以看到，a确实被转为了二维，以下方法是一样的： 12345678910111213141516171819import numpy as npc = a[:,np.newaxis]c(c == b).all()np.newaxis == None'''array([[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]])TrueTrue''' 转为多维时间序列预测中，我们一般需要的是(sample，time_stamp，feature)的3 个channel的数据，即一个三维矩阵，包含若干个sample，每个sample包含若干个时间序列点，而每个时间序列点有包括若干个feature，哪怕我们只是做单变量的时间序列预测，输入RNN网络例如LSTM的时候，数据也必须是三维的格式，下面我们讲一讲这么做的方法。 例如我们有一个若干个时间点每个时间点有两个特征的数据， 123456789101112131415161718a = np.arange(24).reshape((-1,2))a.shapea'''(12, 2)array([[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7], [ 8, 9], [10, 11], [12, 13], [14, 15], [16, 17], [18, 19], [20, 21], [22, 23]])''' 我们将a转化为三个channel，即可以 1234567891011121314151617181920212223242526272829b = a[:,None,:]b.shapeb'''(12, 1, 2)array([[[ 0, 1]], [[ 2, 3]], [[ 4, 5]], [[ 6, 7]], [[ 8, 9]], [[10, 11]], [[12, 13]], [[14, 15]], [[16, 17]], [[18, 19]], [[20, 21]], [[22, 23]]]) ''' 以上对应着pandas的Dataframe，及我们对Dataframe取values属性，会得到一个二维矩阵，做法就如同上面一样，但是如果是Series的话，取values属性得到的是一个一维的，这时候我们的做法则是， 123456789101112131415161718192021222324252627282930313233c = np.arange(12)cd = c[:,None,None]d.shaped'''array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])(12, 1, 1)array([[[ 0]], [[ 1]], [[ 2]], [[ 3]], [[ 4]], [[ 5]], [[ 6]], [[ 7]], [[ 8]], [[ 9]], [[10]], [[11]]])''' 减少维度若要减少数据的维度，我们可以用的方法如下， 1234567891011121314151617181920212223242526272829303132d = np.arange(12)[:,None,None]d.shapede = np.squeeze(d)e.shapee'''(12, 1, 1)array([[[ 0]], [[ 1]], [[ 2]], [[ 3]], [[ 4]], [[ 5]], [[ 6]], [[ 7]], [[ 8]], [[ 9]], [[10]], [[11]]])(12,)array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])'''","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"}]},{"title":"python 正则实用例子","slug":"re-basic-of-python","date":"2018-02-24T10:52:00.000Z","updated":"2018-02-24T11:09:45.871Z","comments":true,"path":"2018/02/24/re-basic-of-python/","link":"","permalink":"http://frankchen.xyz/2018/02/24/re-basic-of-python/","excerpt":"本文主要关于python的正则表达式的符号与方法。","text":"本文主要关于python的正则表达式的符号与方法。 findall: 找寻所有匹配，返回所有组合的列表 search: 找寻第一个匹配并返回 sub: 替换符合规律的内容，并返回替换后的内容 .：匹配除了换行符以外的任意字符 1234a = 'xy123'b = re.findall('x...',a)print(b)# ['xy12'] *：匹配前一个字符0次或者无限次 1234a = 'xyxy123'b = re.findall('x*',a)print(b)# ['x', '', 'x', '', '', '', '', ''] ?：匹配前一个字符0次或者1次 1234a = 'xy123'b = re.findall('x?',a)print(b)# ['x', '', '', '', '', ''] .*：贪心算法 123b = re.findall('xx.*xx',secret_code)print(b)# ['xxIxxfasdjifja134xxlovexx23345sdfxxyouxx'] .*?：非贪心算法 123c = re.findall('xx.*?xx',secret_code)print(c)# ['xxIxx', 'xxlovexx', 'xxyouxx'] ()：括号内结果返回 12345678d = re.findall('xx(.*?)xx',secret_code)print(d)for each in d: print(each)# ['I', 'love', 'you']# I# love# you re.S使得.的作用域包括换行符”\\n” 123456s = '''sdfxxhelloxxfsdfxxworldxxasdf'''d = re.findall('xx(.*?)xx',s,re.S)print(d)# ['hello\\n', 'world'] 对比findall与search的区别 1234567s2 = 'asdfxxIxx123xxlovexxdfd'f = re.search('xx(.*?)xx123xx(.*?)xx',s2).group(2)print(f)f2 = re.findall('xx(.*?)xx123xx(.*?)xx',s2)print(f2[0][1])# love# love 虽然两者结果相同，但是search是搭配group来得到第二个匹配，而findall的结果是[(‘I’, ‘love’)]，包含元组的列表，所以需要f2[0][1]来引入。 sub的使用 1234s = '123rrrrr123'output = re.sub('123(.*?)123','123%d123'%789,s)print(output)# 123789123 例如我们需要将文档中的所有的png图片改变路径，即需要找到所有的.png结尾，再将其都加上路径， 123456789101112import redef multiply(m): # Convert group 0 to an integer. v = m.group(0) print(v) # Multiply integer by 2. # ... Convert back into string and return it. print('basic/'+v) return 'basic/'+v 结果如下 123456789101112131415161718&gt;&gt;&gt; autoencoder.png basic/autoencoder.png RNN.png basic/RNN.png rnn_step_forward.png basic/rnn_step_forward.png rnns.png basic/rnns.png rnn_cell_backprop.png basic/rnn_cell_backprop.png LSTM.png basic/LSTM.png LSTM_rnn.png basic/LSTM_rnn.png attn_mechanism.png basic/attn_mechanism.png attn_model.png basic/attn_model.png 仿照上面案例，我们可以方便的对我们的任务进行定制。 subn 相比sub，subn返回元组，第二个元素表示替换发生的次数： 12345678910111213import redef add(m): # Convert. v = int(m.group(0)) # Add 2. return str(v + 1)# Call re.subn.result = re.subn(\"\\d+\", add, \"1 2 3 4 5\")print(\"Result string:\", result[0])print(\"Number of substitutions:\", result[1]) 123&gt;&gt;&gt; Result string: 11 21 31 41 51Number of substitutions: 5","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"re","slug":"re","permalink":"http://frankchen.xyz/tags/re/"}]},{"title":"Tex 写（中文）毕业论文全攻略","slug":"Writing-Graduation-Thesis-in-Tex","date":"2018-02-07T17:36:41.000Z","updated":"2018-02-07T18:11:41.628Z","comments":true,"path":"2018/02/08/Writing-Graduation-Thesis-in-Tex/","link":"","permalink":"http://frankchen.xyz/2018/02/08/Writing-Graduation-Thesis-in-Tex/","excerpt":"用 Tex 写（中文）毕业论文全攻略，高效、便捷、优雅！","text":"用 Tex 写（中文）毕业论文全攻略，高效、便捷、优雅！ 这里我们并不存在鄙视链，说什么Tex 优于 Word之类的，其实Word作为极其复杂的文本处理软件，我相信Tex能做到的，Word一定有其实现方式，只不过大部分人都只会用到Word的一小部分功能，相比Word，Tex解决方案更加便捷优雅，比如自动排号（章节、表格、参考文献的编号），全局设置的字体、间距格式等等。相比Word事无巨细的维护修改成本，Tex 的解决方案更加programmer，即软件开发，后期主要工作是迭代维护，若能在前期即考虑这点，后期能省下极多的脑细胞和精力。好了不说多，发车吧~ 总论 总体来说，是用上交Tex模板结合Atom编辑器在本地编辑Atom（这个用什么编辑器随意）以及Dropbox同步到云端Dropbox以及云端上在sharelatex服务器上即时编译所见即所得。 工具使用方法首先我们在sharelatex官网Your Projects - ShareLaTeX, Online LaTeX Editor注册账号，免费账号即可，如果需要多人协作可以用邀请小号的方式让自己增加权限（sharelatex新建账号不验证邮箱。。所以你懂的），接下来在上交模板sjtug/SJTUThesis: 上海交通大学 XeLaTeX 学位论文模板 A XeLaTeX template for Shanghai Jiao Tong University (SJTU) thesis.处点击此处添加最新版模板到我们的sharelatex项目， 如图，再点进去，先别急着修改，我们先设置个网盘同步，Dropbox需要梯子，在sharelatex的账号设置处链接到Dropbox， 同时Dropbox安装一个桌面版，需要设置代理，如图，我们使用ss作为代理。 接下来安装Atom编辑器，在插件里装一个如下插件，这里我们需要它只是为了注释这一个功能，因为我们不需要本地编译。 接下来我们就可以在本地用Atom编辑Dropbox网盘在本地的Tex项目，只要我们保存，Dropbox就会同步到sharelatex，如果开启自动编译云端就会展示当下编译的PDF效果，如图 Tex模板使用说明详见此处README.pdf，主要思路就是把各章、摘要、参考文献等分为不同的tex文件，图表等资源放在一处文件夹内，逐个引用，有全局的的设置文件，编译时将这些零件拼接为pdf，后续会添加更多心得。 资料 MathJax basic tutorial and quick reference - Mathematics Meta Stack Exchange：一个常用Latex公式符号的全集 如果上面没找到，可以试试这里，手写识别latex字符Detexify LaTeX handwritten symbol recognition","categories":[],"tags":[{"name":"Tex","slug":"Tex","permalink":"http://frankchen.xyz/tags/Tex/"}]},{"title":"理解TSNE算法","slug":"Understanding-TSNE","date":"2018-01-30T03:39:39.000Z","updated":"2018-01-30T08:17:42.455Z","comments":true,"path":"2018/01/30/Understanding-TSNE/","link":"","permalink":"http://frankchen.xyz/2018/01/30/Understanding-TSNE/","excerpt":"结合论文公式与几个python实现理解t-SNE算法。","text":"结合论文公式与几个python实现理解t-SNE算法。 t-SNE 是一种数据可视化工具，它可以将高维数据降维到2-3维以用于画图，局部相似性被这种embedding所保留。 t-SNE把原空间的数据点之间的距离转换为高斯分布概率，如果两点在高维空间距离越近，那么这个概率值越大。注意到高斯分布的这个标准差$\\sigma_i$ 对每个点都是不同的，这也是算法的创新点之一，因为理论上空间不同位置的点的密度是不同的，条件概率如此计算， $$p_{j|i} = \\frac{\\exp{(-d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) / (2 \\sigma_i^2)})}{\\sum_{i \\neq k} \\exp{(-d(\\boldsymbol{x}_i, \\boldsymbol{x}_k) / (2 \\sigma_i^2)})}, \\quad p_{i|i} = 0,$$ 图中公式是理论方式，实际是先计算条件概率再用下面公式来产生联合分布， $$p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}.$$ 其中 $\\sigma_i$ 将自动确定。这个过程可以通过设置算法的困惑性来影响。 用一个长尾分布(Student-t Distribution，简称为t分布)来表示 embed空间的相似性$$q_{ij} = \\frac{(1 + ||\\boldsymbol{y}_i - \\boldsymbol{y}_j)||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||\\boldsymbol{y}_k - \\boldsymbol{y}_l)||^2)^{-1}},$$ 损失函数是两个分布之间的 Kullback-Leibler divergence（KL散度） $$KL(P|Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$$ 而为什么说tsne保留的是局部相似性呢？我们从KL散度的公式出发来解释，可以看到，当$p_{ij}$很大而$q_{ij}$很小（高维空间距离近，低维空间距离远）惩罚很大，反之惩罚小（高维空间距离远，低维空间距离近）。 而为什么高维空间用高斯分布，低维空间用Student-t Distribution呢？ 原因就是因为降维是必然要带来信息损失，我们要保存局部信息那么必然要损失全局信息，比如我们要把上面的这个2维空间的三个成直角边的点降维到1维，那么把它们放平就保存了局部信息（左中和中右之间的距离保持不变），但是牺牲了全局信息（左右之间的距离变大了）。而Student-t Distribution就能放大这种密度，如下图（tsne默认t分布自由度为1），t分布相比高斯分布更加长尾。梯度计算时有优化技巧，如果按下图中的原公式计算，复杂度为$O(N^2)$ Barnes-Hut 树方法就可以优化到$ O(NlogN)$原理类似于用上图中ABC三点中心的距离乘以三来代替计算三者各自的距离。那么把用barnes树结构来进行深度优先搜索，分别判断其距离是否大于阈值，分块计算距离，这样复杂度就降低了。 以下是计算损失KL散度的公式， 123456789101112131415161718192021222324252627def _kl_divergence(params, P, degrees_of_freedom, n_samples, n_components, skip_num_points=0): \"\"\"t-SNE objective function: gradient of the KL divergence of p_ijs and q_ijs and the absolute error.\"\"\" X_embedded = params.reshape(n_samples, n_components) # Q is a heavy-tailed distribution: Student's t-distribution n = pdist(X_embedded, \"sqeuclidean\") n += 1. n /= degrees_of_freedom n **= (degrees_of_freedom + 1.0) / -2.0 Q = np.maximum(n / (2.0 * np.sum(n)), MACHINE_EPSILON) # Optimization trick below: np.dot(x, y) is faster than # np.sum(x * y) because it calls BLAS # Objective: C (Kullback-Leibler divergence of P and Q) kl_divergence = 2.0 * np.dot(P, np.log(P / Q)) # Gradient: dC/dY grad = np.ndarray((n_samples, n_components)) PQd = squareform((P - Q) * n) for i in range(skip_num_points, n_samples): np.dot(_ravel(PQd[i]), X_embedded[i] - X_embedded, out=grad[i]) grad = grad.ravel() c = 2.0 * (degrees_of_freedom + 1.0) / degrees_of_freedom grad *= c 用梯度下降（和一些tricks）优化，值得注意的是损失函数非对称，并且不同的训练会导致结果的不同。 sklearn里对于binary search计算 联合分布下面的(_utils._binary_search_perplexity)和Barnes-Hut 树计算梯度(_barnes_hut_tsne.gradient)都是C实现，有空再来研究。 123456789101112def _joint_probabilities(distances, desired_perplexity, verbose): \"\"\"Compute joint probabilities p_ij from distances.\"\"\" # Compute conditional probabilities such that they approximately match # the desired perplexity distances = astype(distances, np.float32, copy=False) conditional_P = _utils._binary_search_perplexity( distances, None, desired_perplexity, verbose) P = conditional_P + conditional_P.T sum_P = np.maximum(np.sum(P), MACHINE_EPSILON) P = np.maximum(squareform(P) / sum_P, MACHINE_EPSILON) return P 12345678910111213141516171819202122def _kl_divergence_bh(params, P, neighbors, degrees_of_freedom, n_samples, n_components, angle=0.5, skip_num_points=0, verbose=False): \"\"\"t-SNE objective function: KL divergence of p_ijs and q_ijs.\"\"\" params = astype(params, np.float32, copy=False) X_embedded = params.reshape(n_samples, n_components) neighbors = astype(neighbors, np.int64, copy=False) if len(P.shape) == 1: sP = squareform(P).astype(np.float32) else: sP = P.astype(np.float32) grad = np.zeros(X_embedded.shape, dtype=np.float32) error = _barnes_hut_tsne.gradient(sP, X_embedded, neighbors, grad, angle, n_components, verbose, dof=degrees_of_freedom) c = 2.0 * (degrees_of_freedom + 1.0) / degrees_of_freedom grad = grad.ravel() grad *= c return error, grad t-SNE – Laurens van der Maaten这个链接是作者收集的各种tsne变种及相关实现。","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankchen.xyz/tags/Algorithm/"}]},{"title":"Data Science Pipelines |  特征工程中的管道","slug":"Data-Science-Notes","date":"2018-01-12T06:22:50.000Z","updated":"2018-04-08T08:13:01.013Z","comments":true,"path":"2018/01/12/Data-Science-Notes/","link":"","permalink":"http://frankchen.xyz/2018/01/12/Data-Science-Notes/","excerpt":"暂定为记录各式数据科学项目、Kaggle竞赛里面常用、有用的代码片段、API、神操作等，通常是Numpy、Pandas、Matplotlib、Seaborn等相关，通常来说，项目基本步骤可以分为EDA、特征工程以及调参。","text":"暂定为记录各式数据科学项目、Kaggle竞赛里面常用、有用的代码片段、API、神操作等，通常是Numpy、Pandas、Matplotlib、Seaborn等相关，通常来说，项目基本步骤可以分为EDA、特征工程以及调参。 基本流程 以一个Kaggle上的House Price为案例，机器学习流程分成两个大步骤 ：即EDA与特征工程（只使用Pandas, StatsModel，scipy,numpy, seaborn等库） 输入： 原始Train, Test 数据集，将原始Train和Test 合并成一个数据集combined 处理： Pandas Pipe 根据各种可能和各种特征工程方法定义各种函数（输入combined, 输入pre_combined) 用PandasPipe 将这个函数像搭积木一样连在一起。用列表按序存放这些函数） 这个列表就是，1. 基本的填充空值, 2. 转换数据类型， 3. 空白函数（为了对齐美观而以，啥事不做），4. log 转换，类别数据哑元处理， 5. 导出到hdf5文件， 6.检查R2值 利用各种排列组合，或者各种参数组合，可以产生丰富的pipes，每一个pipes都可以产生一个预处理过的文件。 输出：某文件夹下 的N个预处理过的hdf5文件。 针对各种特征工程的排列组合，或者是Kaggle上面的各种新奇的特征工程方法。 机器学习阶段（训练和产生模型，目标是尽可能获得尽可能低的RMSE值（针对训练数据），同时要具有范化的能力（针对测试数据）） 第一步，建立基准，筛选出最好的一个（几个）预处理文件（随机数设成固定值） 第二步，针对筛选出来的预处理文件，进行调参。找到最合适的几个算法（通常是RMSE值最低，且不同Kernel）（随机数设成固定值） 第三步，用调好的参数来预处理文件中的Traing数据的做average 和stacking 第四部，生成csv文件，提交到Kaggle 看看得分如何。 准备阶段 与 NoteBook Head过滤warning：有句话说的好，在计算机科学里，我们只在意错误不在意warning 12import warningswarnings.filterwarnings(\"ignore\") 工作目录切换到当前python文件所在目录12import osos.chdir(os.path.dirname(os.path.abspath(__file__))) Notebook交互输出所有结果 12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity='all' 结果如下 以上可以通过设置固定下来，方法如下： 一般对train以及test做一个concat，并记录train的条数ntrain12345678910train = pd.read_csv(\"train.csv.gz\")test = pd.read_csv(\"test.csv.gz\")combined = pd.concat([train,test],axis =0, ignore_index =True)ntrain = train.shape[0]Y_train = train[\"SalePrice\"]X_train = train.drop([\"Id\",\"SalePrice\"],axis=1)print(\"train data shape:\\t \",train.shape)print(\"test data shape:\\t \",test.shape)print(\"combined data shape:\\t\",combined.shape) EDA相关1D Scatter 123456nca = NCA(num_dims=1)nca.fit(xx_t, yy)xxxxx = nca.transform(xx)zeros=np.zeros_like(xxxxx)plt.scatter(xxxxx, zeros+1,c=yy[:,np.newaxis])plt.show() 缺失值分析 1234cols_missing_value = combined.isnull().sum()/combined.shape[0]cols_missing_value = cols_missing_value[cols_missing_value&gt;0]print(\"How many features is bad/missing value? The answer is:\",cols_missing_value.shape[0])cols_missing_value.sort_values(ascending=False).head(10).plot.barh() 有缺失 - 需要填充或者删除，通常用均值或者中指，或者用人工分析（人工分析是提分关键） 将若干个Dataframe画在同一个图里面相同坐标 123456789fig, ax = plt.subplots()# desc, group 是一个Dataframe groupby desc 出的结果for desc, group in Energy_sources: group.plot(x = group.index, y='Value', label=desc,ax = ax, title='Carbon Emissions per Energy Source', fontsize = 20) ax.set_xlabel('Time(Monthly)') ax.set_ylabel('Carbon Emissions in MMT') ax.xaxis.label.set_size(20) ax.yaxis.label.set_size(20) ax.legend(fontsize = 16) 结果如下图， 画a*b的子图 12345678fig, axes = plt.subplots(3,3, figsize = (30, 20))# desc, group 是一个Dataframe groupby desc 出的结果 也就是下面的Energy_sourcesfor (desc, group), ax in zip(Energy_sources, axes.flatten()): group.plot(x = group.index, y='Value',ax = ax, title=desc, fontsize = 18) ax.set_xlabel('Time(Monthly)') ax.set_ylabel('Carbon Emissions in MMT') ax.xaxis.label.set_size(18) ax.yaxis.label.set_size(18) 画柱状图 12345678910fig = plt.figure(figsize = (16,9))# CO2_per_source的来源与结构如上图x_label = map(lambda x: x[:20],CO2_per_source.index)x_tick = np.arange(len(cols))plt.bar(x_tick, CO2_per_source, align = 'center', alpha = 0.5)fig.suptitle(\"CO2 Emissions by Electric Power Sector\", fontsize= 25)plt.xticks(x_tick, x_label, rotation = 70, fontsize = 15)plt.yticks(fontsize = 20)plt.xlabel('Carbon Emissions in MMT', fontsize = 20)plt.show() 重叠图 1234567891011121314151617181920from statsmodels.tsa.seasonal import seasonal_decomposedecomposition = seasonal_decompose(mte)trend = decomposition.trendseasonal = decomposition.seasonalresidual = decomposition.residplt.subplot(411)plt.plot(mte, label='Original')plt.legend(loc='best')plt.subplot(412)plt.plot(trend, label='Trend')plt.legend(loc='best')plt.subplot(413)plt.plot(seasonal,label='Seasonality')plt.legend(loc='best')plt.subplot(414)plt.plot(residual, label='Residuals')plt.legend(loc='best')plt.tight_layout() 环形图 123456789101112plt.subplots(figsize=(15,15))data=response['PublicDatasetsSelect'].str.split(',')dataset=[]for i in data.dropna(): dataset.extend(i)pd.Series(dataset).value_counts().plot.pie(autopct='%1.1f%%',colors=sns.color_palette('Paired',10),startangle=90,wedgeprops = &#123; 'linewidth' : 2, 'edgecolor' : 'white' &#125;)plt.title('Dataset Source')my_circle=plt.Circle( (0,0), 0.7, color='white')p=plt.gcf()p.gca().add_artist(my_circle)plt.ylabel('')plt.show() 饼状图 12345678f,ax=plt.subplots(1,2,figsize=(18,8))response['JobSkillImportancePython'].value_counts().plot.pie(ax=ax[0],autopct='%1.1f%%',explode=[0.1,0,0],shadow=True,colors=['g','lightblue','r'])ax[0].set_title('Python Necessity')ax[0].set_ylabel('')response['JobSkillImportanceR'].value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',explode=[0,0.1,0],shadow=True,colors=['lightblue','g','r'])ax[1].set_title('R Necessity')ax[1].set_ylabel('')plt.show() 维恩图 123456f,ax=plt.subplots(1,2,figsize=(18,8))pd.Series([python.shape[0],R.shape[0],both.shape[0]],index=['Python','R','Both']).plot.bar(ax=ax[0])ax[0].set_title('Number of Users')venn2(subsets = (python.shape[0],R.shape[0],both.shape[0]), set_labels = ('Python Users', 'R Users'))plt.title('Venn Diagram for Users')plt.show() Seaborncount plot 123plt.subplots(figsize=(22,12))sns.countplot(y=response['GenderSelect'],order=response['GenderSelect'].value_counts().index)plt.show() 利用squarify画树形图 1234567import squarifytree=response['Country'].value_counts().to_frame()squarify.plot(sizes=tree['Country'].values,label=tree.index,color=sns.color_palette('RdYlGn_r',52))plt.rcParams.update(&#123;'font.size':20&#125;)fig=plt.gcf()fig.set_size_inches(40,15)plt.show() sns画分布图 12345plt.subplots(figsize=(15,8))salary=salary[salary['Salary']&lt;1000000]sns.distplot(salary['Salary'])plt.title('Salary Distribution',size=15)plt.show() sns子图 123456789101112131415f,ax=plt.subplots(1,2,figsize=(18,8))sal_coun=salary.groupby('Country')['Salary'].median().sort_values(ascending=False)[:15].to_frame()sns.barplot('Salary',sal_coun.index,data=sal_coun,palette='RdYlGn',ax=ax[0])ax[0].axvline(salary['Salary'].median(),linestyle='dashed')ax[0].set_title('Highest Salary Paying Countries')ax[0].set_xlabel('')max_coun=salary.groupby('Country')['Salary'].median().to_frame()max_coun=max_coun[max_coun.index.isin(resp_coun.index)]max_coun.sort_values(by='Salary',ascending=True).plot.barh(width=0.8,ax=ax[1],color=sns.color_palette('RdYlGn'))ax[1].axvline(salary['Salary'].median(),linestyle='dashed')ax[1].set_title('Compensation of Top 15 Respondent Countries')ax[1].set_xlabel('')ax[1].set_ylabel('')plt.subplots_adjust(wspace=0.8)plt.show() seaborn箱型图 1234plt.subplots(figsize=(10,8))sns.boxplot(y='GenderSelect',x='Salary',data=salary)plt.ylabel('')plt.show() seaborn count_plot 123456789f,ax=plt.subplots(1,2,figsize=(25,15))sns.countplot(y=response['MajorSelect'],ax=ax[0],order=response['MajorSelect'].value_counts().index)ax[0].set_title('Major')ax[0].set_ylabel('')sns.countplot(y=response['CurrentJobTitleSelect'],ax=ax[1],order=response['CurrentJobTitleSelect'].value_counts().index)ax[1].set_title('Current Job')ax[1].set_ylabel('')plt.subplots_adjust(wspace=0.8)plt.show() seaborn 图中添加文字 12345678sal_job=salary.groupby('CurrentJobTitleSelect')['Salary'].median().to_frame().sort_values(by='Salary',ascending=False)ax=sns.barplot(sal_job.Salary,sal_job.index,palette=sns.color_palette('inferno',20))plt.title('Compensation By Job Title',size=15)for i, v in enumerate(sal_job.Salary): ax.text(.5, i, v,fontsize=10,color='white',weight='bold')fig=plt.gcf()fig.set_size_inches(8,8)plt.show() 词云 12345678910111213141516171819202122232425from wordcloud import WordCloud, STOPWORDSimport nltkfrom nltk.corpus import stopwordsfree=pd.read_csv('../input/freeformResponses.csv')stop_words=set(stopwords.words('english'))stop_words.update(',',';','!','?','.','(',')','$','#','+',':','...')motivation=free['KaggleMotivationFreeForm'].dropna().apply(nltk.word_tokenize)motivate=[]for i in motivation: motivate.extend(i)motivate=pd.Series(motivate)motivate=([i for i in motivate.str.lower() if i not in stop_words])f1=open(\"kaggle.png\", \"wb\")f1.write(codecs.decode(kaggle,'base64'))f1.close()img1 = imread(\"kaggle.png\")hcmask1 = img1wc = WordCloud(background_color=\"black\", max_words=4000, mask=hcmask1, stopwords=STOPWORDS, max_font_size= 60,width=1000,height=1000)wc.generate(\" \".join(motivate))plt.imshow(wc)plt.axis('off')fig=plt.gcf()fig.set_size_inches(10,10)plt.show() 简单情况下的分类展示 123456789101112131415161718192021222324from IPython import displayh = 0.01x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))def visualize(X, y, w, history): \"\"\"draws classifier prediction with matplotlib magic\"\"\" Z = probability(expand(np.c_[xx.ravel(), yy.ravel()]), w) Z = Z.reshape(xx.shape) plt.subplot(1, 2, 1) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.subplot(1, 2, 2) plt.plot(history) plt.grid() ymin, ymax = plt.ylim() plt.ylim(0, ymax) display.clear_output(wait=True) plt.show() 图中插入LaTeX公式 12345678910import matplotlib.pyplot as plt%matplotlib inlinex = np.linspace(-3, 3)x_squared, x_squared_der = s.run([scalar_squared, derivative[0]], &#123;my_scalar:x&#125;)plt.plot(x, x_squared,label=\"$x^2$\")plt.plot(x, x_squared_der, label=r\"$\\frac&#123;dx^2&#125;&#123;dx&#125;$\")plt.legend(); 画多张子图 12345678910111213# show random images from traincols = 8rows = 2fig = plt.figure(figsize=(2 * cols - 1, 2.5 * rows - 1))for i in range(cols): for j in range(rows): random_index = np.random.randint(0, len(y_train)) ax = fig.add_subplot(rows, cols, i * rows + j + 1) ax.grid('off') ax.axis('off') ax.imshow(x_train[random_index, :]) ax.set_title(cifar10_classes[y_train[random_index, 0]])plt.show() 特征工程阶段Numpy区间百分比切分异常值 123# cut off long distance tripslat_low, lat_hgh = np.percentile(latlong[:,0], [2, 98])lon_low, lon_hgh = np.percentile(latlong[:,1], [2, 98]) 初始化同shape向量123456789101112131415161718192021222324252627g2 = np.zeros_like(w)``-------累积sum``` pythona = np.array([[1,2,3], [4,5,6]])&gt;&gt;&gt; np.cumsum(a,axis=1) # sum over columns for each of the 2 rowsarray([[ 1, 3, 6], [ 4, 9, 15]])``` -------numpy array 扩展维度，很简单地将Numpy向量扩展为二维矩阵![](/images/15167894435840.png)![Screen Shot 2018-01-24 at 19.02.32](/images/Screen%20Shot%202018-01-24%20at%2019.02.32.png)-------Numpy 竖着叠放向量`np.column_stack`-------``` python# 用于查看Dataframe各列数据类型ts.dtypes 12345678#skew是单变量工具，用来监测数据是否有长尾，左偏或者右偏print(Y_train.skew())``` ``` python#np.abs 是绝对值函数，用来取整个向量绝对值# 这里对所有train里的特征求偏度并排序np.abs(combined[:ntrain].skew()).sort_values(ascending = False ).head(20) 有偏度 - 需要处理。通常是用log1p 12# 用于将Dataframe中被读取为object的数据转换为数值型，errors='coerce'代表错误将被置为NaNts['Value'] = pd.to_numeric(ts['Value'] , errors='coerce') 过滤index 里面的NaN值，推广也可以过滤其他列 1ts = df.loc[pd.Series(pd.to_datetime(df.index, errors='coerce')).notnull().values] 按月groupby，以及unstack解构 1Emissions.groupby(['Description', pd.TimeGrouper('M')])['Value'].sum().unstack(level = 0) 将value_counts、groupby等Series转换为Dataframe1tree=response['Country'].value_counts().to_frame() 特征工程大杀器，Pandas Pipe这里有个简单的例子，,每个pipes里面都有若干个特征处理函数和一个快速测试的函数，其中为了对齐美观，用bypass函数来填充空白的地方（无用但是为了强行让pipes长度相同） 1234567891011121314151617181920pipe_basic = [pipe_basic_fillna,pipe_bypass,\\ pipe_bypass,pipe_bypass,\\ pipe_bypass,pipe_bypass,\\ pipe_log_getdummies,pipe_bypass, \\ pipe_export,pipe_r2test]pipe_ascat = [pipe_fillna_ascat,pipe_drop_cols,\\ pipe_drop4cols,pipe_outliersdrop,\\ pipe_extract,pipe_bypass,\\ pipe_log_getdummies,pipe_drop_dummycols, \\ pipe_export,pipe_r2test]pipe_ascat_unitprice = [pipe_fillna_ascat,pipe_drop_cols,\\ pipe_drop4cols,pipe_outliersdrop,\\ pipe_extract,pipe_unitprice,\\ pipe_log_getdummies,pipe_drop_dummycols, \\ pipe_export,pipe_r2test]pipes = [pipe_basic,pipe_ascat,pipe_ascat_unitprice ] 跑的代码为 1234567891011121314151617for i in range(len(pipes)): print(\"*\"*10,\"\\n\") pipe_output=pipes[i] output_name =\"_\".join([x.__name__[5:] for x in pipe_output if x.__name__ is not \"pipe_bypass\"]) output_name = \"PIPE_\" +output_name print(output_name) (combined.pipe(pipe_output[0]) .pipe(pipe_output[1]) .pipe(pipe_output[2]) .pipe(pipe_output[3]) .pipe(pipe_output[4]) .pipe(pipe_output[5]) .pipe(pipe_output[6]) .pipe(pipe_output[7]) .pipe(pipe_output[8],name=output_name) .pipe(pipe_output[9]) ） 在这一步，我们可以初步看到三个特征工程的性能。并且文件已经输出到hd5格式文件。后期在训练和预测时，直接取出预处理的文件就可以。各个pipe代码可见此处。 调参阶段在数据准备好后训练时，最基本的就是要调整超参（Hyperparameter）耗时耗力，并且和发生错误和遗漏情况。Stackoverflow上常见的算法训练错误有： 算法预测的结果差异非常大。 其中一个可能就是训练时的标准化步骤，在预测时遗漏了。 算法的调参结果差异非常大。（有的是0.01,有的就是10）。其中的一个可能就是不同的训练步骤中采用的标准化算法不同（例如,一次用了StandardScaler, 另一次用了RobustScaler) 此外，繁多的超参数调整起来异常繁琐。比较容易错误或者写错。 解决方法：Pipeline + Gridsearch + 参数字典 + 容器。使用Pipeline的例子 针对线形回归问题，Sklearn提供了超过15种回归算法。利用Pipeline 大法可以综合测试所有算法，找到最合适的算法。 具体步骤如下： 初始化所有希望调测线形回归。 建立一个字典容器。{“算法名称”:[初始算法对象，参数字典，训练好的Pipeline模型对象，CV的成绩} 在调参步骤，将初始算法用Pipeline包装起来，利用Gridsearch进行调参。调参完成后可以得到针对相应的CV而获得的最后模型对象。 例如： lasso 算法的步骤如下： 包装 pipe=Pipeline([(“scaler”:None),(“selector”:None),(“clf”:Lasso()) Pipe就是刚刚包装好的算法。可以直接用于 训练(fit)和预测(predict) 使用Pipe来处理训练集和测试集可以避免错误和遗漏，提高效率。 但是Pipe中算法是默认的参数，直接训练出的模型RMSE不太理想。（例如：local CV, 0.12~0.14左右）。这时可以考虑调参。 调参第一步：准备参数字典： Params_lasso ={ “Scaler”:[RobustScaler(),StandardScaler()], #两个标准化算法供调模型 “selectorthreshold”:np.logspace(-5,-4,3), #3个选择门限供选特征 “clfalpha”:np.logspace(-5,-1,10) }， #10个alpha指供调参 调参第二步：暴力调参和生成模型 rsearch = GridSearchCV(pipe, param_grid=Params_lasso,scoring =’neg_mean_squared_error’,verbose=verbose,cv=10,refit =True) GridSearch 是暴力调参。遍历所有参数组合，另外有一个RandomedSearch 可以随机选择参数组合，缩短调参时间，并且获得近似的调参性能 Pipe就是刚刚包装好的算法。GridSearch把可选的参数和算法（放入，或者更好的组合。 调参的训练标准是“’neg_mean_squared_error”, RMSE的负数。 这种处理方法，让最大值称为最小的MSE指。只需要对结果做一次np.sqrt( 结果负数）就能获得RMSE值。 cv=10. Cross Validate 数据集为9：1。数据集小的情况，例如House Price. 3折和10折结果甚至比调参差异还大。 refit =True. 在调参完成后，再需要做一次所有数据集的fit. 生成完整的训练模型 Sklearn 流程图","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://frankchen.xyz/tags/Data-Science/"},{"name":"Python","slug":"Python","permalink":"http://frankchen.xyz/tags/Python/"}]},{"title":"DIY远程Jupyter Notebook服务器","slug":"Remote-jupyter-notebook","date":"2017-12-25T11:43:17.000Z","updated":"2017-12-25T11:48:42.351Z","comments":true,"path":"2017/12/25/Remote-jupyter-notebook/","link":"","permalink":"http://frankchen.xyz/2017/12/25/Remote-jupyter-notebook/","excerpt":"构建自己的远程Jupyter Notebook服务器，添加system开机自启，让Jupyter Notebook支持跨网络访问的方法。","text":"构建自己的远程Jupyter Notebook服务器，添加system开机自启，让Jupyter Notebook支持跨网络访问的方法。 完全开放，不需密码1. 登陆远程服务器2.生成配置文件$jupyter notebook --generate-config 3. 修改默认配置文件$vim ~/.jupyter/jupyter_notebook_config.py进行如下修改： 12c.NotebookApp.ip = '0.0.0.0' #支持其它IP访问，关键c.NotebookApp.port = 10000 #随便指定一个端口 4. 启动jupyter notebook：jupyter notebook 5. 远程访问此时应该可以直接从本地浏览器直接访问http://address_of_remote:10000就可以看到jupyter的登陆界面，输入密码即可。 需要密码1. 生成密码打开ipython，创建一个密文的密码： 12345In [1]: from notebook.auth import passwdIn [2]: passwd()Enter password: Verify password: Out[2]: 'sha1:ce23d945972f:34769685a7ccd3d08c84a18c63968a41f1140274' 2. 添加密码$vim ~/.jupyter/jupyter_notebook_config.py进行如下修改： 1c.NotebookApp.password = u'sha:ce...刚才复制的那个密文' 3. 建立ssh通道若还是无法登录，也可用 ssh username@address_of_remote -L 127.0.0.1:10000:127.0.0.1:10000 建立ssh通道，便可以在localhost:10000直接访问远程的jupyter了。 添加system开机自启将 Jupyter Notebook 设定为系统服务并且开机自动启动，这里以 systemd 下的设定为例，创建文件 sudo vim /etc/systemd/system/jupyter.service文件，内容是 123456789101112131415[Unit]Description=Jupyter NotebookAfter=network.target[Service]Type=simpleExecStart=/home/frank/anaconda3/bin/jupyter-notebook --config=/home/frank/.jupyter/jupyter_notebook_config.py --no-browserUser=frankGroup=frankWorkingDirectory=/home/frank/Restart=alwaysRestartSec=10[Install]WantedBy=multi-user.target 上面你需要把我的用户名frank替换掉，保存文件之后执行systemctl enable jupyter再执行systemctl start jupyter即可，需要输入几次密码，之后重启Notebook会自启。 内网穿透结合下文的方法，用ftp即可做到 frp的内网穿透及外网访问内网jupyter-notebook的实现 | 不正经数据科学家 参考自 Jupyter (IPython notebook)用于服务器的配置方法(Windows) - 知乎专栏 远程访问jupyter notebook","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"},{"name":"Jupyter Notebook","slug":"Jupyter-Notebook","permalink":"http://frankchen.xyz/tags/Jupyter-Notebook/"}]},{"title":"深度学习中Keras中的Embedding层的理解与使用","slug":"How-to-Use-Word-Embedding-Layers-for-Deep-Learning-with-Keras","date":"2017-12-18T07:59:41.000Z","updated":"2018-03-08T08:44:07.373Z","comments":true,"path":"2017/12/18/How-to-Use-Word-Embedding-Layers-for-Deep-Learning-with-Keras/","link":"","permalink":"http://frankchen.xyz/2017/12/18/How-to-Use-Word-Embedding-Layers-for-Deep-Learning-with-Keras/","excerpt":"单词嵌入提供了单词的密集表示及其相对含义，它们是对简单包模型表示中使用的稀疏表示的改进，可以从文本数据中学习字嵌入，并在项目之间重复使用。它们也可以作为拟合文本数据的神经网络的一部分来学习。","text":"单词嵌入提供了单词的密集表示及其相对含义，它们是对简单包模型表示中使用的稀疏表示的改进，可以从文本数据中学习字嵌入，并在项目之间重复使用。它们也可以作为拟合文本数据的神经网络的一部分来学习。 Word Embedding单词嵌入是使用密集的矢量表示来表示单词和文档的一类方法。 词嵌入是对传统的词袋模型编码方案的改进，传统方法使用大而稀疏的矢量来表示每个单词或者在矢量内对每个单词进行评分以表示整个词汇表，这些表示是稀疏的，因为每个词汇的表示是巨大的，给定的词或文档主要由零值组成的大向量表示。 相反，在嵌入中，单词由密集向量表示，其中向量表示将单词投影到连续向量空间中。 向量空间中的单词的位置是从文本中学习的，并且基于在使用单词时围绕单词的单词。 学习到的向量空间中的单词的位置被称为它的嵌入：Embedding。 从文本学习单词嵌入方法的两个流行例子包括： Word2Vec. GloVe. 除了这些精心设计的方法之外，还可以将词嵌入学习作为深度学习模型的一部分。这可能是一个较慢的方法，但可以通过这样为特定数据集定制模型。 Keras Embedding LayerKeras提供了一个嵌入层，适用于文本数据的神经网络。 它要求输入数据是整数编码的，所以每个字都用一个唯一的整数表示。这个数据准备步骤可以使用Keras提供的Tokenizer API来执行。 嵌入层用随机权重进行初始化，并将学习训练数据集中所有单词的嵌入。 它是一个灵活的图层，可以以多种方式使用，例如： 它可以单独使用来学习一个单词嵌入，以后可以保存并在另一个模型中使用。 它可以用作深度学习模型的一部分，其中嵌入与模型本身一起学习。 它可以用来加载预先训练的词嵌入模型，这是一种迁移学习。 嵌入层被定义为网络的第一个隐藏层。它必须指定3个参数： input_dim：这是文本数据中词汇的取值可能数。例如，如果您的数据是整数编码为0-9之间的值，那么词汇的大小就是10个单词； output_dim：这是嵌入单词的向量空间的大小。它为每个单词定义了这个层的输出向量的大小。例如，它可能是32或100甚至更大，可以视为具体问题的超参数； input_length：这是输入序列的长度，就像您为Keras模型的任何输入层所定义的一样，也就是一次输入带有的词汇个数。例如，如果您的所有输入文档都由1000个字组成，那么input_length就是1000。 例如，下面我们定义一个词汇表为200的嵌入层（例如从0到199的整数编码的字，包括0到199），一个32维的向量空间，其中将嵌入单词，以及输入文档，每个单词有50个单词。 e = Embedding(input_dim=200, output_dim=32, input_length=50) 嵌入层自带学习的权重，如果将模型保存到文件中，则将包含嵌入图层的权重。 嵌入层的输出是一个二维向量，每个单词在输入文本（输入文档）序列中嵌入一个。 如果您希望直接将Dense层接到Embedding层后面，则必须先使用Flatten层将Embedding层的2D输出矩阵平铺为一维矢量。 现在，让我们看看我们如何在实践中使用嵌入层。 学习 Embedding的例子在本节中，我们将看看如何在文本分类问题上拟合神经网络的同时学习单词嵌入。 我们将定义一个小问题，我们有10个文本文档，每个文档都有一个学生提交的工作评论。每个文本文档被分类为正的“1”或负的“0”。这是一个简单的情感分析问题。 首先，我们将定义文档及其类别标签。 12345678910111213# define documents 定义文档docs = ['Well done!', 'Good work', 'Great effort', 'nice work', 'Excellent!', 'Weak', 'Poor effort!', 'not good', 'poor work', 'Could have done better.']# define class labels 定义分类标签labels = [1,1,1,1,1,0,0,0,0,0] 接下来，我们来整数编码每个文件。这意味着把输入，嵌入层将具有整数序列。我们可以尝试其他更复杂的bag of word 模型比如计数或TF-IDF。 Keras提供one_hot()函数来创建每个单词的散列作为一个有效的整数编码。我们用估计50的词汇表大小，这大大减少了hash函数的冲突概率。 1234# integer encode the documents 独热编码vocab_size = 50encoded_docs = [one_hot(d, vocab_size) for d in docs]print(encoded_docs) 1[[6, 16], [42, 24], [2, 17], [42, 24], [18], [17], [22, 17], [27, 42], [22, 24], [49, 46, 16, 34]] 这样以后序列具有不同的长度，但是Keras更喜欢输入矢量化和所有输入具有相同的长度。我们将填充所有输入序列的长度为4，同样，我们可以使用内置的Keras函数（在这种情况下为pad_sequences()函数）执行此操作, 1234# pad documents to a max length of 4 words 将不足长度的用0填充为长度4max_length = 4padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')print(padded_docs) 12345678910[[ 6 16 0 0] [42 24 0 0] [ 2 17 0 0] [42 24 0 0] [18 0 0 0] [17 0 0 0] [22 17 0 0] [27 42 0 0] [22 24 0 0] [49 46 16 34]] 我们现在准备将我们的嵌入层定义为我们的神经网络模型的一部分。 嵌入的词汇量为50，输入长度为4，我们将选择一个8维的嵌入空间。 该模型是一个简单的二元分类模型。重要的是，嵌入层的输出将是每个8维的4个矢量，每个单词一个。我们将其平铺到一个32个元素的向量上以传递到密集输出层。 123456789# define the model 定义模型model = Sequential()model.add(Embedding(vocab_size, 8, input_length=max_length))model.add(Flatten())model.add(Dense(1, activation='sigmoid'))# compile the model 编译model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])# summarize the model 打印模型信息print(model.summary()) 12345678910111213_________________________________________________________________Layer (type) Output Shape Param #=================================================================embedding_1 (Embedding) (None, 4, 8) 400_________________________________________________________________flatten_1 (Flatten) (None, 32) 0_________________________________________________________________dense_1 (Dense) (None, 1) 33=================================================================Total params: 433Trainable params: 433Non-trainable params: 0_________________________________________________________________ 最后，我们可以拟合和评估分类模型。 12345# fit the model 拟合model.fit(padded_docs, labels, epochs=50, verbose=0)# evaluate the model 评估loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)print('Accuracy: %f' % (accuracy*100)) 1Accuracy: 100.000000 下面是完整的代码，这里我们用函数式API改写了模型定义，不过结构和上面是完全一样的。 123456789101112131415161718192021222324252627282930313233343536373839404142from keras.layers import Dense, Flatten, Inputfrom keras.layers.embeddings import Embeddingfrom keras.models import Modelfrom keras.preprocessing.sequence import pad_sequencesfrom keras.preprocessing.text import one_hot# define documentsdocs = ['Well done!', 'Good work', 'Great effort', 'nice work', 'Excellent!', 'Weak', 'Poor effort!', 'not good', 'poor work', 'Could have done better.']# define class labelslabels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]# integer encode the documentsvocab_size = 50encoded_docs = [one_hot(d, vocab_size) for d in docs]print(encoded_docs)# pad documents to a max length of 4 wordsmax_length = 4padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')print(padded_docs)# define the modelinput = Input(shape=(4, ))x = Embedding(vocab_size, 8, input_length=max_length)(input)x = Flatten()(x)x = Dense(1, activation='sigmoid')(x)model = Model(inputs=input, outputs=x)# compile the modelmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])# summarize the modelprint(model.summary())# fit the modelmodel.fit(padded_docs, labels, epochs=50, verbose=0)# evaluate the modelloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)print('Accuracy: %f' % (accuracy * 100)) 之后，我们可以将嵌入图层中学习的权重保存到文件中，以便以后在其他模型中使用。 通常也可以使用这个模型来分类在测试数据集中看到的同类词汇的其他文档。 接下来，让我们看看在Keras中加载预先训练的词嵌入。 使用预训练GloVE嵌入的示例Keras嵌入层也可以使用在其他地方学习的嵌入字。 在自然语言处理领域，学习，保存和分享提供词嵌入是很常见的。 例如，GloVe方法背后的研究人员提供了一套在公共领域许可下发布的预先训练的词嵌入。看到： GloVe: Global Vectors for Word Representation 最小的包是822Mb，叫做“glove.6B.zip”。它训练了10亿个词汇（单词）的数据集，词汇量为40万字，有几种不同的嵌入矢量尺寸，包括50,100,200和300size。 您可以下载这个嵌入的集合，可以作为Keras嵌入层中训练数据集中的单词预先训练嵌入的权重。 这个例子受Keras项目中的一个例子的启发：pretrained_word_embeddings.py。 下载并解压缩后，您将看到几个文件，其中一个是“glove.6B.100d.txt”，其中包含一个100维版本的嵌入。 如果你在文件内部偷看，你会看到一个token（单词），后面是每行的权重（100个数字）。例如，下面是嵌入的ASCII文本文件的第一行，显示“the”的嵌入。 1the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062 如前一节所述，第一步是定义这些示例，将它们编码为整数，然后将这些序列填充为相同的长度。 在这种情况下，我们需要能够将单词映射到整数以及整数到单词。 Keras提供了一个Tokenizer类，可以适应训练数据，通过调用Tokenizer类的texts_to_sequences（）方法，可以一致地将文本转换为序列，并且可以访问单词在word_index属性中的整数字典映射。 123456789101112131415161718192021222324# define documentsdocs = ['Well done!', 'Good work', 'Great effort', 'nice work', 'Excellent!', 'Weak', 'Poor effort!', 'not good', 'poor work', 'Could have done better.']# define class labelslabels = [1,1,1,1,1,0,0,0,0,0]# prepare tokenizert = Tokenizer()t.fit_on_texts(docs)vocab_size = len(t.word_index) + 1# integer encode the documentsencoded_docs = t.texts_to_sequences(docs)print(encoded_docs)# pad documents to a max length of 4 wordsmax_length = 4padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')print(padded_docs) 接下来，我们需要将整个Glove字嵌入文件作为字的字典加载到内存中以嵌入数组。 12345678910# load the whole embedding into memoryembeddings_index = dict()f = open('glove.6B.100d.txt')for line in f: values = line.split() word = values[0] coefs = asarray(values[1:], dtype='float32') embeddings_index[word] = coefsf.close()print('Loaded %s word vectors.' % len(embeddings_index)) 这很慢。在训练数据中过滤特殊字词的嵌入可能会更好。 接下来，我们需要为训练数据集中的每个单词创建一个嵌入矩阵。我们可以通过枚举Tokenizer.word_index中的所有唯一单词并从加载的GloVe嵌入中找到嵌入权重向量来实现这一点。 结果是一个仅用于训练期间将会看到的单词的权重矩阵。 123456# create a weight matrix for words in training docsembedding_matrix = zeros((vocab_size, 100))for word, i in t.word_index.items(): embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[i] = embedding_vector 现在我们可以像以前一样定义我们的模型，并进行评估。 关键的区别是嵌入层可以用GloVe字嵌入权重来播种。我们选择了100维版本，因此必须使用output_dim将其设置为100来定义嵌入层。最后，我们不希望更新此模型中的学习单词权重，因此我们将设置模型的可训练属性为False 。 1e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False) 下面列出了完整的工作示例。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from numpy import asarrayfrom numpy import zerosfrom keras.preprocessing.text import Tokenizerfrom keras.preprocessing.sequence import pad_sequencesfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.layers import Flattenfrom keras.layers import Embedding# define documentsdocs = ['Well done!', 'Good work', 'Great effort', 'nice work', 'Excellent!', 'Weak', 'Poor effort!', 'not good', 'poor work', 'Could have done better.']# define class labelslabels = [1,1,1,1,1,0,0,0,0,0]# prepare tokenizert = Tokenizer()t.fit_on_texts(docs)vocab_size = len(t.word_index) + 1# integer encode the documentsencoded_docs = t.texts_to_sequences(docs)print(encoded_docs)# pad documents to a max length of 4 wordsmax_length = 4padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')print(padded_docs)# load the whole embedding into memoryembeddings_index = dict()f = open('../glove_data/glove.6B/glove.6B.100d.txt')for line in f: values = line.split() word = values[0] coefs = asarray(values[1:], dtype='float32') embeddings_index[word] = coefsf.close()print('Loaded %s word vectors.' % len(embeddings_index))# create a weight matrix for words in training docsembedding_matrix = zeros((vocab_size, 100))for word, i in t.word_index.items(): embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[i] = embedding_vector# define modelmodel = Sequential()e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)model.add(e)model.add(Flatten())model.add(Dense(1, activation='sigmoid'))# compile the modelmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])# summarize the modelprint(model.summary())# fit the modelmodel.fit(padded_docs, labels, epochs=50, verbose=0)# evaluate the modelloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)print('Accuracy: %f' % (accuracy*100)) 运行这个例子可能需要更长的时间，但是这表明它能够适应这个简单的问题。 12345678910111213141516171819202122232425262728293031[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]][[ 6 2 0 0] [ 3 1 0 0] [ 7 4 0 0] [ 8 1 0 0] [ 9 0 0 0] [10 0 0 0] [ 5 4 0 0] [11 3 0 0] [ 5 1 0 0] [12 13 2 14]]Loaded 400000 word vectors._________________________________________________________________Layer (type) Output Shape Param #=================================================================embedding_1 (Embedding) (None, 4, 100) 1500_________________________________________________________________flatten_1 (Flatten) (None, 400) 0_________________________________________________________________dense_1 (Dense) (None, 1) 401=================================================================Total params: 1,901Trainable params: 401Non-trainable params: 1,500_________________________________________________________________Accuracy: 100.000000 在实践中，最好还是尝试使用预先训练好的嵌入来学习单词嵌入，因为它是固定的，并尝试在预先训练好的嵌入之上进行学习，这就类似于计算机视觉里面用预训练的VGG或者res-net迁移具体问题那样。 不过这取决于什么最适合你的具体问题。 IMDB 数据集Embedding实例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152from keras.models import Sequential,Modelfrom keras.layers import Flatten, Dense, Embedding, Inputinput_layer = Input(shape=(maxlen,)) x = Embedding(input_dim=10000,output_dim=8)(input_layer)# 单独做一个embedding模型，利于后面观察embedding = Model(input_layer,x)x = Flatten()(x)x = Dense(1,activation='sigmoid')(x)model = Model(input_layer,x)model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])model.summary()history = modhistory = modhistory = mod&gt; history = model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_4 (InputLayer) (None, 20) 0 _________________________________________________________________embedding_5 (Embedding) (None, 20, 8) 80000 _________________________________________________________________flatten_5 (Flatten) (None, 160) 0 _________________________________________________________________dense_5 (Dense) (None, 1) 161 =================================================================Total params: 80,161Trainable params: 80,161Non-trainable params: 0_________________________________________________________________Train on 20000 samples, validate on 5000 samplesEpoch 1/1020000/20000 [==============================] - 2s 105us/step - loss: 0.6772 - acc: 0.6006 - val_loss: 0.6448 - val_acc: 0.6704Epoch 2/1020000/20000 [==============================] - 2s 93us/step - loss: 0.5830 - acc: 0.7188 - val_loss: 0.5629 - val_acc: 0.7046Epoch 3/1020000/20000 [==============================] - 2s 95us/step - loss: 0.5152 - acc: 0.7464 - val_loss: 0.5362 - val_acc: 0.7208Epoch 4/1020000/20000 [==============================] - 2s 93us/step - loss: 0.4879 - acc: 0.7607 - val_loss: 0.5299 - val_acc: 0.7292Epoch 5/1020000/20000 [==============================] - 2s 97us/step - loss: 0.4731 - acc: 0.7694 - val_loss: 0.5290 - val_acc: 0.7334Epoch 6/1020000/20000 [==============================] - 2s 98us/step - loss: 0.4633 - acc: 0.7773 - val_loss: 0.5317 - val_acc: 0.7344Epoch 7/1020000/20000 [==============================] - 2s 96us/step - loss: 0.4548 - acc: 0.7819 - val_loss: 0.5333 - val_acc: 0.7318Epoch 8/1020000/20000 [==============================] - 2s 93us/step - loss: 0.4471 - acc: 0.7870 - val_loss: 0.5377 - val_acc: 0.7288Epoch 9/1020000/20000 [==============================] - 2s 95us/step - loss: 0.4399 - acc: 0.7924 - val_loss: 0.5422 - val_acc: 0.7278Epoch 10/1020000/20000 [==============================] - 2s 90us/step - loss: 0.4328 - acc: 0.7957 - val_loss: 0.5458 - val_acc: 0.7290 我们观察一下input的shape 1234567891011x_train[1].shapex_train[1]x_train[:1].shapex_train[:1](20,)array([ 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95], dtype=int32)(1, 20)array([[ 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]], dtype=int32) 再看看embedding的输出， 1234567891011121314151617181920212223242526272829303132333435363738394041424344embedding.predict(x_train[:1]).shapeembedding.predict(x_train[:1])(1, 20, 8)array([[[-0.17401133, -0.08743777, 0.15631911, -0.06831486, -0.09105065, 0.06253908, -0.0798945 , 0.07671431], [ 0.18718374, 0.10347525, -0.06668846, 0.25818944, 0.07522523, 0.07082067, 0.05170904, 0.22902426], [ 0.06872956, -0.00586612, 0.07713806, -0.00182899, 0.00882899, -0.18892162, -0.13580748, -0.03166043], [-0.01912907, -0.01732869, 0.00391375, -0.02338142, 0.02787969, -0.02744135, 0.0074541 , 0.01806928], [ 0.20604047, 0.10910885, 0.06304865, -0.14038748, 0.12123005, 0.06124007, 0.0532628 , 0.17591232], [-0.19636872, -0.0027669 , 0.01087157, -0.02332311, -0.04321857, -0.09228673, -0.03061322, -0.13376454], [ 0.18718374, 0.10347525, -0.06668846, 0.25818944, 0.07522523, 0.07082067, 0.05170904, 0.22902426], [-0.27160701, -0.29296583, 0.1055108 , 0.15896739, -0.24833643, -0.17791845, -0.27316946, -0.241273 ], [-0.02175452, -0.0839383 , 0.04338101, 0.01062139, -0.11473208, -0.18394938, -0.05141308, -0.10405254], [ 0.18718374, 0.10347525, -0.06668846, 0.25818944, 0.07522523, 0.07082067, 0.05170904, 0.22902426], [-0.01912907, -0.01732869, 0.00391375, -0.02338142, 0.02787969, -0.02744135, 0.0074541 , 0.01806928], [-0.14751843, 0.05572686, 0.20332271, -0.01759946, -0.0946402 , -0.14416233, 0.16961734, 0.01381243], [ 0.00282665, -0.17532936, -0.09342033, 0.04514923, -0.04684081, 0.1748796 , -0.09669576, -0.10699435], [ 0.00225757, -0.12751001, -0.12703758, 0.17167819, -0.03712473, 0.04252302, 0.04741228, -0.02731293], [ 0.02198115, 0.03989581, 0.13165356, 0.06523556, 0.14900513, 0.01858517, -0.01644249, -0.02377043], [ 0.18718374, 0.10347525, -0.06668846, 0.25818944, 0.07522523, 0.07082067, 0.05170904, 0.22902426], [-0.01912907, -0.01732869, 0.00391375, -0.02338142, 0.02787969, -0.02744135, 0.0074541 , 0.01806928], [-0.01993229, -0.04436176, 0.07624088, 0.04268746, -0.00883252, 0.00789542, -0.03039453, 0.05851226], [-0.12873659, -0.00083202, -0.03246918, 0.23910245, -0.24635716, 0.10966355, 0.02079294, -0.03829115], [ 0.00225757, -0.12751001, -0.12703758, 0.17167819, -0.03712473, 0.04252302, 0.04741228, -0.02731293]]], dtype=float32) 可以看出，embedding层将(1, 20)的一个输入sample（最长为20个单词的句子，其中每个单词表示为一个int数字），嵌入为一个(1, 20, 8)的向量，即将每个单词embed为一个8维的向量，而整个embedding层的参数就由神经网络学习得到，数据经过embedding层之后就方便地转换为了可以由CNN或者RNN进一步处理的格式。 参考 How to Use Word Embedding Layers for Deep Learning with Keras - Machine Learning Mastery","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankchen.xyz/tags/Deep-Learning/"},{"name":"Keras","slug":"Keras","permalink":"http://frankchen.xyz/tags/Keras/"}]},{"title":"神经网络术语大百科：优化函数、激活函数、损失函数、正则方法的简介","slug":"Neural-Network-Terms-Introduction-to-Optimization-Functions-Activation-Functions-Loss-Functions-and-Regular-Functions","date":"2017-12-15T04:45:44.000Z","updated":"2017-12-16T09:21:29.262Z","comments":true,"path":"2017/12/15/Neural-Network-Terms-Introduction-to-Optimization-Functions-Activation-Functions-Loss-Functions-and-Regular-Functions/","link":"","permalink":"http://frankchen.xyz/2017/12/15/Neural-Network-Terms-Introduction-to-Optimization-Functions-Activation-Functions-Loss-Functions-and-Regular-Functions/","excerpt":"简述关于神经网络的各种优化函数（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）、各种激活函数（Sigmoid，Tanh、Hard Sigmoid、Softplus、ReLU、ElU、PReLU、RReLU）、各种损失函数以及正则方法的简述，并附带代码实现例子。","text":"简述关于神经网络的各种优化函数（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）、各种激活函数（Sigmoid，Tanh、Hard Sigmoid、Softplus、ReLU、ElU、PReLU、RReLU）、各种损失函数以及正则方法的简述，并附带代码实现例子。 优化函数先上两张图 激活函数没有激活函数，神经元就只是一个线性函数，那么无论多少层的神经元叠加是没有意义的。而主流激活函数也随着神经网络、深度学习的发展迭代进化了许多次代。 SigmoidSigmoid是S形状的意思，又因为它是逻辑回归的激活函数又叫logistic函数，函数式为$y = 1 / (1 + exp(-x))$是很早以前最常用的激活函数，其实也是有一些优点的，比如， 值域位于0-1，那么对于逻辑回归，这是对于二分类的一个很自然的表达，也就是概率 处处连续可导 不过呢，我们观察它的形状，可以得出，Sigmoid函数在两端（靠近0和1的部分）梯度很小，这也意味着，如果神经元的输出落到了这个地方，那么它几乎没什么梯度可以传到后面，而随着神经网络的层层削弱，后面的层（靠近输入的层）没有多少梯度能传过来，几乎就“学不到什么”了。这叫做梯度消失问题，一度是阻碍神经网络往更深的层进化的主要困难，导致深度学习专家们绞尽脑汁想了许多方法来对抗这个问题，比如“Xavier and He Initialization”，比如我们要把weight随机初始化为如下的范围， sigmoid的另一个问题是它不是0均值的，Sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢。举例来讲，对，如果所有均为正数或负数，那么其对的导数总是正数或负数，这会导致如下图红色箭头所示的阶梯式更新，这显然并非一个好的优化路径。深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。 如今，sigmoid函数应用最广泛的在于其变种softmax在多元分类中，比如手写数字识别，经过卷积神经网络的处理，最后我们需要网络输出每个预测的概率值，最后预测为某一个数字，这里就需要用到softmax，以下是softmax的Keras代码，注意其中一个trick，e = K.exp(x - K.max(x, axis=axis, keepdims=True))这里每个分量减去最大值是为了减少计算量。1234567891011121314151617181920212223def softmax(x, axis=-1): \"\"\"Softmax activation function. # Arguments x : Tensor. axis: Integer, axis along which the softmax normalization is applied. # Returns Tensor, output of softmax transformation. # Raises ValueError: In case `dim(x) == 1`. \"\"\" ndim = K.ndim(x) if ndim == 2: return K.softmax(x) elif ndim &gt; 2: e = K.exp(x - K.max(x, axis=axis, keepdims=True)) s = K.sum(e, axis=axis, keepdims=True) return e / s else: raise ValueError('Cannot apply softmax to a tensor that is 1D') tanh tanh 是sigmoid的变形： $tanh(x)=2sigmoid(2x)-1$，与 sigmoid 不同的是，tanh 是0均值的。因此，实际应用中，tanh 会比 sigmoid 更好一些， ReLU家族然而标准ReLU不是完美的，比如因为ReLU在小于0的坐标梯度都是0，那么会造成“死亡”的神经元的问题：一旦神经元的输入与权重之乘积是负的，那么经过ReLU的激活，输出就是0，而ReLU的0梯度让“死亡”的神经元无法“复活”：没办法回到输出不是0的状态，这样就出现了许多在ReLU的变种，一般都是对标准ReLU坐标轴左边的部分做文章，比如leaky ReLU。其公式就是$LeakyReLU_ α (z) = max(\\alpha z,z)$。如图， 这篇文章Empirical Evaluation of Rectified Activations in Convolution Network对比了几种leaky ReLU，比如把$\\alpha$设置为0.2效果总是好过0.01，并且，对于randomized leaky ReLU (RReLU)（其中$\\alpha$设置为一个在指定范围内的随机数），效果也不错，而且还具有一定的正则作用。另外，对于parametric leaky ReLU (PReLU)（其中$\\alpha$作为网络的一个参数，被反向传播学习出来，之前的$\\alpha$都是超参数，不能学只能调节），这种变种对于大数据集不错，但是数据量过小就有过拟合的风险。以下是Keras里面relu的代码， 123456789101112131415161718192021222324def relu(x, alpha=0., max_value=None): \"\"\"Rectified linear unit. With default values, it returns element-wise `max(x, 0)`. # Arguments x: A tensor or variable. alpha: A scalar, slope of negative section (default=`0.`). max_value: Saturation threshold. # Returns A tensor. \"\"\" if alpha != 0.: negative_part = tf.nn.relu(-x) x = tf.nn.relu(x) if max_value is not None: max_value = _to_tensor(max_value, x.dtype.base_dtype) zero = _to_tensor(0., x.dtype.base_dtype) x = tf.clip_by_value(x, zero, max_value) if alpha != 0.: alpha = _to_tensor(alpha, x.dtype.base_dtype) x -= alpha * negative_part return x 另外，在这篇文章里面FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)，引入了一种新的ReLU，exponential linear unit (ELU)，公式如下，$$ELU_{\\alpha}(z) = \\alpha (\\exp(z)-1) \\ if \\ z \\lt 0 ; \\ z \\ if \\ z \\gt 0;$$ 与标准ReLU最大的区别在于它处处连续可导，这使得梯度下降得到加速，收敛得到了加速，而使用了指数函数使得其测试阶段的计算代价更高。Keras里elu的实现， 123456789101112131415def elu(x, alpha=1.): \"\"\"Exponential linear unit. # Arguments x: A tenor or variable to compute the activation function for. alpha: A scalar, slope of positive section. # Returns A tensor. \"\"\" res = tf.nn.elu(x) if alpha == 1: return res else: return tf.where(x &gt; 0, res, alpha * res) 激活函数的选择一般来说，我们的选择顺序可以理解为：ELU &gt; leaky ReLU (以及其变种) &gt; ReLU &gt; tanh &gt; logistic。但是， 如果我们更顾虑模型运行速度，那么leaky ReLU可能比ELU更好； 如果我们不想调节超参数，那么用默认的$\\alpha$就行，ReLU和ELU的分别是0.01和1； 如果算力足够可以用来调参，那么如果网络过拟合我们会选择RReLU，如果训练集数据足够多，那可以用PReLU。","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankchen.xyz/tags/Deep-Learning/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankchen.xyz/tags/Algorithm/"},{"name":"Data Science","slug":"Data-Science","permalink":"http://frankchen.xyz/tags/Data-Science/"},{"name":"Python","slug":"Python","permalink":"http://frankchen.xyz/tags/Python/"}]},{"title":"如何理解Pandas 和 Numpy里的axis","slug":"Understanding-the-axis-parameter-in-Pandas-and-Numpy","date":"2017-12-12T10:36:04.000Z","updated":"2018-03-09T11:12:08.000Z","comments":true,"path":"2017/12/12/Understanding-the-axis-parameter-in-Pandas-and-Numpy/","link":"","permalink":"http://frankchen.xyz/2017/12/12/Understanding-the-axis-parameter-in-Pandas-and-Numpy/","excerpt":"简述一种如何直观的理解Pandas 和 Numpy里面的axis参数的方法。","text":"简述一种如何直观的理解Pandas 和 Numpy里面的axis参数的方法。 Numpy 和 Pandas里的sort、mean、drop等操作，不是分行或者列分别用一个method来定义，而是一个method里面用户指定axis来操作的，举例来说： 我们先在此处下载了一份各国酒类消费的csv文件为例。如下是pandas里按axis 0和1进行drop的操作示例，我们很容易看出，axis 0是按行drop，而axis 1是按列drop： 但是，mean操作呢？ 容易看出，axis 0得出了每一列的均值，而axis 1得出了则是每一行的均值。那么，在Numpy里呢？ 容易看出，axis为1的时候得出的是每行的sum，axis为0的时候得出了每列的sum。 由上面的例子，我们似乎可以看出，axis为1代表水平方向上的操作，axis为0代表垂直方向上的操作，比如axis为1的sum得出的就是每一行的和。 但是，在Pandas的Dataframe里面，为什么axis=1代表的是drop整个列呢？以下这个例子也可以说明一些情况： 联系这个视频How do I use the “axis” parameter in pandas? - YouTube，大家也可以得到一些结论，作者说： 0 is the row axis, and 1 is the column axis. When you drop with axis=1, that means drop a column. When you take the mean with axis=1, that means the operation should “move across” the column axis, which produces row means.指的就是一种更加容易理解的方式，“0就是行的axis，1就是列的axis，当以axis=1来drop，那么就是drop一个column，而axis=1 来取mean，那么就是这个操作‘穿越’了列的axis，产生了行上的mean”。 另外，其实我们也可以这样来操作， 可以看出，axis=0与axis=’rows’是一样的（在Pandas里），是不是更加容易理解了？ How to Use t-SNE Effectively这个网站给了一个非常形象的t-SNE在线实验环境，推荐大家去看一看！","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"Data Science","slug":"Data-Science","permalink":"http://frankchen.xyz/tags/Data-Science/"},{"name":"Numpy","slug":"Numpy","permalink":"http://frankchen.xyz/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://frankchen.xyz/tags/Pandas/"}]},{"title":"理解triplet loss","slug":"understanding-triplet-loss-and-example-code","date":"2017-12-01T09:19:05.000Z","updated":"2018-03-30T02:23:22.993Z","comments":true,"path":"2017/12/01/understanding-triplet-loss-and-example-code/","link":"","permalink":"http://frankchen.xyz/2017/12/01/understanding-triplet-loss-and-example-code/","excerpt":"理解triplet loss，与给出TensorFlow和numpy两种形式的example code。","text":"理解triplet loss，与给出TensorFlow和numpy两种形式的example code。 Triplet Loss 是当前应用的很广泛的一种损失函数，在人脸识别和聚类领域，这是一种很自然的映射与计算损失的方式，比如FaceNet里，通过构建一种embedding 方式，将人脸图像直接映射到欧式空间，而优化这种embedding的方法可以概括为，构建许多组三元组（Anchor，Positive，Negative），其中Anchor与Positive同label，Anchor与Negative不同label（在人脸识别里面，就是Anchor与Positive是同一个个体，而与Negative是不同个体），通过学习优化这个embedding，使得欧式空间内的Anchor与Positive 的距离比与Negative的距离要近。 公式表示用公式表示就是，我们希望： $$\\left\\lVert f(x^a_i) - f(x^p_i) \\right\\rVert ^2_2 +\\alpha \\lt \\left\\lVert f(x^a_i) - f(x^n_i) \\right\\rVert ^2_2 , \\\\forall (f(x^a_i) , f(x^p_i) , f(x^n_i)) \\in \\mathscr T$$ 其中$\\alpha$ 是强制的正例和负例之间的margin，$\\mathscr T$是具有基数为$N$的训练集中的三元组的集合。 那么，损失函数很自然的可以写为： $$\\sum^N_i\\Bigl [\\left\\lVert f(x^a_i) - f(x^p_i) \\right\\rVert ^2_2 + \\left\\lVert f(x^a_i) - f(x^n_i) \\right\\rVert ^2_2 + \\alpha \\Bigr ] _ +$$ 其中加号指的，如果中括号内部分大于0，则没有损失（Anchor与Positive的距离加上margin小于与Negative的距离），否则计算这个距离为损失。 代码表示Numpy 实现 123456789101112131415import numpy as npbatch_size = 3*12embedding_size = 16# 构造batch_size * embedding_size 维度的随机矩阵emb = np.random.uniform(size=[])# 对emb逢三取1、2、3行分别为Anchor、Positive、Negative# 计算其2范数的距离即欧氏距离pos_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[1::3,:]), axis=1) neg_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[2::3,:]), axis=1)# 这里就是照抄公式了，注意mean和sum是一样的np_triplet_loss = np.mean(np.maximum(0., pos_dist_sqr-neg_dist_sqr+alpha)) TensorFlow 实现 123456789101112131415161718import tensorflow as tfbatch_size = 3*12embedding_size = 16alpha = 0.2def triplet_loss(anchor, positive, negative, alpha): with tf.variable_scope('triplet_loss'): pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1) neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1) basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha) loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), None) return loss# 构建矩阵embeddings = tf.placeholder(np.float64, shape=(batch_size, embedding_size), name='embeddings')# 先将embeddings矩阵第0维resize为(?, 3)维，第1维不变，变为三维矩阵(-1, 3, embedding_size)，再在其第二维度为3上unstack为三份anchor, positive, negative = tf.unstack(tf.reshape(embeddings, shape=(-1, 3, embedding_size)), axis=1) 完整代码如下，这里测试对比了两种实现： 12345678910111213141516171819202122232425262728293031import tensorflow as tfimport numpy as npbatch_size = 3*12embedding_size = 16alpha = 0.2def triplet_loss(anchor, positive, negative, alpha): with tf.variable_scope('triplet_loss'): pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1) neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1) basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha) loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), None) return losswith tf.Graph().as_default(): embeddings = tf.placeholder(np.float64, shape=(batch_size, embedding_size), name='embeddings') anchor, positive, negative = tf.unstack(tf.reshape(embeddings, shape=(-1, 3, embedding_size)), axis=1) triplet_loss = triplet_loss(anchor, positive, negative, alpha) sess = tf.Session() with sess.as_default(): np.random.seed(666) emb = np.random.uniform(size=[batch_size, embedding_size]) tf_triplet_loss = sess.run(triplet_loss, feed_dict=&#123;embeddings:emb&#125;) pos_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[1::3,:]), axis=1) neg_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[2::3,:]), axis=1) np_triplet_loss = np.mean(np.maximum(0., pos_dist_sqr-neg_dist_sqr+alpha)) np.testing.assert_almost_equal(tf_triplet_loss, np_triplet_loss, decimal=5, err_msg='Triplet loss is incorrect')","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankchen.xyz/tags/Algorithm/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://frankchen.xyz/tags/TensorFlow/"}]},{"title":"数据的标准化与归一化","slug":"Data-Normalization-and-Standardization","date":"2017-11-29T03:53:57.000Z","updated":"2017-12-01T11:10:13.685Z","comments":true,"path":"2017/11/29/Data-Normalization-and-Standardization/","link":"","permalink":"http://frankchen.xyz/2017/11/29/Data-Normalization-and-Standardization/","excerpt":"聊一聊Normalization and Standardization","text":"聊一聊Normalization and Standardization 什么是Normalization就是归一化，是最小-最大缩放(min-max scaling)的特例，指的是将数据缩放到指定range，这个range通常是0~1或者-1~+1，直观来讲就是下图，在数据不包含离群点时很有用， 公式则是 $$x^{(i)}_{norm} = \\frac {x^{(i)} - x_{min}} {x_{max} - x_{min}}$$ 若要缩放至-1~+1，则是$$x’ = \\frac{x - min}{max - min}$$ 代码实现 12345678#导入数据预处理库from sklearn import preprocessing#范围缩放标准化min_max_scaler = preprocessing.MinMaxScaler()#训练集缩放标准化min_max_scaler.fit_transform(X_train) 12#测试集缩放标准化min_max_scaler.fit_transform(X_test) Z-score 标准化指的是，通过缩放让数据的均值为0（移除均值），标准差为固定值（比如1）。在许多模型里，如SVM的RBF、线性模型的 L1 &amp; L2 正则项对于所有的feature都有这样的假设。$$x^{(i)}_{std} = \\frac{x^{(i)} - \\mu_x}{\\sigma_x}$$ 以下是一个简单的例子展示了两者的区别： 12345678#导入数据预处理库from sklearn import preprocessing#数据标准化scaler = preprocessing.StandardScaler().fit(X_train)#训练集数据标准化scaler.transform(X_train) 同时对测试集的数据进行标准化处理，以保证训练集和测试集的变换方式相同。 12#测试集数据标准化scaler.transform(X_test) 值得注意从流程上讲，标准化和归一化应该在读入数据、处理缺失值，切分训练测试集之后，而且我们要做的是在切分之后，在训练集fit，再去transform测试集，而不是在整个数据上转换以后再切分，因为无论是标准化还是归一化，我们要么利用到了数据的max min值，要么利用到了数据的均值和标准差，这些数值在训练之前是不能被测试集所影响的。 类似于缺失值的填充，举个例子，我们使用均值填充以下数据， 123456#使用均值填充缺失值imp = Imputer(missing_values=\"NaN\", strategy='mean', axis=0)imp.fit(X_train)#填充训练集X_train=imp.transform(X_train) 以同样的方式填充测试集，以保证测试集和训练集的数据填充方式保持一致。 12#填充测试集X_test=imp.transform(X_test)","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"Data Science","slug":"Data-Science","permalink":"http://frankchen.xyz/tags/Data-Science/"}]},{"title":"TensorFlow on a GTX 1080","slug":"TensorFlow-on-a-GTX-1080","date":"2017-11-13T10:23:28.000Z","updated":"2018-03-15T05:10:21.147Z","comments":true,"path":"2017/11/13/TensorFlow-on-a-GTX-1080/","link":"","permalink":"http://frankchen.xyz/2017/11/13/TensorFlow-on-a-GTX-1080/","excerpt":"Ubuntu 16.03 安装 CUDA、NVIDIA驱动，CUDNN及GPU版TensorFlow。","text":"Ubuntu 16.03 安装 CUDA、NVIDIA驱动，CUDNN及GPU版TensorFlow。GPU 支持的TensorFlow让算力大幅提升，但是安装好一切支持却不那么容易！其实主要是三个东西： Nvidia 驱动：显卡驱动 CUDA Toolkit CUDA工具箱 CUDNN：CUDA Deep Neural Network library 神经网络库函数依赖 12345678910111213$ sudo apt-get update$ sudo apt-get install \\ freeglut3-dev \\ g++-4.9 \\ gcc-4.9 \\ libglu1-mesa-dev \\ libx11-dev \\ libxi-dev \\ libxmu-dev \\ nvidia-modprobe \\ python-dev \\ python-pip \\ python-virtualenv 安装Nvidia驱动1234$ sudo apt-get purge nvidia-* 删除nvidia 之前的$ sudo add-apt-repository ppa:graphics-drivers/ppa$ sudo apt-get update$ sudo apt-get install nvidia-384 可在Proprietary GPU Drivers : “Graphics Drivers” team查看当前稳定版本Nvidia驱动，如笔者当前（2017-11-13）版本是‘nvidia-384’。 接下来重启$ sudo reboot。重启后，检测Nvidia驱动安装情况， 1$ cat /proc/driver/nvidia/version 12NVRM version: NVIDIA UNIX x86_64 Kernel Module 384.98 Thu Oct 26 15:16:01 PDT 2017GCC version: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 显示Nvidia’s system management interface： 1$ sudo nvidia-smi 123456789101112131415161718+-----------------------------------------------------------------------------+| NVIDIA-SMI 384.98 Driver Version: 384.98 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 1080 Off | 00000000:01:00.0 On | N/A || 0% 47C P8 12W / 215W | 7992MiB / 8112MiB | 2% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 994 G /usr/lib/xorg/Xorg 193MiB || 0 1889 G compiz 151MiB || 0 5068 C /home/frank/anaconda3/bin/python 7643MiB |+-----------------------------------------------------------------------------+ 设置GCC 4.9为默认 12345$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 10$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 20$ sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 10$ sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 20 安装CUDA当前虽然CUDA-9.0已经发布，但是TensorFlow默认编译版本还是基于CUDA-8.0的，我们在这里CUDA Toolkit 8.0 - Feb 2017 | NVIDIA Developer下载runfile 使用如下安装1sudo cuda_8.0.61_375.26_linux.run --override 安装时记得 12345678910111213141516171819202122232425262728293031Do you accept the previously read EULA? (accept/decline/quit): acceptYou are attempting to install on an unsupported configuration. Do you wish to continue? ((y)es/(n)o) [ default is no ]: yesInstall NVIDIA Accelerated Graphics Driver for Linux-x86_64 352.39? ((y)es/(n)o/(q)uit): noInstall the CUDA 8.0 Toolkit? ((y)es/(n)o/(q)uit): yesEnter Toolkit Location [ default is /usr/local/cuda-8.0 ]:Do you want to install a symbolic link at /usr/local/cuda? ((y)es/(n)o/(q)uit): yesInstall the CUDA 8.0 Samples? ((y)es/(n)o/(q)uit): noInstalling the CUDA Toolkit in /usr/local/cuda-8.0 ...============ Summary =8.0===========Driver: Not SelectedToolkit: Installed in /usr/local/cuda-8.0Samples: Not SelectedPlease make sure that - PATH includes /usr/local/cuda-8.0/bin - LD_LIBRARY_PATH includes /usr/local/cuda-8.0/lib64, or, add /usr/local/cuda-8.0/lib64 to /etc/ld.so.conf and run ldconfig as rootTo uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-8.0/binTo uninstall the NVIDIA Driver, run nvidia-uninstallPlease see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-8.0/doc/pdf for detailed information on setting up CUDA.***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 352.00 is required for CUDA 8.0 functionality to work.To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file: sudo &lt;CudaInstaller&gt;.run -silent -driverLogfile is /tmp/cuda_install_14557.log 记得上面这里也有个询问你是否安装Nvidia驱动的地方，因为我们前面已经安装了最新的版本，这里当然选择no。 添加环境变量 123$ echo 'export PATH=/usr/local/cuda/bin:$PATH' &gt;&gt; ~/.bashrc$ echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc$ source ~/.bashrc 查看CUDA compiler 1$ nvcc -V 1234nvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2016 NVIDIA CorporationBuilt on Tue_Jan_10_13:22:03_CST_2017Cuda compilation tools, release 8.0, V8.0.61 安装CUDA Deep Neural Network library ：CUDNN在此处下载cuDNN Download | NVIDIA Developer，可能需要我们注册账号登录。选择适配CUDA的版本，以及cuDNN v7.0 Library for Linux，这个就是个targz文件。 接下来操作就是把cudnn的几个库放到cuda里面：1234$ tar xvf cudnn-8.0-linux-x64-v7.tgz$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/$ sudo chmod a+r /usr/local/cuda/lib64/libcudnn* TensorFlow安装pip install --upgrade tfBinaryURL即可，这里的tfBinaryURL可在Installing TensorFlow on Ubuntu | TensorFlow选取，例如我这里选取Python3.6的GPU Support： 验证TensorFlow安装1234567891011121314In [1]: import tensorflow as tfIn [2]: sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))2017-11-13 18:54:59.081831: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA2017-11-13 18:54:59.186280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2017-11-13 18:54:59.186604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.86pciBusID: 0000:01:00.0totalMemory: 7.92GiB freeMemory: 7.46GiB2017-11-13 18:54:59.186617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)Device mapping:/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.12017-11-13 18:54:59.216573: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1 如上，打印出这些信息就证明安装成功啦！ CUDA ToolKit 8.0 升级到9.0指南主要就是需要下载CUDA ToolKit 9.0 的安装包，和8.0一样安装，注意下面的四步骤我们只需要第二步（ToolKit）和第四步（创建软链接，原有的是指向8.0的） 因为CUDNN被放在CUDA ToolKit 8.0内，所有这里我们需要重新下载CUDNN并解压到CUDA ToolKit 9.0文件夹内， 再在Installing TensorFlow on Ubuntu | TensorFlow 12pip install --ignore-installed --upgrade \\&lt;url&gt; url选取如下的你需要的python版本的GPU网址即可。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://frankchen.xyz/tags/TensorFlow/"}]},{"title":"frp的内网穿透及外网访问内网jupyter-notebook的实现","slug":"ftp-using","date":"2017-11-11T18:20:38.000Z","updated":"2017-11-13T08:02:04.255Z","comments":true,"path":"2017/11/12/ftp-using/","link":"","permalink":"http://frankchen.xyz/2017/11/12/ftp-using/","excerpt":"","text":"fatedier/frp: A fast reverse proxy to help you expose a local server behind a NAT or firewall to the internet.是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp, http, https 协议， 这里我们用它来搭建外网访问内网jupyter-notebook的服务。我们基于Ubuntu16.04 选用amd64版本， server设置server就是你拥有外网IP的服务器123[common]bind_port = 7000vhost_http_port = 8888 用./frps -c ./frps.ini启动，注意服务端需要先启动。 client设置client就是没有外网IP，但是你想在外网访问的机器， XXXXXXXXX就是上面的server的外网IP。 12345678[common]server_addr = XXXXXXXXXserver_port = 7000[web]type = httplocal_port = 8888custom_domains = XXXXXXXXX 以./frpc -c ./frpc.ini启动。 使用接下来，在client端，在8888段端口启动jupyter-notebook即可在XXXXXXXXX:8888访问内网机器上的Notebook了。 另外，由于jupyter-notebook自带终端，这也一举两得，也是一个内网穿透ssh的方案。当然，必须使用一些运维工具来保证服务的稳定性，如supervisor，可参考使用supervisor支持Python3程序 | 不正经数据科学家。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"}]},{"title":"Pycharm Pro","slug":"pycharm-pro","date":"2017-11-08T10:44:13.000Z","updated":"2017-11-09T08:31:53.000Z","comments":true,"path":"2017/11/08/pycharm-pro/","link":"","permalink":"http://frankchen.xyz/2017/11/08/pycharm-pro/","excerpt":"Pycharm，只为提高python开发者的生产力！","text":"Pycharm，只为提高python开发者的生产力！ 版本管理版本管理，主要是依靠这两个按钮，左边是pull，右边是commit。一般我们开发，打开项目，先pull下代码仓库的变更，开始开发，然后commit，再pull，合并冲突，再push。pycharm非常人性化地为我们标出了，黑色则是没有变更，蓝色是有变更，绿色是新add的文件。commit时，可以很方便地看出变更对比，对于需要回滚的零时操作文件可以用紫色的revert按钮回退变更，总之填写commit message之后就可以commit了。然后，为防止在此期间，代码仓库又有人push了新变更，在push之前，我们需要再次pull，如果没有变更，push即可。如果有冲突呢，pycharm有非常human的解决冲突界面，总之，选择修改的、丢弃的、保留的，就可以push了，当然，这次push会有两条message，第二条是解决冲突的。 远程调试利用pycharm我们可以在服务器直接run、debug，非常的便捷。首先要设置deployment，选择SFTP，也就是ssh，这里用密码或者私钥都是ok的。设置好映射目录，接下来添加远程解释器，，并勾选auto upload，那么，每次本地的更改都会同步到服务器，直接run或者debug都是获取服务器的结果，非常方便。 ## DEBUGDEBUG可谓是开发中最最重要的技能了，也许我们在初级的开发的时候，还可以依靠各种print变量来查看，那么工程一旦变复杂，debug就必不可少了，；举个栗子，我们用TensorFlow写深度学习工程的时候，如果出现矩阵维度不匹配的情况，这个时候用debug去观察各个维度就相当高效了。以下图为例，我们做debug，当然先要在我们希望观察的代码处打断点（Break Point），就是左边的红色小点，第一个断点debug停下来的地方。接下来就是debug按钮了，也就是红框内的这些箭头，依次是 Step Over 直接从当前断点步进到下一个断点，也就是我们不希望看到中间的任何调用，直接跳过 Step Into，从当前断点，每一次调用都进入，如果调用比较多，可能很繁因为会一步步深入 Step Into Mycode，简单明了，一行一行的步进，不跳转到调用 Force Step Into，强制步进进入 Step Out 跳出当前调用，和Step Into结合，一个进入、一个跳出 Run to Cursor 不需打断点，直接步进到光标所在处，这个也很方便，只需把光标放在某处点击即可此处类似堆栈，后面调用的进程在上面。当然，最主要的，上面这些步骤按钮的最终目的就是观察Variable的变化，通过步进观察各个变量的信息，以找出bug等等。 Python Console相比iPython与Notebook，Pycharm自带的Python Console有其独到的优势，比如Special Variables与Code History ，如图，分别用一个两个三个下划线代表上一次上两次上三次的变量，在这里可以直接像debug一样查看各个变量的值；Code History则是使用户方便地使用之前的代码。 快捷键调整代码行次序Command + Shift + ⬆️/⬇️有时我们需要调整两行代码的上下次序，那么用剪切、粘贴的方法不如这个方法简洁自然。 查看源码及跳转摁住Command键去点击即可跳转到源码处，而查看源码时我们可以通过Command + [/]来方便的前进或后退。 批量展开收缩代码Command + Shift +/- 当项目写到一定规模的时候，难免方法/函数会很多，这个时候我们可以使用此命令来收缩代码，这个主要是为了方便查看。 快速插入常用代码Command + J 是弹出插入常用代码块的快捷键，比如Dict/List/Set 的comprehension都有，之前我只会‘main’然后跳出if __name__ == &#39;__main__&#39;:😜 一键 PEP8其实在了解这个tips之前我都是点击函数名，等待一个黄色的小灯泡再去点击灯泡。。。其实只需要Command+Option+L即可！ cheat sheet顺便更新两张cheat sheet","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"Pycharm","slug":"Pycharm","permalink":"http://frankchen.xyz/tags/Pycharm/"}]},{"title":"Go 语言操作与扫描 Hbase 实例","slug":"go-hbase","date":"2017-11-08T03:10:21.000Z","updated":"2017-11-09T02:01:07.000Z","comments":true,"path":"2017/11/08/go-hbase/","link":"","permalink":"http://frankchen.xyz/2017/11/08/go-hbase/","excerpt":"记录纯go语言的gohbase客户端的扫描操作。","text":"记录纯go语言的gohbase客户端的扫描操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package mainimport ( \"github.com/tsuna/gohbase\" \"github.com/tsuna/gohbase/hrpc\" \"context\" \"io\" \"fmt\" \"github.com/tsuna/gohbase/filter\" \"strconv\" \"time\")const table = \"user\"func beforeMiniTimeStamps(beforeMini int) string &#123; //当前时刻某分钟之前的时间戳 return strconv.Itoa(int(time.Now().Add(- time.Duration(beforeMini) * time.Minute).UnixNano() / 1000000))&#125;func fetch() []*hrpc.Result &#123; client := gohbase.NewClient(\"hmaster.shise.com,rm.shise.com,nn.shise.com\") //client := gohbase.NewClient(\"wwj.shise.com,czn.shise.com,czn.shise.com\") // 列族 family := hrpc.Families(map[string][]string&#123;\"c\": nil&#125;) // 全局hbase filter时间间隔 //timeRange := hrpc.TimeRange(time.Now().Add(- time.Duration(minute)*time.Minute), time.Now()) // 某列value的filter notRecommendFilter := filter.NewSingleColumnValueFilter([]byte(\"c\"), []byte(\"notRecommend\"), filter.NotEqual, filter.NewBinaryComparator(filter.NewByteArrayComparable([]byte(\"true\"))), true, true) violationFilter := filter.NewSingleColumnValueFilter([]byte(\"c\"), []byte(\"violation\"), filter.NotEqual, filter.NewBinaryComparator(filter.NewByteArrayComparable([]byte(\"true\"))), true, true) // filter某列value的时间戳 timeStartFilter := filter.NewSingleColumnValueFilter([]byte(\"c\"), []byte(\"createDate\"), filter.Greater, filter.NewBinaryComparator(filter.NewByteArrayComparable([]byte(beforeMiniTimeStamps(4*60)))), true, true) timeEndFilter := filter.NewSingleColumnValueFilter([]byte(\"c\"), []byte(\"createDate\"), filter.Less, filter.NewBinaryComparator(filter.NewByteArrayComparable([]byte(beforeMiniTimeStamps(2*60)))), true, true) //filter 列表 filters := filter.NewList(filter.MustPassAll, notRecommendFilter, violationFilter, timeStartFilter, timeEndFilter) //创建scan对象 scan, _ := hrpc.NewScanStr(context.Background(), table, family, hrpc.Filters(filters)) var rsp []*hrpc.Result scanner := client.Scan(scan) for &#123; res, err := scanner.Next() if err == io.EOF &#123; break &#125; if err != nil &#123; print(err) &#125; if hasHeadImage(res) &#123; rsp = append(rsp, res) &#125; &#125; return rsp&#125;func hasHeadImage(res *hrpc.Result) bool &#123; return true&#125;func main() &#123; rsp := fetch() for _, item := range rsp &#123; fmt.Println(*item) break &#125; fmt.Println(len(rsp))&#125;","categories":[],"tags":[{"name":"Go","slug":"Go","permalink":"http://frankchen.xyz/tags/Go/"},{"name":"Hbase","slug":"Hbase","permalink":"http://frankchen.xyz/tags/Hbase/"}]},{"title":"Ubuntu16.04 安装IKEV2 VPN 并在Mac上使用以解决Google Drive 同步问题","slug":"Google-Drive-Sync","date":"2017-11-01T01:56:43.000Z","updated":"2017-11-01T02:21:02.000Z","comments":true,"path":"2017/11/01/Google-Drive-Sync/","link":"","permalink":"http://frankchen.xyz/2017/11/01/Google-Drive-Sync/","excerpt":"","text":"Google Drive无法识别shadowsocks所用的socks5代理，故这边有需求在VPS上部署http代理的VPN。 服务端安装说明 下载脚本: wget --no-check-certificate https://raw.githubusercontent.com/quericy/one-key-ikev2-vpn/master/one-key-ikev2.sh 运行脚本： 12chmod +x one-key-ikev2.shbash one-key-ikev2.sh 等待自动配置部分内容后，选择vps类型（OpenVZ还是Xen、KVM），选错将无法成功连接，请务必核实服务器的类型。输入服务器ip或者绑定的域名(连接vpn时服务器地址将需要与此保持一致,如果是导入泛域名证书这里需要写*.域名的形式),这里推荐直接输入域名。 选择使用使用证书颁发机构签发的SSL证书还是生成自签名证书，这里我们选择自签名即选择no：‘’ 如果选择no,使用自签名证书（客户端如果使用IkeV2方式连接，将需要导入生成的证书并信任）则需要填写证书的相关信息(C,O,CN)，为空将使用默认值(default value)，确认无误后按任意键继续,后续安装过程中会出现输入两次pkcs12证书的密码的提示(可以设置为空) 接下来一直空格即可。*看到install Complete字样即表示安装完成。默认用户名密码将以黄字显示，可根据提示自行修改配置文件中的用户名密码,多用户则在配置文件中按格式一行一个(多用户时用户名不能使用%any),保存并重启服务生效。 将提示信息中的证书文件ca.cert.pem拷贝到客户端，修改后缀名为.cer后导入。ios设备使用Ikev1无需导入证书，而是需要在连接时输入共享密钥，共享密钥即是提示信息中的黄字PSK. 客户端配置说明 iOS/OSX/Windows7+/WindowsPhone8.1+/Linux 均可使用IkeV2,认证方式为用户名+密码。使用SSL证书则无需导入证书；使用自签名证书则需要先导入证书才能连接,可将ca.cert.pem更改后缀名作为邮件附件发送给客户端,手机端也可通过浏览器导入,其中: iOS/OSX 的远程ID和服务器地址保持一致,用户鉴定选择”用户名”.如果通过浏览器导入,将证书放在可访问的远程外链上,并在系统浏览器(Safari)中访问外链地址; 注意OSX导入后需要在钥匙串内设置信任，如： 设置连接成功后Google Drive就会连接成功开始同步😎😎😎😎😎😎 参考自 CentOS/Ubuntu一键安装IPSEC/IKEV2 VPN服务器 | Quericy Eden* 安装完成后，mac不能连接 · Issue #58 · quericy/one-key-ikev2-vpn","categories":[],"tags":[{"name":"VPN","slug":"VPN","permalink":"http://frankchen.xyz/tags/VPN/"}]},{"title":"Install Opencv3.2 on Ununtu 16.04","slug":"Install-Opencv3-2-on-Ununtu-16-04","date":"2017-10-25T04:34:28.000Z","updated":"2017-11-09T02:00:24.000Z","comments":true,"path":"2017/10/25/Install-Opencv3-2-on-Ununtu-16-04/","link":"","permalink":"http://frankchen.xyz/2017/10/25/Install-Opencv3-2-on-Ununtu-16-04/","excerpt":"Opencv3.2 在 Ununtu 16.04 上的编译安装","text":"Opencv3.2 在 Ununtu 16.04 上的编译安装 参考自Ubuntu 16.04编译安装OpenCV（Python） – WTF Daily Blog，不过这位博主装的是3.1版本，而且有些问题。 安装OpenCV依赖123456789sudo apt-get updatesudo apt-get upgradesudo apt-get install build-essential cmake pkg-config sudo apt-get install libjpeg8-dev libtiff5-dev libjasper-dev libpng12-dev sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev sudo apt-get install libxvidcore-dev libx264-devsudo apt-get install libgtk-3-devsudo apt-get install libatlas-base-dev gfortransudo apt-get install python2.7-dev python3.5-dev 下载OpenCV源码这里下载 3.2.0 123$ cd ~$ wget -O opencv.zip https://github.com/Itseez/opencv/archive/3.2.0.zip$ unzip opencv.zip 下载和OpenCV版本对应的opencv_contrib（一些扩展功能和non-free代码）：12$ wget -O opencv_contrib.zip https://github.com/Itseez/opencv_contrib/archive/3.2.0.zip$ unzip opencv_contrib.zip 编译安装123$ cd ~/opencv-3.2.0/$ mkdir build$ cd build 12345cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D INSTALL_C_EXAMPLES=OFF \\ -D OPENCV_EXTRA_MODULES_PATH=/root/Downloads/opencv_contrib-3.2.0/modules -D PYTHON_EXECUTABLE=/root/miniconda3/bin/python .. 其中OPENCV_EXTRA_MODULES_PATH是opencv_contrib的解压后的地址，PYTHON_EXECUTABLE是# 你的python 解释器地址 可用witch python 查看。若出现，需要下载ippicv_linux_20151201.tgz的长时间等待，可在此opencv_3rdparty/ippicv at ippicv/master_20151201 · opencv/opencv_3rdparty手动下载对应文件，并放在对应位置如Put the ippicv_linux…tgz under&lt;…&gt;/opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e/， 参考自incorrect hash in cmake ippicv when installing · Issue #5973 · opencv/opencv。 编译： $ make 安装： 12$ sudo make install$ sudo ldconfig 再pip install opencv即可😎","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"}]},{"title":"git notes in action | 生产环境各场景下git常用命令","slug":"git-notes-in-action","date":"2017-10-21T06:03:47.000Z","updated":"2017-11-09T01:59:12.000Z","comments":true,"path":"2017/10/21/git-notes-in-action/","link":"","permalink":"http://frankchen.xyz/2017/10/21/git-notes-in-action/","excerpt":"","text":"更改远程链接 Changing a remote’s URL - User Documentation redo add单文件操作 Undo a git add - remove files staged for a git commit | Open Data 移除所有已经add的文件，场景例如你刚刚add了all，才发现有许多是可以ignore的，那么就运行git rm --cached -r .移除也就是撤回刚刚add的所有文件，再去管理ignore，再add就好了。 revert当前文件变更，场景例如比如你上传本地的一个config上服务器调试，调试结束，需要将这个文件回滚变更，那么git checkout -- &lt;file-you-want-revert&gt;即可。","categories":[],"tags":[{"name":"git","slug":"git","permalink":"http://frankchen.xyz/tags/git/"}]},{"title":"服务器负载GUI 神器sargraph 的安装","slug":"sargraph-install","date":"2017-10-11T03:40:21.000Z","updated":"2017-10-12T09:25:36.000Z","comments":true,"path":"2017/10/11/sargraph-install/","link":"","permalink":"http://frankchen.xyz/2017/10/11/sargraph-install/","excerpt":"通过Linux 的sar命令可以很容易知道服务器的负载，那么如何通过网页等更好地可视化呢？本文介绍实现此功能的神器SARGRAPH-Graphical front-end for sar的使用及安装。 sar的配置通过这里我们可以看到sar 找出系统瓶颈的利器 — Linux Tools Quick Tutorial安装sar 有的linux系统下，默认可能没有安装这个包，使用apt-get install sysstat 来安装； 安装完毕，将性能收集工具的开关打开： vi /etc/default/sysstat 设置 ENABLED=”true” 启动这个工具来收集系统性能数据： /etc/init.d/sysstat start","text":"通过Linux 的sar命令可以很容易知道服务器的负载，那么如何通过网页等更好地可视化呢？本文介绍实现此功能的神器SARGRAPH-Graphical front-end for sar的使用及安装。 sar的配置通过这里我们可以看到sar 找出系统瓶颈的利器 — Linux Tools Quick Tutorial安装sar 有的linux系统下，默认可能没有安装这个包，使用apt-get install sysstat 来安装； 安装完毕，将性能收集工具的开关打开： vi /etc/default/sysstat 设置 ENABLED=”true” 启动这个工具来收集系统性能数据： /etc/init.d/sysstat start 可使用命令vi /etc/cron.d/sysstat调整报告频率，例如下面就将默认的十分钟修改为隔两分钟报告一次。 sargraph 的安装安装依赖根据Documentation， 安装php sudo apt-get install php libapache2-mod-php 安装apache2 12sudo apt-get updatesudo apt-get install apache2 配置apache2systemctl status apache2查看apache2的情况，若发现其不是active，可能是由于与nginx监听默认端口冲突，那么需要在vim /etc/apache2/ports.conf，把80修改为81即可。再‘service apache2 restart’，查看systemctl status apache2为active即成功， sargraph安装及配置参考Download Sargraph安装即可， Download sargraph_version3.tgz to /tmp. Unzip and untar it. And run the INSTALLER tar xzf sargraph_version3.tgz cd sargraph_version3 ./INSTALLER 注意需要修改其config，vim /etc/sargraph.conf， 注意是SARUSER修改为需要监听的服务器的用户名，KEY修改为sargraph服务器当前用户公钥文件即可。 添加server使用/var/www/html/sargraph/scripts/addserver datalab添加server，比如之前我们把config里user改为root，那么这里我们添加的server就是root@datalab，再可用/var/www/html/sargraph/scripts里的脚本添加删除用户修改密码等等。 最后在如Sargraph Login访问即可，负载等信息可视化出现！ 😎","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"}]},{"title":"避免 spark 提交 上传自带 jar包解决办法","slug":"spark-submit-avoid-upload-jars","date":"2017-09-01T08:15:04.000Z","updated":"2017-09-01T08:25:21.000Z","comments":true,"path":"2017/09/01/spark-submit-avoid-upload-jars/","link":"","permalink":"http://frankchen.xyz/2017/09/01/spark-submit-avoid-upload-jars/","excerpt":"","text":"1217/09/01 15:38:59 INFO yarn.Client: Uploading resource file:/usr/local/spark-2.1.1-bin-without-hadoop/spark-46d1bd70-b346-4027-bce4-9540f4b6035a/__spark_libs__4051900056689219834.zip -&gt; hdfs://wwj.shise.com:9000/user/hadoop/.sparkStaging/application_1504148698505_0021/__spark_libs__4051900056689219834.zip17/09/01 15:41:45 INFO yarn.Client: Uploading resource file:/Users/frank/IdeaProjects/simpleApp/target/scala-2.11/simpleApp-assembly-1.0.jar -&gt; hdfs://wwj.shise.com:9000/user/hadoop/.sparkStaging/application_1504148698505_0021/simpleApp-assembly-1.0.jar 可以看到，上传花费约3分钟，这段时间是为了将$SPARK_HOME/jar下的所有jar包上传到yarn，实际上可以完全避免。 实际上这部分文件完全可以就放在hdfs上， 先将这部分jar包复制到hdfs：hadoop fs -mkdir /tmp/spark/lib_jars/hadoop fs -put $SPARK_HOME/jars/* /tmp/spark/lib_jars/ 设置vim $SPARK_HOME/conf/spark-defaults.conf：添加这行1spark.yarn.jars /tmp/spark/lib_jars/* ##这里用hdfs相对路径即可 再submit不会出现将jar文件打包成zip文件上传的信息了。","categories":[],"tags":[{"name":"spark","slug":"spark","permalink":"http://frankchen.xyz/tags/spark/"}]},{"title":"hbase rest 绑定到内网localhost","slug":"hbase-rest-bindAddress-to-localhost","date":"2017-08-29T08:34:08.000Z","updated":"2017-08-29T08:42:14.000Z","comments":true,"path":"2017/08/29/hbase-rest-bindAddress-to-localhost/","link":"","permalink":"http://frankchen.xyz/2017/08/29/hbase-rest-bindAddress-to-localhost/","excerpt":"","text":"通过开启Hbase 的REST 服务我们可以很方便的以API的形式访问Hbase， 12345# Foreground$ bin/hbase rest start -p &lt;port&gt;# Background, logging to a file in $HBASE_LOGS_DIR$ bin/hbase-daemon.sh start rest -p &lt;port&gt; 但是其默认是绑定0.0.0.0地址的，也就是对外网开放，而通过REST 服务别有用心的人是可以删表的。。。如何只对内网开放呢？ 查了无数中英文网页不得，最后决定：看源码！最后在这里发现如下片段，hbase/RESTServer.java at master · apache/hbase那么解决方法就显而易见了： 1234567# sudo vim /usr/local/hbase/conf/hbase-site.xml&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hbase.rest.host&lt;/name&gt; &lt;value&gt;127.0.0.1&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;","categories":[],"tags":[{"name":"Hbase","slug":"Hbase","permalink":"http://frankchen.xyz/tags/Hbase/"}]},{"title":"scala-notes","slug":"scala-note","date":"2017-08-22T03:06:48.000Z","updated":"2017-08-22T03:06:48.000Z","comments":true,"path":"2017/08/22/scala-note/","link":"","permalink":"http://frankchen.xyz/2017/08/22/scala-note/","excerpt":"","text":"call by name 与 call by value的区别两者的区别就是调用之前需不需要evaluation，前者不需要，后者需要。例如一个函数$f(x, y) = x$，我们分别调用$f(1+1, 2 )$，call by name 直接引用1+1，再计算出为2，而 call by value是先算出函数参数的值，再去调用$f(2, 2 )$，scala默认是call by value，但是可以在需要call by name 的参数加箭头如=&gt;。 scala里面定义变量def和val的区别即在此，前者是call by name，例如我们分别定义两个函数： 12345def loop: Boolean = loopdef x = loopval y = loop 函数x可以被成功定义，而后者不行，因为在call by name 的参数evaluation的时候就进入死循环了。","categories":[],"tags":[{"name":"scala","slug":"scala","permalink":"http://frankchen.xyz/tags/scala/"}]},{"title":"详解Coursera 奖学金申请步骤","slug":"coursera-scholarship","date":"2017-08-21T04:06:29.000Z","updated":"2017-09-05T09:30:38.000Z","comments":true,"path":"2017/08/21/coursera-scholarship/","link":"","permalink":"http://frankchen.xyz/2017/08/21/coursera-scholarship/","excerpt":"最新Coursera 奖学金申请步骤！成功率100%🏆","text":"最新Coursera 奖学金申请步骤！成功率100%🏆 以这门Scala 函数式程序设计原理 | Coursera为例： 以下问题必须填写英文，必须大于等于150字，从我的申请记录来看，基本不会审核你究竟填写了什么，这只是为免费用户设置一个门槛，吸引你氪金😂😎 之前是立即可以获得证书，现在需要等待十五天即可。 关于上面的三个问题回复提供一模板： I am a graduate student in mainland China, was born in a peasant family, the family has four people, my father, my mother, my brother and I, we four and grandparents living in the same home. Mom and Dad did not work, can only rely on two acres of the family to be barely subsistence income is very meager, good for my brother and I can learn, the family live frugally money supply year my brother and I go to school Reading used. The family also owe a lot of money, so my brother and I grew to know two people not to spend money. Dad, Mom no cultural knowledge, they know the importance of knowledge, so small they are strict requirements of our brothers and both learn to be good in the future to test a good university, find a good job, do not like them, did not work in the countryside. We two brothers are also very competitive, it has been among the best in school, in our view, only with honors in order to make my parents happy, to return to their pains. Monthly income of around one hundred US dollars to pay tuition for this course certificate will be spent half of my cost of living, it will bring a lot of economic pressure to my normal life, and this course is the first door I finished on the site class, have a special meaning for me, I wanted to get this certificate course, this will inspire my passion for learning and motivation. 1, many people have recommended this course for the learning experience to enhance learning method above, I think learning is a lifelong, this course I will gain the knowledge of my lifetime. All the copyright. This door exercise logic and thinking about Coursera course is one of the most popular courses, course descriptions and practical reasoning methods and common logical fallacies, teach you how to properly reasoning, learning a few simple but critical general rules apply to all topics, while avoiding prone to problems when reasoning. 2, almost all individuals can enhance the quality of life of the curriculum to enhance areas of interest I have, I will continue to follow up. 3. Harvest course certificate for me is a recognition of my pay, my motivation for future learning enhance the effect is self-evident, thank you! 1, I guarantee independence to complete a full course, to ensure that all academic tasks independently myself by myself. 2, actively participate in discussions and course work, upload your own achievement, strive to contribute their efforts for curriculum community. 3, actively publicize the site to friends and relatives, for future expansion of community development programs and make a contribution. 4, to participate in and complete the course more sites to learn more new knowledge.","categories":[],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"http://frankchen.xyz/tags/Coursera/"}]},{"title":"server certificate verification failed solution","slug":"server-certificate-verification-failed-solution","date":"2017-08-15T10:23:03.000Z","updated":"2017-08-15T10:26:14.000Z","comments":true,"path":"2017/08/15/server-certificate-verification-failed-solution/","link":"","permalink":"http://frankchen.xyz/2017/08/15/server-certificate-verification-failed-solution/","excerpt":"","text":"server certificate verification failed. CAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none 的正确解决方法。 出现此错误时，问题出在证书的缺失，不可用如export GIT_SSL_NO_VERIFY=1方法去解除安全限制，正确方法是下载证书， 1234hostname=XXXport=443trust_cert_file_location=`curl-config --ca`sudo bash -c \"echo -n | openssl s_client -showcerts -connect $hostname:$port 2&gt;/dev/null | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt;&gt; $trust_cert_file_location\" 若不起作用，可用IP代替真实hostname。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"}]},{"title":"nginx 反向代理 REST API","slug":"nginx-reverse-proxy-for-rest-API","date":"2017-08-04T08:31:16.000Z","updated":"2017-08-09T07:57:13.000Z","comments":true,"path":"2017/08/04/nginx-reverse-proxy-for-rest-API/","link":"","permalink":"http://frankchen.xyz/2017/08/04/nginx-reverse-proxy-for-rest-API/","excerpt":"利用nginx 为REST API提供负载均衡。利用nginx的负载均衡可以极大提升API服务的稳定性，本文简述此过程配置方法。","text":"利用nginx 为REST API提供负载均衡。利用nginx的负载均衡可以极大提升API服务的稳定性，本文简述此过程配置方法。 sudo apt install nginx安装nginx，接下来找出nginx配置地址，使用代码nginx -V可打印出一系列配置信息，不同平台和发行版可能不同，我这边是--prefix=/usr/share/nginx，即为nginx根目录，--conf-path=/etc/nginx/nginx.conf即为配置目录。 vim /etc/nginx/nginx.conf注意其中的 123...61 include /etc/nginx/conf.d/*.conf;62 include /etc/nginx/sites-enabled/*; 在/etc/nginx/conf.d/新建service.conf， 123456789101112131415upstream tornadoes &#123; server 127.0.0.1:6001; server 127.0.0.1:6002; server 127.0.0.1:6003; server 127.0.0.1:6004;&#125;server &#123; listen 5000; ## Individual nginx logs access_log /var/log/nginx/web_proxy_access.log; error_log /var/log/nginx/web_proxy_error.log; location / &#123; proxy_pass http://tornadoes; ## 和upstream 名称组对应即可 &#125;&#125; 这里我们将本地的5000端口负载均衡到四个REST Tornado服务上。service nginx restart即可。","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"http://frankchen.xyz/tags/运维/"},{"name":"nginx","slug":"nginx","permalink":"http://frankchen.xyz/tags/nginx/"}]},{"title":"Personal Tips on Mac & Linux & Iphone","slug":"All_kinds_of_TIPS","date":"2017-07-18T02:43:17.000Z","updated":"2017-09-05T09:38:57.000Z","comments":true,"path":"2017/07/18/All_kinds_of_TIPS/","link":"","permalink":"http://frankchen.xyz/2017/07/18/All_kinds_of_TIPS/","excerpt":"随手记录自己在用Mac开发上随时发现的各种Tips。","text":"随手记录自己在用Mac开发上随时发现的各种Tips。 Linux别用Bash啦用zsh！🈲zsh的自动补全功能各种神奇，可以补齐路径，补齐命令，补齐参数等，再也不用RTFM了。按下连按Tap还有二级菜单。 kill命令不需ps aux | grep xxx，只需kill xxx然后tap即可，如，这里我需要kill jupyter notebook只需要kill python再tap，非常方便。 跳转时，只需..即可不需cd，而...等于../../。 输入d，将列出当前 session 访问过的所有目录，再按提示的数字即可进入相应目录。 查找：zsh 的历史记录跨 session，可以共享。历史记录支持受限查找。比如，输入git，再按向上箭头，会搜索用过的所有 git 命令。搭配oh-my-zsh更佳哦😎 工具篇MWeb 目前体验最好的markdown编辑器MWeb - 专业的Markdown写作、记笔记、静态博客生成软件 - MWeb，各种markdown编辑器都用过，什么Mou、Macdown等等，都是远不如这个MWeb的。个人最喜欢图片拖入功能，在此处设置好后 之间拖入图片，即可自动写入路径并实时预览（我用的hexo搭的博客）😘杀手级功能，效率神器！，除此之外其他的功能也是各种方便，这只需要￥98，还在等什么，赶快行动吧！ TransserraiOS App，可以离线下载Coursera课程，杀手功能：翻译字幕！！！，看公开课实在是太方便啦！不过需要注意，不要在还有下载任务的时候开启cellular（蜂窝网络）下使用，本人就是这样在月初被烧完了整个月的流量！！！👻， 翻译功能需要付费，不过不贵。 Iterm2iTerm2 - macOS Terminal Replacement是一款Mac上体验极佳的终端软件，拥有许多出色特性，如拖动字符串、多Tap广播输入等等按住⌘键: 可以拖拽选中的字符串； 点击 url：调用默认浏览器访问该网址； 点击文件：调用默认程序打开文件； 如果文件名是filename:42，且默认文本编辑器是 Macvim、Textmate或BBEdit，将会直接打开到这一行； 点击文件夹：在 finder 中打开该文件夹； 同时按住option键，可以以矩形选中，类似于vim中的ctrl v操作。 常用快捷键 切换 tab：⌘+←, ⌘+→, ⌘+{, ⌘+}。⌘+数字直接定位到该 tab； 新建 tab：⌘+t； 顺序切换 pane：⌘+[, ⌘+]； 按方向切换 pane：⌘+Option+方向键； 切分屏幕：⌘+d 水平切分，⌘+Shift+d 垂直切分； 智能查找，支持正则查找：⌘+f。 iTerm2 可以自动补齐命令，输入若干字符，按⌘+;弹出自动补齐窗口，列出曾经使用过的命令。iTerm2 也可以使用历史记录，按⌘+Shift+h弹出历史粘贴记录窗口， ⌘+Shift+;弹出历史命令记录窗口。⌘+Option+e全屏展示所有的 tab，可以搜索。一个标签页中开的窗口太多，有时候会找不到当前的鼠标，⌘+/找到它。 这里收集了大量 iTerm2 的主题，你可以选择使用。我用的是Zenburn。在其 github repo 里下载对应的xxx.itermcolors文件，双击安装使用。 更新： 取消鼠标滚轮浏览历史记录的设置：如何关闭iTerm2中的“scrolling the history” - 共享笔记 Mosh 替代SSH的Mosh: the mobile shell，极大降低ssh延迟，并且持续时间极佳。 快捷键表情符号按 Control-Command-空格键。此时会显示“字符显示程序”弹出窗口： Linux 终端快速清行有时候我们在终端里打出了一个很长很长的命令，这时候我们需要清空重新输入，有个快捷键就是ctr+p，即可快速清除当前输入。 Linux 切到上一个目录cd -即可： 截图 Command(⌘)-Shift-3 对整个屏幕拍摄屏幕快照 Command(⌘)-Shift-4 对屏幕的某个部分拍摄屏幕快照，可以选择截图区域 Command(⌘)-Shift-4-空格键 对某个窗口拍摄屏幕快照 Command(⌘)-Ctrl-Shift-4 对选定区域进行截屏，屏幕截图，文件保存在剪贴板","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"},{"name":"Mac","slug":"Mac","permalink":"http://frankchen.xyz/tags/Mac/"}]},{"title":"Hive install on Ubuntu，并以Mysql作为Metastore","slug":"Hive-install-on-Ubuntu","date":"2017-07-07T02:43:17.000Z","updated":"2017-07-07T03:03:35.000Z","comments":true,"path":"2017/07/07/Hive-install-on-Ubuntu/","link":"","permalink":"http://frankchen.xyz/2017/07/07/Hive-install-on-Ubuntu/","excerpt":"安装配置Hive虽然比较简单，但是网上的资料各种坑，总结下来写下本文作为成功后的记录。","text":"安装配置Hive虽然比较简单，但是网上的资料各种坑，总结下来写下本文作为成功后的记录。 下载与放置HiveHive下载地址 12345$ sudo tar xzvf apache-hive-2.1.1-bin.tar.gz -C /usr/local$ cd /usr/local$ sudo mv apache-hive-2.1.1-bin/ hive$ sudo chown hadoop@hadoop -R hive$ mysql$ sudo apt-get install mysql-server mysql Java连接$ sudo apt-get install libmysql-java并创建软连接$ ln -s /usr/share/java/mysql-connector-java.jar $HIVE_HOME/lib/mysql-connector-java.jar 环境变量在~/.profile里添加12export HIVE_HOME=/usr/local/hiveexport PATH=$PATH:$HIVE_HOME/bin 并source ~/.profile 建表与连接初始表格在mysql里创建数据库，格式同hive-schema-2.1.0.mysql.sql ，这里依据你的版本号来 12345$ mysql -u root -pEnter password:mysql&gt; CREATE DATABASE metastore;mysql&gt; USE metastore;mysql&gt; SOURCE /usr/local/hive/scripts/metastore/upgrade/mysql/hive-schema-2.1.0.mysql.sql;; 创建用户并给予权限123mysql&gt; CREATE USER 'hiveuser'@'%' IDENTIFIED BY 'hivepassword'; mysql&gt; GRANT all on *.* to 'hiveuser'@localhost identified by 'hivepassword';mysql&gt; flush privileges; hive-site.xml在$HIVE_HOME/conf文件夹创建hive-site.xml文件，配置如下 12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;metadata is stored in a MySQL server&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;MySQL JDBC driver class&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hiveuser&lt;/value&gt; &lt;description&gt;user name for connecting to mysql server&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hivepassword&lt;/value&gt; &lt;description&gt;password for connecting to mysql server&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; OK，启动Hive Shell试试看吧测试一下，在hive shell里建表hive&gt; create table saurzcode(id int, name string); 再在mysql里查看 12345mysql -u root -pEnter password: mysql&gt; use metastore;mysql&gt; show tables ;mysql&gt; select * from TBLS; 若可以看见上面在hive里建的表saurzcode，恭喜你大功告成！😆😎🤠","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"},{"name":"Hive","slug":"Hive","permalink":"http://frankchen.xyz/tags/Hive/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://frankchen.xyz/tags/Hadoop/"}]},{"title":"ssh 建立本地localhost与远程服务器localhost的连接","slug":"ssh-web-tunnel","date":"2017-07-06T06:23:29.000Z","updated":"2017-07-06T07:06:27.000Z","comments":true,"path":"2017/07/06/ssh-web-tunnel/","link":"","permalink":"http://frankchen.xyz/2017/07/06/ssh-web-tunnel/","excerpt":"","text":"远程连接服务器开发时，经常需要连接到服务器的localhost的web界面查看，如深度学习服务器的jupyter notebook、服务器上的hadoop的hdfs和yarn以及spark的web界面，还有Supervisor等等，很多时候服务器都没有图形界面，很多时候用服务器IP在本地查看又因为防火墙的原因被阻挡，那么这里有一个很简单的方法即可达到目的：即从本地建立一个ssh通道，如 1ssh username@address_of_remote -L 127.0.0.1:1234:127.0.0.1:8888 即可在本地的1234端口访问远程服务器的127.0.0.1:8888地址了！","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"}]},{"title":"使用supervisor支持Python3程序","slug":"Use-supervisor-support-Python3-program","date":"2017-07-06T03:23:07.000Z","updated":"2017-08-03T07:54:40.000Z","comments":true,"path":"2017/07/06/Use-supervisor-support-Python3-program/","link":"","permalink":"http://frankchen.xyz/2017/07/06/Use-supervisor-support-Python3-program/","excerpt":"Supervisor是python2写就的一款强大的运维工具，众所周知，目前Supervisor还不支持python3，那么怎么利用Supervisor监控python3程序呢？本文主要讲述Supervisor在Ubuntu下的安装部署以及上述问题的解决。","text":"Supervisor是python2写就的一款强大的运维工具，众所周知，目前Supervisor还不支持python3，那么怎么利用Supervisor监控python3程序呢？本文主要讲述Supervisor在Ubuntu下的安装部署以及上述问题的解决。 安装及设置可通过pip安装，如果你已经是python3的pip，会安装失败，那么可以用sudo apt-get install supervisor来安装，默认由Ubuntu自带的/usr/bin/python2.7驱动。 运行echo_supervisord_conf &gt; /etc/supervisor/supervisord.conf来产生设置，未避免产生非root用户的权限错误，将/etc/supervisor/supervisord.conf内[unix_http_server]这项改为（;即是注释）： 123456[unix_http_server]file=/tmp/supervisor.sock ; (the path to the socket file)chmod=0766 ; socket file mode (default 0700);chown=nobody:nogroup ; socket file uid:gid owner;username=user ; (default is no username (open server));password=123 ; (default is no password (open server)) 再将末尾的[include]部分改为：123[include]files = /etc/supervisor/*.conffiles = /etc/supervisor/conf.d/*.conf 这样方便为每个app单独设置conf文件而不必全部写在全局设置里面。 在启动supervisorctl须先启动supervisord，否则会出现error: &lt;class &#39;socket.error&#39;&gt;, [Errno 99] Cannot assign requested address: file: /usr/lib/python2.7/socket.py line: 575错误： 12sudo supervisord -c /etc/supervisor/supervisord.confsudo supervisorctl -c /etc/supervisor/supervisord.conf 在/etc/supervisor/conf.d/里新建app.conf文件，12345678910111213[program:app]directory = ~/su/ ; 程序的启动目录command = /home/hadoop/anaconda3/bin/python /home/hadoop/su/app.py ; 启动命令，可以看出与手动在命令行启动的命令是一样的，注意这里home不可用~代替autostart = true ; 在 supervisord 启动的时候也自动启动startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了autorestart = true ; 程序异常退出后自动重启startretries = 3 ; 启动失败自动重试次数，默认是 3user = hadoop ; 用哪个用户启动redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 falsestdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MBstdout_logfile_backups = 20 ; stdout 日志文件备份数; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件）stdout_logfile = /tmp/app.log 再介绍两个有用的配置项stopasgroup和killasgroup，如果我们用Flask等Rest服务，通常其会开启几个进程，那么如果stopasgroup不启用的话，supervisor无法重启此服务（关闭主进程时其子进程没有关闭，再开启主进程时会提示端口被占用等错误信息）。 1234; 默认为 false，如果设置为 true，当进程收到 stop 信号时，会自动将该信号发给该进程的子进程。如果这个配置项为 true，那么也隐含 killasgroup 为 true。例如在 Debug 模式使用 Flask 时，Flask 不会将接收到的 stop 信号也传递给它的子进程，因此就需要设置这个配置项。stopasgroup=false ; send stop signal to the UNIX process ; 默认为 false，如果设置为 true，当进程收到 kill 信号时，会自动将该信号发给该进程的子进程。如果这个程序使用了 python 的 multiprocessing 时，就能自动停止它的子线程。killasgroup=false ; SIGKILL the UNIX process group (def false) 这里我们可以看出，虽然supervisor是python2写的，但只要我们指定运行的python3解释器去运行程序就行了。 运行supervisorctl，即可在shell里面方便的操作，如start app、restart app等。 若需要web界面，可在/etc/supervisor/supervisord.conf内修改， 1234[inet_http_server] ; inet (TCP) server disabled by defaultport=127.0.0.1:9001 ; (ip_address:port specifier, *:port for all iface, 若的形式*:port则开放外网访问 );username=user ; (default is no username (open server));password=123 ; (default is no password (open server)) 重启supervisorctl后即可在127.0.0.1:9001见到web界面， 注意事项 如果修改了 /etc/supervisord.conf ,需要执行 supervisorctl reload 来重新加载配置文件，否则不会生效。。。 很多时候用supervisor管理后台进程容易失败，如hbase/bin/hbase-daemon.sh start thrift，这时候可以改用前台进程如/usr/local/hbase/bin/hbase thrift start。","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"运维","slug":"运维","permalink":"http://frankchen.xyz/tags/运维/"}]},{"title":"Ubuntu16.04 固定IP与设置DNS","slug":"Ubuntu-16-04-static-IP","date":"2017-06-20T07:13:31.000Z","updated":"2018-02-28T04:22:49.320Z","comments":true,"path":"2017/06/20/Ubuntu-16-04-static-IP/","link":"","permalink":"http://frankchen.xyz/2017/06/20/Ubuntu-16-04-static-IP/","excerpt":"设置Hadoop集群的第一步很可能就是设置固定IP于DNS，而网上这一做法由于Ubuntu版本及桌面版服务器版的不同导致残差不齐，本文记录一下Ubuntu16.04在非图形界面固定IP与设置DNS的过程。","text":"设置Hadoop集群的第一步很可能就是设置固定IP于DNS，而网上这一做法由于Ubuntu版本及桌面版服务器版的不同导致残差不齐，本文记录一下Ubuntu16.04在非图形界面固定IP与设置DNS的过程。 Ubuntu16.04 固定IPStep-One【Ubuntu-server不需，跳过即可】123sudo vim /etc/NetworkManager/NetworkManager.conf# 将`managed=false`修改成`managed=true`sudo reboot Step-Two如下1234567891011121314151617# 修改配置文件sudo vim /etc/network/interfaces``` 改为``` bash# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackauto enp0s31f6iface enp0s31f6 inet staticaddress 192.168.1.109netmask 255.255.255.0#network 192.168.1.0#broadcast 192.168.1.255gateway 192.168.1.254 其中‘enp0s31f6’就是ifconfig中以太网名称。 附Linux下查看网关gateway方法： route -n ip route show traceroute www.baidu.com -s 100 【第一行就是自己的网关】 netstat -r more /etc/network/interfaces 【Debian/Ubuntu Linux】 more /etc/sysconfig/network-scripts/ifcfg-eth0 【Red Hat Linux】 如不清楚网关和子网掩码等参数，可在Ubuntu设置里将network里的ipv4先设置为DHCP（自动获取网络），再利用ifconfig和上述方法查看参数后更改固定参数。 Step-Three12# 重启networking服务sudo systemctl restart networking.service Ubuntu16.04 设置DNS但是固定IP使得电脑很可能无法上网，那么需要手动设置DNS。 12# 默认文件不存在sudo vim /etc/resolvconf/resolv.conf.d/base 添加下面内容： 12345meserver 8.8.8.8nameserver 8.8.4.4nameserver 192.168.1.254nameserver 114.114.114.114 其中114.114.114.114是国内移动、电信和联通通用的DNS，8.8.8.8和8.8.4.4是GOOGLE公司提供的DNS，192.168.1.254是网关地址。 参考自 ubuntu16.04固定IP与设置DNS - 简书","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"}]},{"title":"Learning Something from Fluent Python","slug":"Learning-Something-from-Fluent-Python","date":"2017-06-18T11:42:32.000Z","updated":"2017-06-18T12:01:59.000Z","comments":true,"path":"2017/06/18/Learning-Something-from-Fluent-Python/","link":"","permalink":"http://frankchen.xyz/2017/06/18/Learning-Something-from-Fluent-Python/","excerpt":"“Fluent Python” 真是一本不可多得的进阶Python神书！读时感觉就像一个老司机讲他的经历，而你情不自禁的喊“666~”。本文记录一些从书里看到的精彩片段与心得。","text":"“Fluent Python” 真是一本不可多得的进阶Python神书！读时感觉就像一个老司机讲他的经历，而你情不自禁的喊“666~”。本文记录一些从书里看到的精彩片段与心得。 singledispatch装饰器functools.singledispatch 装饰器是在Python 3.4中引入的新特性，类似java 的方法重载，可以让你方便的为不同的类型参数调用不同函数。书中举的一个场景是比如，我们需要为不同的参数生成不同格式的HTML tag，那么如果用一个函数来表达这个逻辑，可能需要非常多的if-else逻辑的跳转，这不仅很繁杂，而且不利于后期的维护与迭代，那么singledispatch 装饰器就可以让你从这个局面里解放出来，如： 123456789101112131415161718192021222324252627282930# -*- coding:utf8 -*-# Created by frank at 18/06/2017from functools import singledispatch@singledispatchdef myprint(): pass@myprint.register(str)def _(text): print(\"FUCKING STRING! &#123;0&#125;\".format(text))@myprint.register(int)def _(n): print(\"FUCKING INT! &#123;0&#125;\".format(n))@myprint.register(float)def _(f): print(\"FUCKING FLOAT! &#123;0&#125;\".format(f))@myprint.register(tuple)def _(seq): print(\"FUCKING TUPLE!\" + str([item for item in seq]))if __name__ == '__main__': myprint(1) myprint(0.5) myprint('frank') myprint((1, 2, 3)) 结果是1234FUCK INT! 1FUCK FLOAT! 0.5FUCK STRING! frankFUCK tuple![1, 2, 3] 以上的简单的例子中，singledispatch装饰器为myprint函数的不同参数选择了不同的调用。","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"}]},{"title":"programmer humor week 4","slug":"programmer-humor-week-4","date":"2017-06-18T11:28:36.000Z","updated":"2017-06-19T11:50:07.000Z","comments":true,"path":"2017/06/18/programmer-humor-week-4/","link":"","permalink":"http://frankchen.xyz/2017/06/18/programmer-humor-week-4/","excerpt":"programmer humor from reddit 🤣😂😜","text":"programmer humor from reddit 🤣😂😜 Passwords Haters will say he/she doesn’t even code Just a bit. Happy Birthday Linux! How to get tomorrow’s date 引用自Programmer Humor","categories":[],"tags":[{"name":"Funny","slug":"Funny","permalink":"http://frankchen.xyz/tags/Funny/"}]},{"title":"Python PATH是怎么一回事？从“import pyspark”报错说起","slug":"About-Python-PATH","date":"2017-06-14T18:57:57.000Z","updated":"2017-06-17T06:11:39.000Z","comments":true,"path":"2017/06/15/About-Python-PATH/","link":"","permalink":"http://frankchen.xyz/2017/06/15/About-Python-PATH/","excerpt":"为何import pyspark报错？让我们从捋一捋Python的环境变量PATH开始！","text":"为何import pyspark报错？让我们从捋一捋Python的环境变量PATH开始！ Spark装好后直接在代码开头import pyspark不出意外是要报错的，没有这个模块？当然不是，只是python找不到这个地址而已，有很quick and dirty的解决方法，就是利用包findspark，只要在开头12import findsparkfindspark.init() 接下来即可顺利import pyspark，不过这个在“import其他依赖之前先运行函数”实在是有碍观赏性。。。什么是“优雅的解决方法”呢？别急，我们先来看看import findspark做了什么吧： 12345678910111213141516# ensure SPARK_HOME is definedos.environ['SPARK_HOME'] = spark_home# ensure PYSPARK_PYTHON is definedos.environ['PYSPARK_PYTHON'] = python_path# add pyspark to sys.pathspark_python = os.path.join(spark_home, 'python')py4j = glob(os.path.join(spark_python, 'lib', 'py4j-*.zip'))[0]sys.path[:0] = [spark_python, py4j]if edit_rc: change_rc(spark_home, spark_python, py4j) if edit_profile: edit_ipython_profile(spark_home, spark_python, py4j) debug一下，原来init()的作用即是将如’/usr/local/spark/python’和’/usr/local/spark/python/lib/py4j-0.9-src.zip’写入 sys.path内，其中可选是否写入’~/.bashrc’或者IPython profile里。再通过查阅一些资料，我们开始验证想法，比如，我们在桌面建立’fuck.py’文件： echo &quot;print(&#39;FUCK&#39;)&quot; &gt; fuck.py那么，只要我们import fuck 成功即打印此文字，我们在Desktop下： 1234frank@mac:Desktop$ python -c \"import fuck\"FUCKfrank@mac:Desktop$ python -c \"import sys; print(sys.path)\"['', '/Users/frank/Desktop', '/Users/frank/anaconda/lib/python36.zip', '/Users/frank/anaconda/lib/python3.6', '/Users/frank/anaconda/lib/python3.6/lib-dynload', '/Users/frank/anaconda/lib/python3.6/site-packages', '/Users/frank/anaconda/lib/python3.6/site-packages/Sphinx-1.5.1-py3.6.egg', '/Users/frank/anaconda/lib/python3.6/site-packages/aeosa', '/Users/frank/anaconda/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg'] 没问题，因为sys.path虽不包括当前目录，但是buildin函数import默认寻找当前目录。 那我们切到其他文件夹？ 12345678frank@mac:tmp$ pythonfrank@mac:tmp$ python -c \"import fuck\"Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt;ModuleNotFoundError: No module named 'fuck'frank@mac:tmp$ python -c \"import sys; print(sys.path)\"['', '/Users/frank/anaconda/lib/python36.zip', '/Users/frank/anaconda/lib/python3.6', '/Users/frank/anaconda/lib/python3.6/lib-dynload', '/Users/frank/anaconda/lib/python3.6/site-packages', '/Users/frank/anaconda/lib/python3.6/site-packages/Sphinx-1.5.1-py3.6.egg', '/Users/frank/anaconda/lib/python3.6/site-packages/aeosa', '/Users/frank/anaconda/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg'] fuck.py既不在当前目录也不在sys.path，那么如何添加呢？有两种方法：一是添加进sys.path里： 123&gt;&gt;&gt; sys.path.append('/Users/frank/Desktop')&gt;&gt;&gt; import fuckFUCK 不过这个只对当前程序起作用，想要持久效果的话，通过查阅29.1. sys — System-specific parameters and functions — Python 3.6.1 documentation可得，sys.path是通过PYTHONPATH环境变量起作用的，那么把需要的目录添加到PYTHONPATH即可： 1frank@mac:~$ echo \"export PYTHONPATH=/Users/frank/Desktop:$PYTHONPATH\" &gt;&gt; ~/.profile &amp;&amp; source ~/.profile 再import即可成功： 12frank@mac:tmp$ python -c \"import fuck\"FUCK 综上，需要一劳永逸解决import pyspark失败的问题，那么在`~/.profile’内添加如下即可（py4j-0.10.4-部分版本号可能不同））： 12export SPARK_HOME=/usr/local/sparkexport PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH 在source ~/.profile即可。","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"}]},{"title":"programmer humor week 3","slug":"programmer-humor-week-3","date":"2017-05-27T09:58:39.000Z","updated":"2017-06-07T08:45:31.000Z","comments":true,"path":"2017/05/27/programmer-humor-week-3/","link":"","permalink":"http://frankchen.xyz/2017/05/27/programmer-humor-week-3/","excerpt":"programmer humor from reddit 🤣😂😜","text":"programmer humor from reddit 🤣😂😜 “what sales does” Harry Potter can code Python The neverending story. relationship Functional Programming For Beginners 引用自Programmer Humor","categories":[],"tags":[{"name":"Funny","slug":"Funny","permalink":"http://frankchen.xyz/tags/Funny/"}]},{"title":"Ubuntu 16.04 搭建 Spark & Hadoop集群的详细步骤及报错填坑","slug":"Ubuntu-16-04-install-Spark-Hadoop","date":"2017-05-27T08:13:04.000Z","updated":"2017-08-02T06:06:53.000Z","comments":true,"path":"2017/05/27/Ubuntu-16-04-install-Spark-Hadoop/","link":"","permalink":"http://frankchen.xyz/2017/05/27/Ubuntu-16-04-install-Spark-Hadoop/","excerpt":"在Ubuntu 16.04 安装 Spark &amp; Hadoop分布式集群的记录","text":"在Ubuntu 16.04 安装 Spark &amp; Hadoop分布式集群的记录 环境配置如下设置默认都在每一台机器上都要进行。 配置hadoop用户首先是在每个机器上创建hadoop用户，设置密码（方便起见建议都设置一样的），并赋予其root权限 123sudo useradd -m hadoop -s /bin/bashsudo passwd hadoopsudo adduser hadoop sudo 固定IP为防止重启导致IP变化，需要固定Ip方法参考Ubuntu16.04 固定IP与设置DNS | 不正经数据科学家 配置hosts为方便部署，可配置hosts名，方便输入地址，如下 12345vim /etc/hosts192.168.1.113 czn.shise.com192.168.1.102 czm.shise.com192.168.1.120 wwj.shise.com192.168.1.123 bas.shise.com 测试能否ping通ping czn.shise.com -c 3 配置 ssh 无密码访问集群机器每台机器之间以及每台机器与自己的localhost都需配置ssh免密码登录。如未安装ssh需 sudo apt install openssh-server 1234ssh localhostexit # 退出刚才的 ssh localhostcd ~/.ssh/ # 若没有该目录，请先执行一次ssh localhostssh-keygen -t rsa # 会有提示，都按回车就可以 再用ssh-copy-id来设置免密码登录，如 1234ssh-copy-id localhostssh-copy-id czn.shise.comssh-copy-id czm.shise.comssh-copy-id bas.shise.com 在每个机器上都如此设置与其它机器的免密码登录。这里默认都是hadoop用户，所以不需带用户名，如果不是则需要带用户名如ssh-copy-id hadoop@czn.shise.com Java环境统一使用openjdk-8，sudo apt-get install openjdk-8-jre openjdk-8-jdk会默认安装在/usr/lib/jvm/java-8-openjdk-amd64 Hadoop集群配置我们先登录至作为master的机器，配置好之后再将hadoop环境复制至各slave即可，非常方便。首先是Hadoop集群的配置，下载Index of /apache/hadoop/common，我们选择2.7.3版本。放置1234sudo tar -zxf ~/Downloads/hadoop-2.7.3.tar.gz -C /usr/local # 解压到/usr/local中cd /usr/local/sudo mv ./hadoop-2.7.3/ ./hadoop # 将文件夹名改为hadoopsudo chown -R hadoop:hadoop ./hadoop # 修改文件权限 各文件配置配置hadoop/etc/hadoop文件夹下的各文件， 在hadoop-env.sh里添加12export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export HADOOP_PREFIX=/usr/local/hadoop 在core-site.xml里添加 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://wwj.shise.com:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 在yarn-site.xml里添加 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;wwj.shise.com&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 改写slaves为 1234czn.shise.comczm.shise.comwwj.shise.combas.shise.com 改写mapred-site.xml文件 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;wwj.shise.com:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;wwj.shise.com:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml文件 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;wwj.shise.com:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 复制配置好的hadoop至各机器在master上压缩文件12345678cd /usr/localsudo rm -r ./hadoop/tmp # 删除 Hadoop 临时文件sudo rm -r ./hadoop/logs/* # 删除日志文件tar -zcf ~/hadoop.master.tar.gz ./hadoop # 先压缩再复制cd ~scp ./hadoop.master.tar.gz czn.shise.com:/home/hadoop # 复制至各机器scp ./hadoop.master.tar.gz czm.shise.com:/home/hadoopscp ./hadoop.master.tar.gz bas.shise.com:/home/hadoop 再登录至复制过去的每个slave123sudo rm -r /usr/local/hadoop # 删掉旧的（如果存在）sudo tar -zxf ~/hadoop.master.tar.gz -C /usr/localsudo chown -R hadoop /usr/local/hadoop 各机器环境变量sudo vim ~/.profile我的环境变量设置如下123456789# Java Envexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin# Hadoop Envexport HADOOP_HOME=/usr/local/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source ~/.profile 启动检查登录master机器，执行 NameNode 的格式化：hdfs namenode -format先dfs、再yarn123start-dfs.shstart-yarn.shmr-jobhistory-daemon.sh start historyserver 验证hadoop安装可以通过jps命令查看各个节点启动的进程是否正常。在 master 上应该有以下几个进程： 12345$ jps #run on master3407 SecondaryNameNode3218 NameNode3552 ResourceManager3910 Jps 在每个slave上应该有以下几个进程：1234$ jps #run on slaves2072 NodeManager2213 Jps1962 DataNode Spark分布式配置下载及安置官网下载，Downloads | Apache Spark ，这里我们选择2.2.1版本， 及Pre-build with user-provided Apache Hadoop， 1234sudo tar -zxf ~/Downloads/spark-2.1.1-bin-without-hadoop.tgz -C /usr/local/cd /usr/localsudo mv ./spark-2.1.1-bin-without-hadoop/ ./sparksudo chown -R hadoop:hadoop ./spark # 此处的 hadoop 为你的用户名 配置Spark123cd /usr/local/spark/conf #进入spark配置目录cp spark-env.sh.template spark-env.sh #从配置模板复制vim spark-env.sh #添加配置内容 在spark-env.sh末尾添加以下内容（这是我的配置，你可以自行修改）： 1234567#export SCALA_HOME=/home/spark/workspace/scala-2.10.4export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export HADOOP_HOME=/usr/local/hadoopexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopSPARK_MASTER_IP=wwj.shise.comSPARK_LOCAL_DIRS=/usr/local/sparkSPARK_DRIVER_MEMORY=1G 参考Using Spark’s “Hadoop Free” Build - Spark 2.2.0 Documentation，spark-env.sh还需加上export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)。 vim slaves在slaves文件下填上slave主机名： 1234czn.shise.comczm.shise.comwwj.shise.combas.shise.com 复制配置好的Spark至各机器12345tar -zcf ~/spark.master.tar.gz ./spark # 先压缩再复制cd ~/scp ./spark.master.tar.gz czn.shise.com:/home/hadoop #传输数据scp ./spark.master.tar.gz czm.shise.com:/home/hadoop #传输数据scp ./spark.master.tar.gz bas.shise.com:/home/hadoop #传输数据 启动Sparksbin/start-all.sh用jps检查，在 master 上应该有以下几个进程： 123456$ jps7949 Jps7328 SecondaryNameNode7805 Master7137 NameNode7475 ResourceManager 在 slave 上应该有以下几个进程： 12345$jps3132 DataNode3759 Worker3858 Jps3231 NodeManager 报错及解决 Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.出现这个错误的原因是为worker与driver设置了不同的Python版本，解决方法是（以anaconda3.6为例）： vim /usr/local/spark/conf/spark-env.sh，添加 123export ANACONDA_ROOT=/home/hadoop/anaconda3export PYSPARK_DRIVER_PYTHON=$ANACONDA_ROOT/bin/pythonexport PYSPARK_PYTHON=$ANACONDA_ROOT/bin/pythonexport PYSPARK_PYTHON=$ANACONDA_ROOT/bin/python 值得注意的是，很多教程会让你在.bashrc里设置，这不是推荐的方法。参考自pyspark - How do I set the driver’s python version in spark? - Stack Overflow。 参考自 Spark On YARN 集群安装部署 | Jark’s Blog Hadoop集群安装配置教程_Hadoop2.6.0_Ubuntu/CentOS_厦大数据库实验室博客 Hadoop集群完全分布式搭建教程-CentOS - 在路上 - 博客频道 - CSDN.NET Hadoop 2.6.4分布式集群环境搭建 - JackieYeah的个人空间 Spark On YARN 集群安装部署 - 简书","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://frankchen.xyz/tags/Hadoop/"},{"name":"Spark","slug":"Spark","permalink":"http://frankchen.xyz/tags/Spark/"}]},{"title":"解决Excel长数字被转为科学计数法的问题","slug":"examples-for-processing-excel-and-time-series","date":"2017-05-12T03:43:34.000Z","updated":"2017-05-12T05:34:50.000Z","comments":true,"path":"2017/05/12/examples-for-processing-excel-and-time-series/","link":"","permalink":"http://frankchen.xyz/2017/05/12/examples-for-processing-excel-and-time-series/","excerpt":"记录一些利用Pandas处理Excel和时间戳的例子。","text":"记录一些利用Pandas处理Excel和时间戳的例子。 读取和保存Excel1234# sheetname指定读取子表dfs = pd.read_excel('/Users/frank/Desktop/hebing.xlsx', sheetname=0)c.to_excel('/Users/frank/Desktop/result.xlsx',index=False) DataFrame合并及排序12dfs_group = dfs.groupby(by='订单号').sum()dfs_group.sort_values(ascending=False, by='子订单金额') 时间段切片这个我暂时没发现如何用一行表达式完成，暂时用的这种low方式🤣12a = dfs.loc[dfs[\"付款时间\"] &lt; '2017-05-10 09:00:00']b = a.loc[a[\"付款时间\"] &gt; '2017-05-08 10:00:00'] 解决长数字被转为科学计数法的问题比如好好的订单号， 保存为xlsx或者csv就变成了这样，这是什么玩意儿，完全不能忍啊！🤡🤓🤑 如何解决呢？如下 1c['订单号'] = c['订单号'].apply(lambda x: '&#123;:.0f&#125;'.format(x)) 即将其长数字转为不带小数点的浮点数形式即可😎","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://frankchen.xyz/tags/Data-Science/"},{"name":"Pandas","slug":"Pandas","permalink":"http://frankchen.xyz/tags/Pandas/"}]},{"title":"面试常见基础算法:排序/查找/树的遍历等（python版）","slug":"Basic-Algorithms-in-Python","date":"2017-05-04T06:08:10.000Z","updated":"2017-12-15T04:52:33.616Z","comments":true,"path":"2017/05/04/Basic-Algorithms-in-Python/","link":"","permalink":"http://frankchen.xyz/2017/05/04/Basic-Algorithms-in-Python/","excerpt":"总结一下基本算法，以备忘，以复习。当然，你别指望考官会直接考你这些，太基础，但是，当他知道你这些都写不出来的时候，你面试绝对没希望了。 PS: 真的有面试官会考啊，至少Baidu会考…………","text":"总结一下基本算法，以备忘，以复习。当然，你别指望考官会直接考你这些，太基础，但是，当他知道你这些都写不出来的时候，你面试绝对没希望了。 PS: 真的有面试官会考啊，至少Baidu会考………… 8大排序冒泡排序 稳定排序，简单易于实现，复杂度$Ο(n ^2)$1234567def bubble_sort(arr): length = len(arr) for i in range(length): for j in range(i, length): if arr[i] &gt; arr[j]: arr[i], arr[j] = arr[j], arr[i] return arr 选择排序 不稳定排序，复杂度$Ο(n ^2)$123456789def select_sort(arr): length = len(arr) for i in range(length): _min = i for j in range(i+1, length): if arr[_min] &gt; arr[j]: _min = j arr[i], arr[_min] = arr[_min], arr[i] return arr 插入排序123456789101112def insert_sort(alist): \"\"\"插入排序\"\"\" n = len(alist) for j in range(1, n): # 控制将拿到的元素放到前面有序序列中正确位置的过程 for i in range(j, 0, -1): # 如果比前面的元素小，则往前移动 if alist[i] &lt; alist[i - 1]: alist[i], alist[i - 1] = alist[i - 1], alist[i] # 否则代表比前面的所有元素都小，不需要再移动 else: break 希尔排序1234567891011121314def shell_sort(alist): \"\"\"希尔排序\"\"\" n = len(alist) gap = n // 2 while gap &gt;= 1: for j in range(gap, n): i = j while (i - gap) &gt;= 0: if alist[i] &lt; alist[i - gap]: alist[i], alist[i - gap] = alist[i - gap], alist[i] i -= gap else: break gap //= 2 归并排序稳定排序，复杂度 $Ο(n log n)$ ，该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并。 1234567891011121314151617def msort2(x): if len(x) &lt; 2: return x result = [] mid = int(len(x) / 2) y = msort2(x[:mid]) z = msort2(x[mid:]) while (len(y) &gt; 0) and (len(z) &gt; 0): if y[0] &gt; z[0]: result.append(z[0]) z.pop(0) else: result.append(y[0]) y.pop(0) result += y result += z return result 快速排序 QuickSort为不稳定排序，快速排序通常明显比同为 $Ο(n log n)$ 的其他算法更快，因此常被采用，而且快排采用了分治法的思想，所以在很多笔试面试中能经常看到快排的影子。可见掌握快排的重要性. 123456789def quicksort(arr): if len(arr) &lt;= 1: return arr pivot = arr[len(arr) // 2] left = [i for i in arr if i&lt;pivot] middle = [i for i in arr if i==pivot] right = [i for i in arr if i&gt;pivot] return quicksort(left) + middle + quicksort(right) 堆排序 HeapSort1234567891011121314151617181920212223242526import randomdef MAX_Heapify(heap,HeapSize,root):#在堆中做结构调整使得父节点的值大于子节点 left = 2*root + 1 right = left + 1 larger = root if left &lt; HeapSize and heap[larger] &lt; heap[left]: larger = left if right &lt; HeapSize and heap[larger] &lt; heap[right]: larger = right if larger != root:#如果做了堆调整则larger的值等于左节点或者右节点的，这个时候做对调值操作 heap[larger],heap[root] = heap[root],heap[larger] MAX_Heapify(heap, HeapSize, larger)def Build_MAX_Heap(heap):#构造一个堆，将堆中所有数据重新排序 HeapSize = len(heap)#将堆的长度当独拿出来方便 for i in xrange((HeapSize -2)//2,-1,-1):#从后往前出数 MAX_Heapify(heap,HeapSize,i)def HeapSort(heap):#将根节点取出与最后一位做对调，对前面len-1个节点继续进行对调整过程。 Build_MAX_Heap(heap) for i in range(len(heap)-1,-1,-1): heap[0],heap[i] = heap[i],heap[0] MAX_Heapify(heap, i, 0) return heap 基数排序123456789import randomdef radixSort(): A=[random.randint(1,9999) for i in xrange(10000)] for k in xrange(4): #4轮排序 s=[[] for i in xrange(10)] for i in A: s[i/(10**k)%10].append(i) A=[a for b in s for a in b] return A 查找：二分查找只适用于有序数组，复杂度$Ο(log n)$1234567891011def binary_search(arr, key): low = 0 high = len(arr) - 1 while low &lt;= high: mid = (low + high) // 2 if arr[mid] == key: return mid elif arr[mid] &lt; key: low = mid + 1 elif arr[mid] &gt; key: high = mid - 1 return -1 树的遍历123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146class Node(object): \"\"\"节点类\"\"\" def __init__(self, elem=-1, lchild=None, rchild=None): self.elem = elem self.lchild = lchild self.rchild = rchildclass Tree(object): \"\"\"树类\"\"\" def __init__(self): self.root = Node() self.myQueue = [] def add(self, elem): \"\"\"为树添加节点\"\"\" node = Node(elem) if self.root.elem == -1: # 如果树是空的，则对根节点赋值 self.root = node self.myQueue.append(self.root) else: treeNode = self.myQueue[0] # 此结点的子树还没有齐。 if treeNode.lchild == None: treeNode.lchild = node self.myQueue.append(treeNode.lchild) else: treeNode.rchild = node self.myQueue.append(treeNode.rchild) self.myQueue.pop(0) # 如果该结点存在右子树，将此结点丢弃。 def front_digui(self, root): \"\"\"利用递归实现树的先序遍历\"\"\" if root == None: return print root.elem, self.front_digui(root.lchild) self.front_digui(root.rchild) def middle_digui(self, root): \"\"\"利用递归实现树的中序遍历\"\"\" if root == None: return self.middle_digui(root.lchild) print root.elem, self.middle_digui(root.rchild) def later_digui(self, root): \"\"\"利用递归实现树的后序遍历\"\"\" if root == None: return self.later_digui(root.lchild) self.later_digui(root.rchild) print root.elem, def front_stack(self, root): \"\"\"利用堆栈实现树的先序遍历\"\"\" if root == None: return myStack = [] node = root while node or myStack: while node: #从根节点开始，一直找它的左子树 print node.elem, myStack.append(node) node = node.lchild node = myStack.pop() #while结束表示当前节点node为空，即前一个节点没有左子树了 node = node.rchild #开始查看它的右子树 def middle_stack(self, root): \"\"\"利用堆栈实现树的中序遍历\"\"\" if root == None: return myStack = [] node = root while node or myStack: while node: #从根节点开始，一直找它的左子树 myStack.append(node) node = node.lchild node = myStack.pop() #while结束表示当前节点node为空，即前一个节点没有左子树了 print node.elem, node = node.rchild #开始查看它的右子树 def later_stack(self, root): \"\"\"利用堆栈实现树的后序遍历\"\"\" if root == None: return myStack1 = [] myStack2 = [] node = root myStack1.append(node) while myStack1: #这个while循环的功能是找出后序遍历的逆序，存在myStack2里面 node = myStack1.pop() if node.lchild: myStack1.append(node.lchild) if node.rchild: myStack1.append(node.rchild) myStack2.append(node) while myStack2: #将myStack2中的元素出栈，即为后序遍历次序 print myStack2.pop().elem, def level_queue(self, root): \"\"\"利用队列实现树的层次遍历\"\"\" if root == None: return myQueue = [] node = root myQueue.append(node) while myQueue: node = myQueue.pop(0) print node.elem, if node.lchild != None: myQueue.append(node.lchild) if node.rchild != None: myQueue.append(node.rchild)if __name__ == '__main__': \"\"\"主函数\"\"\" elems = range(10) #生成十个数据作为树节点 tree = Tree() #新建一个树对象 for elem in elems: tree.add(elem) #逐个添加树的节点 print '队列实现层次遍历:' tree.level_queue(tree.root) print '\\n\\n递归实现先序遍历:' tree.front_digui(tree.root) print '\\n递归实现中序遍历:' tree.middle_digui(tree.root) print '\\n递归实现后序遍历:' tree.later_digui(tree.root) print '\\n\\n堆栈实现先序遍历:' tree.front_stack(tree.root) print '\\n堆栈实现中序遍历:' tree.middle_stack(tree.root) print '\\n堆栈实现后序遍历:' tree.later_stack(tree.root)","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankchen.xyz/tags/Algorithm/"}]},{"title":"个人面经（百度、腾讯、鹏元数据、行云智能数据岗）","slug":"个人面经","date":"2017-04-22T06:51:17.000Z","updated":"2017-04-25T01:23:44.000Z","comments":true,"path":"2017/04/22/个人面经/","link":"","permalink":"http://frankchen.xyz/2017/04/22/个人面经/","excerpt":"记录自找工作以来个人的面试经历与一些思考。","text":"记录自找工作以来个人的面试经历与一些思考。 百度数据挖掘一面（电话面） 介绍项目 问题 基础知识：java的多态、map和垃圾回收 如何用网络知识让抢火车票更快 快排的思想、时间和空间复杂度、如果是整数排序有没有O(n)的解法 逻辑回归线性回归区别 linux怎么查看某文件当前被哪些进程访问 vim如何查找替换 百度运维一面（电话面） 聊项目 python字符串的替换 SQL的优化 LInux 如何找进程杀进程 百度运维二面（现场面） 聊项目 手写冒泡 行云智能一面（现场面）遇到面试官是西电校友 聊项目 CNN的思想：pooling的方式、卷积的思想 设计模式有什么了解 多线程多进程的了解 快排的思想 手写代码二叉树删除 中科乐创一面（现场面）最尴尬的一次，面试官是南洋理工的，聊了聊我的项目就似乎对我不感兴趣，就开始和我聊家常。。。 鹏元数据一面（现场面） 聊项目 做了张试卷如 推导极大似然估计 聚类与分类区别，列举常用聚类算法及程序包 一些简单的SQL命令 编程题：列举一串数字内奇偶数出现次数及引申出的结合他们业务的评级转换矩阵的打印 腾讯基础研究一面（现场面） 聊项目 聚类与分类区别，常用聚类算法思考：诸如此类列举算法的问题，最好是迅速流利的列举出多个，不要有迟疑，不过对于其基本含义要有了解。 场景题，两个含有数字的文件，找出同时出现在两个文件内的数字；若文件太大放不进内存该怎么办？思考：这种问题可小可大，可难可易。因为哪怕再小的问题在规模变大也就是涉及到大数据都是不简单的，这个问题，对于小文件，两三行代码即可搞定，那么你写出来之后，面试官基本上就会进一步问你：如果文件很大，无法同时把这两个文件装进内存，怎么办？我当时回答的是用Pandas的read_csv分块读取，这是个很不好的回答，因为掉包不是基本功。我回头想了想，也许这个答案是用generator比较好。","categories":[],"tags":[{"name":"Interview","slug":"Interview","permalink":"http://frankchen.xyz/tags/Interview/"}]},{"title":"Ubuntu 16.04下为TITAN 1080 显卡安装驱动(Cuda&CudNN)及Gpu版TensorFlow","slug":"Ubuntu-16-04下为TITAN-1080-显卡安装驱动-Cuda-CudNN-及Gpu版TensorFlow","date":"2017-04-20T01:29:22.000Z","updated":"2017-04-20T07:49:47.000Z","comments":true,"path":"2017/04/20/Ubuntu-16-04下为TITAN-1080-显卡安装驱动-Cuda-CudNN-及Gpu版TensorFlow/","link":"","permalink":"http://frankchen.xyz/2017/04/20/Ubuntu-16-04下为TITAN-1080-显卡安装驱动-Cuda-CudNN-及Gpu版TensorFlow/","excerpt":"近来入坑了TITAN 1080显卡，在Ubuntu 16.04下为装好驱动以使用Gpu版TensorFlow可不简单，踩了许多坑之后写下此篇为记录。","text":"近来入坑了TITAN 1080显卡，在Ubuntu 16.04下为装好驱动以使用Gpu版TensorFlow可不简单，踩了许多坑之后写下此篇为记录。 下载Cuda按装官方教程，我们可以应该安装Cuda8.0和Cudnn V5.1，在此下载CUDA 8.0 Downloads | NVIDIA Developer 在这里最好选runfile local，因为选deb的话会遇到apt get的源损坏问题。 降级gcc和g++由于Cuda不支持新版本的gcc和g++，所以如果建议先降级到4版本，方法见ubuntu 中 gcc/g++版本降级 安装显卡驱动sudo apt-get install nvidia-367 安装Cuda关闭你的图形界面sudo service lightdm stop 此时电脑应该会黑屏， CTRL + ALT + F1进入命令行，登录，cd 到你存放下载的目录，执行 sudo bash cuda_8.0.44_linux.run 然后你会看到如Do you accept the previously read EULA?accept/decline/quit: 输入accept Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 3xx.xx? 输入 no 之后还会问你是否安装X configuration 输入no 安装好了之后，再用命令sudo bash cuda_8.0.44_linux.run -slient -driver 来安装驱动。 最后sudo service lightdm start或者重启。 设置Cuda环境变量sudo vi ~/.bashrc 添加 12export LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64&quot;export CUDA_HOME=/usr/local/cuda-8.0 保存退出 再source .bashrc Cudnn 安装下载Cudnn在此处下载：Membership Required | NVIDIA Developer，这里你先得注册一个NVIDA账号，填写一堆问卷。有两种方法，一是deb安装包，二是下载tar 方法一选择 Download cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0下载cuDNN v5.1 Runtime Library for Ubuntu16.04 Power8 (Deb)安装 方法二二是下载tar，解压后会得到一个Cuda文件夹，复制到Cuda-8.0文件夹中123sudo cp cuda/include/cudnn.h /usr/local/cuda-8.0/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda-8.0/lib64sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda-8.0/lib64/libcudnn* 安装TensorFlow这里我们使用Anaconda装Python3， 下载下载好安装脚本之后，bash ~/Downloads/Anaconda3-4.3.0-Linux-x86_64.sh安装，记得在询问是否添加PATH时选择yes pip install tensorflow-gpu 建立虚拟环境新建环境conda create -n tensorflow激活环境source activate tensorflow此时已处于此环境下 安装TensorFlowconda install tensorflow-gpu这里Anaconda会自动安装依赖，直到全部完成 测试TF123456789101112$ python...&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')&gt;&gt;&gt; sess = tf.Session()&gt;&gt;&gt; print(sess.run(hello))Hello, TensorFlow!&gt;&gt;&gt; a = tf.constant(10)&gt;&gt;&gt; b = tf.constant(32)&gt;&gt;&gt; print(sess.run(a + b))42&gt;&gt;&gt; 若打印一系列包含Gpu信息的说明，恭喜你，安装成功！！！","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://frankchen.xyz/tags/TensorFlow/"}]},{"title":"创建双击可执行的sh脚本","slug":"创建双击可执行的sh脚本","date":"2017-04-19T01:36:49.000Z","updated":"2017-04-19T01:48:15.000Z","comments":true,"path":"2017/04/19/创建双击可执行的sh脚本/","link":"","permalink":"http://frankchen.xyz/2017/04/19/创建双击可执行的sh脚本/","excerpt":"在Mac&amp;Linux上创建双击可执行的sh脚本的方法。","text":"在Mac&amp;Linux上创建双击可执行的sh脚本的方法。 Mac首先创建测试脚本touch clickexe.shopen -e clickexe.sh在脚本中输入内容echo &quot;hello world&quot; 再执行命令chmod +x clickexe.sh 然后在取景器右键单击文件，并选择“get info”，然后选择“Open with” 这里你可以选择你想要的文件执行到应用程序中，为了能够选择你需要从“推荐应用”到“所有应用程序”选中“Terminal”即可。 Linux在Ubuntu中，自从13.04以后，双击sh脚本文件就已经默认是geidt打开了，要想运行，从文件管理器–&gt;文件–&gt;首选项–&gt;行为–&gt;可执行文件 有三个选项，默认是第二个，如果想要直接运行，选第一个，而每次询问就是弹出一个窗口，问你是运行，在终端中运行，还是用gedit查看。 记得在脚本文件右键–&gt;属性–&gt;权限：允许以程序执行文件 参考自 ubuntu sh脚本双击运行 - Findxiaoxun - 博客园","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"},{"name":"Mac","slug":"Mac","permalink":"http://frankchen.xyz/tags/Mac/"},{"name":"bash","slug":"bash","permalink":"http://frankchen.xyz/tags/bash/"}]},{"title":"Python2 Python3的各种冲突及解决方法","slug":"Python3-安装opencv的简易方法","date":"2017-04-18T09:52:06.000Z","updated":"2017-04-18T10:05:33.000Z","comments":true,"path":"2017/04/18/Python3-安装opencv的简易方法/","link":"","permalink":"http://frankchen.xyz/2017/04/18/Python3-安装opencv的简易方法/","excerpt":"本文主要记录Python2 Python3的各种冲突及解决方法。","text":"本文主要记录Python2 Python3的各种冲突及解决方法。 Opencv在Python3下的安装目前opencv对于Python2.7支持不错，我折腾了许久才弄好Python3下的安装方法，最简单的既是利用Anaconda来安装，步骤如 安装Anaconda下载对应平台的安装包Download Anaconda Now! | Continuum 下载完毕，用终端命令安装刚下的包bash Anaconda3-4.3.0-MacOSX-x86_64.sh 命令行会询问是否需要添加PATH如.bash_profile &gt;&gt;填yes 刷新 cd &amp;&amp; source .bash_profile 检测安装位置check python，结果若是$ which python $ /.../anaconda/bin/python则OK 最后用这条命令安装pip install opencv-pyhton（不需要pip3） urlparseimport urlparse 需要变为 import urllib.parse as urlparse 整数相除python2的 / 等价于3里的 // map 与 zipPython3里的map与zip返回生成器，所以需要将list(map(func, b))， list(zip(a,b))之后才可以按下标访问。","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"}]},{"title":"programmer humor week 2","slug":"programmer-humor-week-2","date":"2017-04-17T09:58:39.000Z","updated":"2017-04-17T09:41:37.000Z","comments":true,"path":"2017/04/17/programmer-humor-week-2/","link":"","permalink":"http://frankchen.xyz/2017/04/17/programmer-humor-week-2/","excerpt":"programmer humor from reddit 🤣😂😜","text":"programmer humor from reddit 🤣😂😜 “Hey, you know Binary Search Trees?” “Say no more.” After setting up a new floor on campus… Logins should be unique [Cringe] How to do coding How to learn coding in a single night. 引用自Programmer Humor","categories":[],"tags":[{"name":"Funny","slug":"Funny","permalink":"http://frankchen.xyz/tags/Funny/"}]},{"title":"利用git 同步本地与服务器代码","slug":"利用git-同步本地与服务器代码","date":"2017-04-12T16:53:49.000Z","updated":"2017-04-18T08:12:17.000Z","comments":true,"path":"2017/04/13/利用git-同步本地与服务器代码/","link":"","permalink":"http://frankchen.xyz/2017/04/13/利用git-同步本地与服务器代码/","excerpt":"在服务器端直接用vim写代码固然不是个好体验，本文介绍用git在两者之间同步的方法。","text":"在服务器端直接用vim写代码固然不是个好体验，本文介绍用git在两者之间同步的方法。 免认证登录先设置ssh公钥私钥来免密码登录。 12cd ~/.sshssh-keygen -t rsa -C \"mail@domain.com\" 复制过去 123scp ~/.ssh/id_rsa.pub user@vps:./ssh user@vpscat id_rsa.pub &gt;&gt; /home/user/.ssh/authorized_keys 注意cat 后是&gt;&gt; 不是&gt;，前者是追加，后者是覆盖。若22端口不能使用，可以通过-P port_number指定使用的端口号 进入服务器项目1234567ssh user@vps cd /home/projectgit initecho &quot;hello&quot; &gt;&gt; READMEgit add READMEgit commit -m &quot;add README&quot;exit 在本机上把项目 Clone 下来： git clone user@vps:/home/project 若22端口不能使用，可以执行： git clone ssh://user@vps:1280/home/project 修改钩子使其接受push在服务器端git config receive.denyCurrentBranch ignore 编辑 VPS 端 Git 钩子.git/hooks/post-receive 文件，内容为：123#!/bin/shcd ..env -i git reset --hard 最后将文件设置为可执行：chmod a+x .git/hooks/post-receive","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"}]},{"title":"利用Pandas读取远程mysql数据库","slug":"利用Pandas读取远程mysql数据库","date":"2017-04-11T09:31:49.000Z","updated":"2017-04-11T10:03:12.000Z","comments":true,"path":"2017/04/11/利用Pandas读取远程mysql数据库/","link":"","permalink":"http://frankchen.xyz/2017/04/11/利用Pandas读取远程mysql数据库/","excerpt":"利用pymysql与Pandas读取远程数据库上的表格，并存储为本地csv文件。","text":"利用pymysql与Pandas读取远程数据库上的表格，并存储为本地csv文件。 因为业务需要读取服务器上的数据进行分析，能连接到sql服务器，但是当我企图用mysqldump备份（复制）database到本地却经常出现mysqldump: Error 2013: Lost connection to MySQL server during query when dumping table错误，这个需要改net_write_timeout来增加等待时长来解决，苦于我的账号并没有权限，难不成只好之间上sql用odbs大法直接做数据分析？我的天，那得多麻烦，经过几番思考，我的解决方案是：用sql的python接口包pymysql进行sql登录以及操作，再用Pandas以DataFrame格式接受table数据，最后存为csv文件。 登录模块注意，若数据库含有中文，需要use_unicode=True避免乱码 1234567891011121314151617181920212223# -*_coding:utf8-*-# Created by frank at 07/04/2017import pymysqlhost = '#########'port = ####user = '########'password = '#######'def get_database(): \"\"\" :return: return the pre-connect database \"\"\" connection = pymysql.connect(host=host, user=user, password=password, port=port, charset='utf8mb4', use_unicode=True,) return connection 处理模块123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# -*_coding:utf8-*-# Created by frank at 07/04/2017# 连接数据库from connection import get_databaseimport pandas as pd# 执行sql命令，打印返回结果def execute(sql_command): try: with connection.cursor() as cursor: cursor.execute(sql_command) result = cursor.fetchall() print(\"COMMAND: \", sql_command) print(\"RESULT: \", result) print() dic[sql_command] = result except: print(\"Command %s failed!\" % sql_command) print()# 建立连接connection = get_database()connection.commit()# 存放命令与对应结果，便于之后调试dic = &#123;&#125;# SQL 查询语句sql_commands = [# \"SHOW DATABASES;\",\"use wind_data;\",# \"show tables;\",# \"SHOW COLUMNS FROM absdescription;\",# \"SHOW COLUMNS FROM absdescription2;\",# \"SHOW COLUMNS FROM ashareagency;\"# \"select * from absdescription;\",# \"select count(*) from absdescription;\"# \"SHOW COLUMNS FROM xcashflow;\",]for sql_command in sql_commands: execute(sql_command)# 将所有table名字存于listtables = []l = list(dic[\"show tables;\"])for i in l: tables.append(i[0])# 取得所有table，分别存放于csv文件for table in tables:# table = 'absdescription' sql_cmd = \"select * from %s\" % table df = pd.read_sql(sql=sql_cmd, con=connection) df.to_csv(\"data/%s.csv\" % table)# 断开连接connection.close() 参考自 mysqldump 错误2013 Lost connection - 简书 MySQL常见错误分析与解决方法总结 - 运维生存时间 Python中从SQL型数据库读写dataframe型数据 - Arkenstone - 博客园 pandas教程：[19]读写sql数据库_百度经验","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"Pandas","slug":"Pandas","permalink":"http://frankchen.xyz/tags/Pandas/"},{"name":"SQL","slug":"SQL","permalink":"http://frankchen.xyz/tags/SQL/"}]},{"title":"Data Science cheatsheet","slug":"machine-learning-cheat-sheet","date":"2017-04-05T08:45:56.000Z","updated":"2017-04-07T01:24:01.000Z","comments":true,"path":"2017/04/05/machine-learning-cheat-sheet/","link":"","permalink":"http://frankchen.xyz/2017/04/05/machine-learning-cheat-sheet/","excerpt":"记录若干关于数据科学的cheatsheet","text":"记录若干关于数据科学的cheatsheet Pandas Sklearn Anaconda 引用自 scikit-learn/ML_MAPS_README.rst at 4d9a12d175a38f2bcb720389ad2213f71a3d7697 · scikit-learn/scikit-learn pandas/doc/cheatsheet at master · pandas-dev/pandas Conda cheat sheet — Conda documentation","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://frankchen.xyz/tags/Data-Science/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://frankchen.xyz/tags/Machine-Learning/"}]},{"title":"programmer humor week 1","slug":"programmer-humor-week-1","date":"2017-04-01T09:58:39.000Z","updated":"2017-04-01T10:13:57.000Z","comments":true,"path":"2017/04/01/programmer-humor-week-1/","link":"","permalink":"http://frankchen.xyz/2017/04/01/programmer-humor-week-1/","excerpt":"programmer humor from reddit 🤣😂😜","text":"programmer humor from reddit 🤣😂😜 How OS fanboys (and girls) meet The doors at work are version-controlled. 5 word horror story Windows has a built-in java code optimizer with a simple drag-and-drop interface GitHub. The place where I fork. 引用自Programmer Humor","categories":[],"tags":[{"name":"Funny","slug":"Funny","permalink":"http://frankchen.xyz/tags/Funny/"}]},{"title":"Learn More,Study Less","slug":"learn more study less","date":"2017-03-26T07:11:56.000Z","updated":"2017-03-26T07:47:48.000Z","comments":true,"path":"2017/03/26/learn more study less/","link":"","permalink":"http://frankchen.xyz/2017/03/26/learn more study less/","excerpt":"关于learn more study less 的思维导图","text":"关于learn more study less 的思维导图 holistic learning 的流程Acquire 获取 准确、精炼地得到信息 简化 提取精华、略去无用信息 容量 知道的信息越多越好 速度 尽快完成获取阶段 Understand 理解 了解信息的基本意思，并联系上下文来理解 理解信息的第一阶段：理解信息的表层含义 当遇到不明白之处，应该分解问题，找到真正不明白之处 Explore 扩展 形成高速公路、模型及内部联系良好的结构 深度扩展 知识的由来、背景的探究 横向扩展 构造模型：寻找类似的知识的联系 纵向扩展 创建高速公路：跨学科、跨领域的知识联系 最重要、最有挑战性的扩展方法，有益于创造性思考 比喻metaphor 内在法visceralization Getting Started with Visceralization Test 测试Debug纠错 在模型和高速公路中寻找改正错误 知识网络的修剪，添加特殊例子，删减错误的联系等 阅读与你观点相反的书籍 将你的知识放入现实世界来观察等 Apply 应用 创造新途径，将所学知识知识应用起来，这是学习的最终目的 通过比较知识如何在现实中应用来调整，防止成为“书呆子” 找出自己的薄弱环节获取 症状 阅读和听讲速度慢 需要反复阅读 原因 阅读/学习习惯不好 记笔记习惯不好 不理解基本概念、术语 解决 养成良好的学习、阅读和记笔记的习惯 理解 表现 阅读时不明白作者意思 笔记很完善，却不明白其含义 解决 放慢阅读速度，寻找其他资料，以求得更加详细，另一角度的阐述 基本的理解是后续高级技巧的基石 扩展 表现 缺少灵活性 已经掌握了一个新东西，但是没有或不会将 它与其他学过的知识联系起来，假如让你用这个新的知识去解决一个非常规问题，就常常束手无策。 纠错 表现 错误联系太多 在课堂上很少出现，而在实际世界中却很普遍 课堂上人们很少去扩展：发生错误机会少 实际中人们经常想出各种联系，却不验证这些联系是否正确：各种迷信的产生 应用 表现 不能在真实世界中很好地运用知识 解决 更多的实践，抛开书本，走出去，去做实验，去接触生活，去融入社会 holistic learning techniques获取知识 快速阅读法 指读法 用食指放在你要读的那一行下面，不断移动食指 练习阅读 对比指读法与常规读法，测试阅读速度提高潜力 对比先读后记与边读边记来测试理解能力 积极阅读 边读边记笔记 这一节中主要点是什么？ 是否完整地获取信息 我怎样才能记住主要点？ 联系信息，视觉化和比喻法 我要怎样将主要点拓展开以及应用它 将信息应用在不同的情境中 流笔记法 重点在于提取主要信息，删减次要信息，重点记录联系和按照你理解的方式给信息分类 用很短的单词来记下最主要的观点 在相联系的观点之间划上一些箭头 理解与扩展 关键信息 学习其他知识的基础，与之后的信息关系极强，对于之后的理解至关重要 比喻 要点：不熟悉的知识和熟悉的知识联系起来 确定你要深入理解和记忆信息 在你的个人经验中寻找与信息部分相似的东西，不求达到完全符合，可找到多个部分符合的“不完美比喻”。 重复上述过程，检查比喻不恰当的地方 运用技巧：要有寻找比喻的欲望 遇到有理解困难的知识，首先应想到比喻 注意第一个出现在脑海中的念头 优化和测试你的比喻 多找几个不同角度的比喻 抽象信息较适合比喻 内在化 调动更多的感知、情感与知识联系在一起，形成强联系 视觉 听觉 运动 情感 步骤 明确要内在化概念 从建立脑海中的静态图像开始 将脑海中的静态图像转换为栩栩如生的动态场景 加上其他感官，试着用手去拿，去摸，去打它，去嗅它的味道，去听它的声音，动用你身体的所有感官，将所有的感觉与运动的图像相联系。 加入更多的感觉或情感 不断重复和优化图像 具体信息较适合内在化 简图法 简图与比喻和内在化结合在一起 简图可以和比喻及内在化混合在一起，以加深对知识的理解 流简图 从一个简单的元素开始，然后在这个成分及与之相联系的不同知识之间 画出联系箭头 概念图 将观点联系在一起，与流笔记密切相关。概念图里的关系观点之间的内在关系，在不同观点之间画上箭头， 箭头上还需要加上一些简单的话语 图像简图 粗糙简单的涂鸦来代替文字 困难信息 联想法 将一系列观点串在一起，就像链条 步骤 创造顺序 先在纸上写下你打算记住的很多信息，在你能理解的前提下，迅速地将信息分成几个类 别 设计符号 要能迅速让你联想到原始的知识 创建属于自己的联想 发挥想象力，把符号之间连接起来 难点 符号重复 可以尝试不同颜色来区分 断裂的联系 符号串不宜太长 难以辨认的符号 用只有自己能懂的很好，但是一定不能让自己迷惑 触发物丢失 防止记不起第一个：增加中第一项和触发物之间的联想 抽象信息、随意信息，关联程度不强 信息的结构随意信息 特点 缺少内在逻辑联系，需要死记硬背的知识 方法 着力去寻找知识点之间的联系 观点信息 特点 存在争议的信息 方法 在获取阶段，快速检索大量信息：找寻其中的模式（不记忆具体细节） 速读 图示：以加强对比以提取模式，增进理解 大多数学科都是这几种信息不同比例的组合过程信息 特点 教导如何行动，一系列操作的集合 方法 打造良好的结构与模型：加速过程信息的学习 需要大量时间投入练习 具体信息 特点 能与感官、实际相联系的信息 方法 内在化：有助于你将信息与多个感 官相联系 运用各种方法找寻信息之间的联系：以把弱结构的信息转化为强结构的信息抽象信息 特点 缺少与感官的直接联系 方法 运用方法将抽象信息降低到一个基本的级别 内在法 比喻 三个基本概念Constructs Familiar Constructs 感官结构 来自人类的感官知觉，是最熟悉、最生动的结构 关系结构 来自人类生活经验中人与事务、事物之间的联系 基本数学结构 来自我们以掌握的简单的基本数学概念 结构：就是一系列联系紧密的知识，一个知识点与越多的其它的知识点相联系，则你对于这个知识点的掌握就越牢固，应用起来也就越简单明了。 Model 模型是对结构的索引，是对于结构的形象化的抽象，它将关键的结构压缩和概括， Highway 高速公路是不同结构之间的联系，这种联系是跨学科的，也即是我们可以把A领域的知识迁移到B领域。 将每个 知识点都要经过整体性学习里的明白、拓展和应用三个阶段比喻法、图表法以及信息压 缩技术","categories":[],"tags":[{"name":"Time Management","slug":"Time-Management","permalink":"http://frankchen.xyz/tags/Time-Management/"},{"name":"Life","slug":"Life","permalink":"http://frankchen.xyz/tags/Life/"}]},{"title":"Basic Linux/Unix command Interview Questions","slug":"Basic-Linux-Unix-command-Interview-Questions","date":"2017-03-18T16:53:08.000Z","updated":"2017-03-19T05:42:16.000Z","comments":true,"path":"2017/03/19/Basic-Linux-Unix-command-Interview-Questions/","link":"","permalink":"http://frankchen.xyz/2017/03/19/Basic-Linux-Unix-command-Interview-Questions/","excerpt":"Overview of Basic Linux/Unix command Interview Questions: 常见Linux命令的概述记录","text":"Overview of Basic Linux/Unix command Interview Questions: 常见Linux命令的概述记录 初级问题 列出目录内所有的软链接文件 ls -la | grep &quot;^l&quot; ls -la会以ASCII列出所有文件，而软链接文件都是以l开头的，grep &quot;^l&quot;会找出它们。关于grep的正则pattern可以见此：Using Grep &amp; Regular Expressions to Search for Text Patterns in Linux | DigitalOcean 创建一个只读的文件？ 12touch filechmod 400 file 更多可见：File permissions in UNIX Linux with Example &gt;&gt; Unix Tutorial 如何将一个进程在后端运行？如何将其返回前端？如何杀死进程？ 分别是在命令后加上&amp;；fg jobid(用jobs来看jobid)；kill -KILL PID 可见How do I run a Unix process in the background? 测试远程链接？ 用ping或telnet即可 找寻历史命令？ history | grep &quot;Pattern&quot; 远程复制文件？ scp，或者rsync，sftp 进程的CPU占用？ top 当前磁盘用量？ df -h Swapping交换和Paging分页的区别？ paging指的是以页为单位的交换，swapping指的是以整个进程为单位的交换。 paging机制是进程内外交换的主流，某些情况下仍然会使用swapping机制，比如：1、系统内存严重短缺，paging机制的速度已经不足以满足需要。2、进程处于完全非激活状态超过10秒钟。swapping会把整个进程移出主存，而不像paging那样只弄点页面出去。 swapping和paging都是内存与磁盘(虚拟内存)之间的交互方式当内存不够用时，为了加载急需的程序进内存，需要将一部分暂时不用的内存存到磁盘上，两者对比swapping更快更粗暴，paging更慢更flexibly，更多可见此[转载]内存的分页与交换_流苏_新浪博客 中级问题 ps -ef 和 ps -auxwww有什么区别？ ps -auxwww可以打印死进程？？ cpu信息？ cat /proc/cpuinfo 软链接与硬链接区别 How to Create, Update and Remove Soft link in UNIX 什么是僵尸进程？如何找到僵尸进程？ When a program forks and the child finishes before the parent, the kernel still keeps some of its information about the child in case the parent might need it - for example, the parent may need to check the child’s exit status. To be able to get this information, the parent calls ‘wait()’; In the interval between the child terminating and the parent calling ‘wait()’, the child is said to be a ‘zombie’ ps中，僵尸进程的状态将带有’Z’ 如何统计一个文件内字符”Unix”出现次数？ grep -c &quot;Unix&quot; filename chmod是什么意思？r-- -w- --x代表着什么？ chmod用于改变一个文件或者目录在Linux里的权限，rwx分别代表读写执行，而字符顺序代表着用户 组 以及 其他， 那么这里的意思就是这个文件用户可读，组内用户可写，其他只能执行。 系统内有个文件包含字符”UnixCommandInterviewQuestions”，如何找到？ 见此10 Example of find command in Unix and Linux 如何检查是否某进程在监听某端口？ telnet hostname port Top 10 basic networking commands in linux/unix 高级问题 怎么知道某个文件被哪些进程使用？ lsof，尼玛呀，这就是我baidu电话面试时碰到的问题。。 查看你的某个端口被什么远程服务器连接？ netstat -a | grep &quot;port&quot; nohup是什么？ 用于在后端运行程序，但是和&amp;不同，nohup在用户log off 时不会停止运行，而&amp;会终止。 ephemeral port？ 临时端口也叫做暂时端口。通常存在于客户机中。它在客户端应用程序需要连接到服务器时建立而在客户端应用程序终止时取消。它的端口号是随机选取的，数值大于1023。 如果一个进程正在往MYSQL里写数据，如何管查它写入的速度？ watch 替换某文件内所有某字符？ sed s/Unix/UNIX/g fileName 某文件包含以tab分隔的Name, Address and Phone Number，如何提取所有的Phone Number？ cut -f3 filename 服务器运行时间？ uptime IP 与 hostname互转？ nslookup","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"}]},{"title":"python源码解析0：趣事","slug":"python-source-code-reading-0","date":"2017-03-17T19:56:23.000Z","updated":"2017-04-18T01:14:26.000Z","comments":true,"path":"2017/03/18/python-source-code-reading-0/","link":"","permalink":"http://frankchen.xyz/2017/03/18/python-source-code-reading-0/","excerpt":"About 两处python源码的趣味解读。","text":"About 两处python源码的趣味解读。 The Zen of Python我们知道，import this会打印如下“蟒蛇之禅”： 123456789101112131415161718192021The Zen of Python, by Tim PetersBeautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.Special cases aren't special enough to break the rules.Although practicality beats purity.Errors should never pass silently.Unless explicitly silenced.In the face of ambiguity, refuse the temptation to guess.There should be one-- and preferably only one --obvious way to do it.Although that way may not be obvious at first unless you're Dutch.Now is better than never.Although never is often better than *right* now.If the implementation is hard to explain, it's a bad idea.If the implementation is easy to explain, it may be a good idea.Namespaces are one honking great idea -- let's do more of those! 但是在source里面也就是2.7/Lib/this.py里，是这样的： 12345678910111213141516171819202122232425262728s = \"\"\"Gur Mra bs Clguba, ol Gvz CrgrefOrnhgvshy vf orggre guna htyl.Rkcyvpvg vf orggre guna vzcyvpvg.Fvzcyr vf orggre guna pbzcyrk.Pbzcyrk vf orggre guna pbzcyvpngrq.Syng vf orggre guna arfgrq.Fcnefr vf orggre guna qrafr.Ernqnovyvgl pbhagf.Fcrpvny pnfrf nera'g fcrpvny rabhtu gb oernx gur ehyrf.Nygubhtu cenpgvpnyvgl orngf chevgl.Reebef fubhyq arire cnff fvyragyl.Hayrff rkcyvpvgyl fvyraprq.Va gur snpr bs nzovthvgl, ershfr gur grzcgngvba gb thrff.Gurer fubhyq or bar-- naq cersrenoyl bayl bar --boivbhf jnl gb qb vg.Nygubhtu gung jnl znl abg or boivbhf ng svefg hayrff lbh'er Qhgpu.Abj vf orggre guna arire.Nygubhtu arire vf bsgra orggre guna *evtug* abj.Vs gur vzcyrzragngvba vf uneq gb rkcynva, vg'f n onq vqrn.Vs gur vzcyrzragngvba vf rnfl gb rkcynva, vg znl or n tbbq vqrn.Anzrfcnprf ner bar ubaxvat terng vqrn -- yrg'f qb zber bs gubfr!\"\"\"d = &#123;&#125;for c in (65, 97): for i in range(26): d[chr(i+c)] = chr((i+13) % 26 + c)print \"\".join([d.get(c, c) for c in s]) 哈哈，这不就是凯撒密码么。。。每个字母分大小写(65:A, 97:a)都被对应到ASCII 表后13个位置，于是就有 123456789101112131415&#123;'A': 'N', 'B': 'O', 'C': 'P', 'D': 'Q', 'E': 'R', 'F': 'S',....'s': 'f', 't': 'g', 'u': 'h', 'v': 'i', 'w': 'j', 'x': 'k', 'y': 'l', 'z': 'm'&#125; 所以，原作者应该是这样写出来的密码的 12345678910111213141516171819202122232425262728s = \"\"\"The Zen of Python, by Tim PetersBeautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.Special cases aren't special enough to break the rules.Although practicality beats purity.Errors should never pass silently.Unless explicitly silenced.In the face of ambiguity, refuse the temptation to guess.There should be one-- and preferably only one --obvious way to do it.Although that way may not be obvious at first unless you're Dutch.Now is better than never.Although never is often better than *right* now.If the implementation is hard to explain, it's a bad idea.If the implementation is easy to explain, it may be a good idea.Namespaces are one honking great idea -- let's do more of those!\"\"\"d = &#123;&#125;for c in (65, 97): for i in range(26): d[chr(i+c)] = chr((i-13) % 26 + c)print(\"\".join([d.get(c, c) for c in s])) 很神奇吧？哈哈，其实就是逆向一下就ok😎 antigravity 我们知道，在python里，import antigravity会让我们弹到一个网站，里面有这幅著名的图画 python带我们飞！那么为什么会这样呢？ 看看源码， 123import webbrowserwebbrowser.open(\"http://xkcd.com/353/\") 哈哈，原来就这两行，Are you kidding me? 关于这个的来龙去脉，可以看看这里：The History of Python: import antigravity以及这里：import antigravity | sciyoshi.com，大意是源自google的一个复活节彩蛋。值得注意的是，python3的antigravity彩蛋升级了！不信你看 12345678910111213141516import webbrowserimport hashlibwebbrowser.open(\"https://xkcd.com/353/\")def geohash(latitude, longitude, datedow): '''Compute geohash() using the Munroe algorithm. &gt;&gt;&gt; geohash(37.421542, -122.085589, b'2005-05-26-10458.68') 37.857713 -122.544543 ''' # http://xkcd.com/426/ h = hashlib.md5(datedow).hexdigest() p, q = [('%f' % float.fromhex('0.' + x)) for x in (h[:16], h[16:32])] print('%d%s %d%s' % (latitude, p[1:], longitude, q[1:])) 里面隐藏了一个计算Geohashing的算法，可以根据给定点坐标的经纬度以及当前时间计算一个随机的新的点坐标。","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"}]},{"title":"programmer humor week 0","slug":"programmer-humor-week-0","date":"2017-03-15T10:33:39.000Z","updated":"2017-03-15T11:51:39.000Z","comments":true,"path":"2017/03/15/programmer-humor-week-0/","link":"","permalink":"http://frankchen.xyz/2017/03/15/programmer-humor-week-0/","excerpt":"programmer humor from reddit 🤣😂😜","text":"programmer humor from reddit 🤣😂😜 Huge improvements in both code quality and performance Coding in MS Paint What debuggers actually do Every time One last compilation 引用自Programmer Humor","categories":[],"tags":[{"name":"Funny","slug":"Funny","permalink":"http://frankchen.xyz/tags/Funny/"}]},{"title":"数据挖掘流程：数据可视化与预处理","slug":"数据挖掘流程：数据可视化与预处理","date":"2017-03-15T03:14:11.000Z","updated":"2017-03-16T10:46:45.000Z","comments":true,"path":"2017/03/15/数据挖掘流程：数据可视化与预处理/","link":"","permalink":"http://frankchen.xyz/2017/03/15/数据挖掘流程：数据可视化与预处理/","excerpt":"数据挖掘的第一步，当我们手中拿到一份数据，当然是对数据进行观察与预处理了。本文主要对这两个方面做一个个人的总结。","text":"数据挖掘的第一步，当我们手中拿到一份数据，当然是对数据进行观察与预处理了。本文主要对这两个方面做一个个人的总结。 import 掉包？第0步，调包。。 通常numpy、pandas、scipy、seaborn已经matplotlib是必须的 为防止烦人的warning，还需要 12import warningswarnings.filterwarnings(&apos;ignore&apos;) 因为我们不管warning只管error，哈哈😂 seaborn的设置颜色格式 1sns.set_style(&apos;whitegrid&apos;) 以及 1%matplotlib inline 观察与可视化一般来说，我们应该首先观察标签的分布以及特征的分布，对于前者通常使用pandas Dataframe的columns，对于后者则是value_counts()和describe()来展示。如 1df_train.columns 1int_level = train_df[&apos;interest_level&apos;].value_counts() 1df_train[&apos;SalePrice&apos;].describe() describe()会给出标签的各种数值信息，如 123456789count 1460.000000mean 180921.195890std 79442.502883min 34900.00000025% 129975.00000050% 163000.00000075% 214000.000000max 755000.000000Name: SalePrice, dtype: float64 同时也可以借助于seaborn图来显示 1sns.distplot(df_train[&apos;SalePrice&apos;]); 接下来可以看看标签的偏度和峰度(skewness and kurtosis) 12print(&quot;Skewness: %f&quot; % df_train[&apos;SalePrice&apos;].skew())print(&quot;Kurtosis: %f&quot; % df_train[&apos;SalePrice&apos;].kurt()) 特征与标签关系对于数值特征可用散点图来展示 123var = 'GrLivArea'data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000)); 对于类别特征，则可用箱型图 12345var = &apos;OverallQual&apos;data = pd.concat([df_train[&apos;SalePrice&apos;], df_train[var]], axis=1)f, ax = plt.subplots(figsize=(8, 6))fig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data)fig.axis(ymin=0, ymax=800000); 时间特征类似 123456var = &apos;YearBuilt&apos;data = pd.concat([df_train[&apos;SalePrice&apos;], df_train[var]], axis=1)f, ax = plt.subplots(figsize=(16, 8))fig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data)fig.axis(ymin=0, ymax=800000);plt.xticks(rotation=90); 特征与特征的关系用关联矩阵Correlation matrix的heatmap类型可以展示所有特征与特征的关系 1234#correlation matrixcorrmat = df_train.corr()f, ax = plt.subplots(figsize=(12, 9))sns.heatmap(corrmat, vmax=.8, square=True); 从关联矩阵我们可以对特征的总体关系有个认识，那么进一步可以用zoomed heatmap来筛选几个特征进行观察其之间的关系 1234567#saleprice correlation matrixk = 10 #number of variables for heatmapcols = corrmat.nlargest(k, &apos;SalePrice&apos;)[&apos;SalePrice&apos;].indexcm = np.corrcoef(df_train[cols].values.T)sns.set(font_scale=1.25)hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;, yticklabels=cols.values, xticklabels=cols.values)plt.show() 大杀器则是scatterplot，如 12345#scatterplotsns.set()cols = [&apos;SalePrice&apos;, &apos;OverallQual&apos;, &apos;GrLivArea&apos;, &apos;GarageCars&apos;, &apos;TotalBsmtSF&apos;, &apos;FullBath&apos;, &apos;YearBuilt&apos;]sns.pairplot(df_train[cols], size = 2.5)plt.show(); 预处理缺失值 Missing data¶首先观察包含缺失值的特征 12345#missing datatotal = df_train.isnull().sum().sort_values(ascending=False)percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)missing_data = pd.concat([total, percent], axis=1, keys=[&apos;Total&apos;, &apos;Percent&apos;])missing_data.head(20) 以上将打印包含缺失值的特征的缺失个数和比例 当然对于缺失值，不同情况有不同的处理方法，例如 这里我们简单的将missing超过1的feature丢弃，再将missing值为1的样本删去，其他方法有待之后补充。 1234#dealing with missing datadf_train = df_train.drop((missing_data[missing_data[&apos;Total&apos;] &gt; 1]).index,1)df_train = df_train.drop(df_train.loc[df_train[&apos;Electrical&apos;].isnull()].index)df_train.isnull().sum().max() 离群点单特征情况，可以以如下形式观察 12345678#standardizing datasaleprice_scaled = StandardScaler().fit_transform(df_train[&apos;SalePrice&apos;][:,np.newaxis]);low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]print(&apos;outer range (low) of the distribution:&apos;)print(low_range)print(&apos;\\nouter range (high) of the distribution:&apos;)print(high_range) 1234567891011121314151617181920212223outer range (low) of the distribution:[[-1.83820775] [-1.83303414] [-1.80044422] [-1.78282123] [-1.77400974] [-1.62295562] [-1.6166617 ] [-1.58519209] [-1.58519209] [-1.57269236]]outer range (high) of the distribution:[[ 3.82758058] [ 4.0395221 ] [ 4.49473628] [ 4.70872962] [ 4.728631 ] [ 5.06034585] [ 5.42191907] [ 5.58987866] [ 7.10041987] [ 7.22629831]] 双特征关系如特征标签关系那样，先concat再scatter1234#bivariate analysis saleprice/grlivareavar = &apos;GrLivArea&apos;data = pd.concat([df_train[&apos;SalePrice&apos;], df_train[var]], axis=1)data.plot.scatter(x=var, y=&apos;SalePrice&apos;, ylim=(0,800000)); 离群点可以简单删去，如 正态分布用histogram 和 normal probability可以观察数据的峰度偏度以及是否符合正态分布 123sns.distplot(df_train[&apos;SalePrice&apos;], fit=norm);fig = plt.figure()res = stats.probplot(df_train[&apos;SalePrice&apos;], plot=plt) 对于正偏度，取log通常不错 1234567#applying log transformationdf_train[&apos;SalePrice&apos;] = np.log(df_train[&apos;SalePrice&apos;])In [22]:#transformed histogram and normal probability plotsns.distplot(df_train[&apos;SalePrice&apos;], fit=norm);fig = plt.figure()res = stats.probplot(df_train[&apos;SalePrice&apos;], plot=plt) 有时我们会遇到具有许多0值得特征，而从istogram 和 normal probability可以看出其具有偏度，那么可以使用如下方法：将非零点取log保留零点。 12345678#create column for new variable (one is enough because it&apos;s a binary categorical feature)#if area&gt;0 it gets 1, for area==0 it gets 0df_train[&apos;HasBsmt&apos;] = pd.Series(len(df_train[&apos;TotalBsmtSF&apos;]), index=df_train.index)df_train[&apos;HasBsmt&apos;] = 0df_train.loc[df_train[&apos;TotalBsmtSF&apos;]&gt;0,&apos;HasBsmt&apos;] = 1In [28]:#transform datadf_train.loc[df_train[&apos;HasBsmt&apos;]==1,&apos;TotalBsmtSF&apos;] = np.log(df_train[&apos;TotalBsmtSF&apos;]) 对于标签，如果偏度太大，可以使用log1p来处理，并对比前后的差距 12345matplotlib.rcParams[&apos;figure.figsize&apos;] = (12.0, 6.0)prices = pd.DataFrame(&#123;&quot;price&quot;:train[&quot;SalePrice&quot;], &quot;log(price + 1)&quot;:np.log1p(train[&quot;SalePrice&quot;]) &#125;)prices.hist() 对全部Dataframe处理需要concat test与train，一并处理， 1all_data = pd.concat((train.loc[:,&quot;MSSubClass&quot;: &quot;SaleCondition&quot;], test.loc[:,&quot;MSSubClass&quot;: &quot;SaleCondition&quot;])) 最后需要将预测的标签值放大还原，如 1lasso_preds = np.expm1(model_lasso.predict(X_test)) 参考出处 House Prices: Advanced Regression Techniques | Kaggle","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://frankchen.xyz/tags/Data-Science/"}]},{"title":"Amdahl's law","slug":"Amdahl-s-law","date":"2017-02-22T10:03:29.000Z","updated":"2017-03-15T05:19:46.000Z","comments":true,"path":"2017/02/22/Amdahl-s-law/","link":"","permalink":"http://frankchen.xyz/2017/02/22/Amdahl-s-law/","excerpt":"关于Amdahl’s law。","text":"关于Amdahl’s law。 Amdahl’s law，又叫做阿姆达尔定律，其意义在于表示了计算机并行计算处理任务的加速的极限。 简略证明解释如下，假设一个任务需要时间$T$来完成，而其中可以被并行处理加速的部分比例为$p$，那么不能被并行计算加速的部分就为$1-p$，比如从磁盘上读取程序段就不能并行加速，而读取完成到内存中就可以开多个进程或者利用分布式的方法并行处理来加速。 $$T = (1-p)T + pT$$ 开启加速后，可以加速的部分时间减少至原来的 $\\frac {p} {s}$ $$T’ = (1-p)T + \\frac p s T$$ 则加速比就为 $$S_{\\text{latency}}(s) = \\frac {T} {T’} = \\frac{1}{1 - p + \\frac p s}$$ 意义Amdahl’s law揭示了不管如何的进行并行计算，只要任务包含无法被并行加速的部分，那么加速比是有上限的，因为 $$\\lim_{s \\rightarrow \\infty} S_{\\text{latency}}(s) = \\frac {T} {T’} = \\frac{1}{1 - p}$$ 例如如下图中的例子，如果任务的95%可以并行，那么加速的极限约为20倍。","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://frankchen.xyz/tags/Data-Science/"}]},{"title":"Deep Learning Interview","slug":"Deep-Learning-Interview","date":"2017-02-18T02:25:31.000Z","updated":"2017-02-18T03:21:37.000Z","comments":true,"path":"2017/02/18/Deep-Learning-Interview/","link":"","permalink":"http://frankchen.xyz/2017/02/18/Deep-Learning-Interview/","excerpt":"整理一些关于Deep Learning的面试问题。","text":"整理一些关于Deep Learning的面试问题。 问题列表 CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？ 以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示。 CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。 局部连接使网络可以提取数据的局部特征； 权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积； 池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。 为什么很多做人脸的Paper会最后加入一个Local Connected Conv？ 如果每一个点的处理使用相同的Filter，则为全卷积，如果使用不同的Filter，则为Local-Conv。 后接了3个Local-Conv层，这里是用Local-Conv的原因是，人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。 什么样的资料集不适合用深度学习? 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。 对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法? No Free Lunch定律：不存在一个通用普适的模型，对于所有的学习问题都能做到性能最佳。 对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。 也就是说：对于所有问题，无论学习算法A多聪明，学习算法 B多笨拙，它们的期望性能相同。 但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。 用贝叶斯机率说明Dropout的原理 Dropout as a Bayesian Approximation: Insights and Applications 何为共线性, 跟过拟合有啥关联? Multicollinearity－Wikipedia 共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。 共线性会造成冗余，导致过拟合。 解决方法：排除变量的相关性／加入权重正则。 说明如何用支持向量机实现深度学习(列出相关数学公式) 广义线性模型是怎被应用在深度学习中? A Statistical View of Deep Learning (I): Recursive GLMs ← The Spectator 深度学习从统计学角度，可以看做递归的广义线性模型。 广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。 深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者的困惑。下图是一个对照表： 什么造成梯度消失问题? 推导一下 How does the ReLu solve the vanishing gradient problem? - Quora Yes you should understand backprop – Medium 神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0。造成学习停止 Weights Initialization. 不同的方式，造成的后果。为什么会造成这样的结果。 几种主要的权值初始化方法：lecun_uniform / glorot_normal / he_normal / batch_normal lecun_uniform：Efficient BackProp (PDF Download Available) glorot_normal：Understanding the difficulty of training deep feedforward neural networks he_normal：Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification batch_normal：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift 为什么网络够深(Neurons 足够多)的时候，总是可以避开较差Local Optima？ [1412.0233] The Loss Surfaces of Multilayer Networks Loss. 有哪些定义方式（基于什么？）， 有哪些优化方式，怎么优化，各自的好处，以及解释。 Cross-Entropy / MSE / K-L散度 Dropout。 怎么做，有什么用处，解释。 How does the dropout method work in deep learning? - Quora An empirical analysis of dropout in piecewise linear networks Improving neural networks by preventing co-adaptation of feature detectors Activation Function. 选用什么，有什么好处，为什么会有这样的好处。 几种主要的激活函数：Sigmond / ReLU ／PReLU Deep Sparse Rectifier Neural Networks Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification 参考 如果你是面试官，你怎么去判断一个面试者的深度学习水平？ - 知乎 深度学习相关的职位面试时一般会问什么？会问一些传统的机器学习算法吗？ - 知乎","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankchen.xyz/tags/Deep-Learning/"}]},{"title":"在Windows上安装并加速拉取Docker镜像的方法","slug":"Docker-Windows-Speed-up","date":"2017-02-17T06:38:44.000Z","updated":"2017-02-17T06:53:01.000Z","comments":true,"path":"2017/02/17/Docker-Windows-Speed-up/","link":"","permalink":"http://frankchen.xyz/2017/02/17/Docker-Windows-Speed-up/","excerpt":"Docker容器服务近来可谓是一日千里爆炸式的发展，但是在国内安装Docker或者拉取庞大的Docker镜像都不可避免的遇到蜗牛式速度的问题，本文主要关于加速下载DockerWindows客户端和镜像的方法。","text":"Docker容器服务近来可谓是一日千里爆炸式的发展，但是在国内安装Docker或者拉取庞大的Docker镜像都不可避免的遇到蜗牛式速度的问题，本文主要关于加速下载DockerWindows客户端和镜像的方法。 下载Docker Windows客户端使用Daocloud提供的下载地址，基本上几分钟即可下载完100Mb左右的Docker Windows客户端，而用Docker官网的下载地址可谓几乎没有速度。 加速拉取Docker镜像不挂VPN直接拉取Docker镜像绝对是灾难性的体验，不仅慢如狗而且极容易中断连接。这里我们同样使用Daocloud提供的镜像站如这里，使用方法为在Docker右键菜单的设置选项内的DockerDaemon里面，加上一段地址改为如下形式， 123456&#123; &quot;registry-mirrors&quot;: [ &quot;http://59dcc468.m.daocloud.io&quot; ], &quot;insecure-registries&quot;: []&#125; 之后Docker会自动重启，之后再Pool镜像即可享受火箭加速般的提升！","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://frankchen.xyz/tags/Docker/"}]},{"title":"Explanation of Convolutional Neural Networks","slug":"Explanation-of-Convolutional-Neural-Networks","date":"2017-02-16T06:56:16.000Z","updated":"2017-02-17T12:16:54.000Z","comments":true,"path":"2017/02/16/Explanation-of-Convolutional-Neural-Networks/","link":"","permalink":"http://frankchen.xyz/2017/02/16/Explanation-of-Convolutional-Neural-Networks/","excerpt":"卷积神经网络(Convolutional Neural Network)，简称CNN，是机器视觉领域的主要神经网络模型。本文主要对CNN做一个简明全面的介绍。","text":"卷积神经网络(Convolutional Neural Network)，简称CNN，是机器视觉领域的主要神经网络模型。本文主要对CNN做一个简明全面的介绍。 上图是90年代的LeNet的结构，当时主要用于字符的识别，当前有许多的新的CNN结构，但是大体上都是采用了与LeNet类似的主要思想。本文主要以LeNet为例来做讲解。 LeNet的主要组成部分： 卷积层 非线性变换(ReLU) 池化层或者下采样 分类(全连接) 图片就是矩阵在讲卷积与卷积层之前，我们首先介绍一下图片的相关的概念。本质上，每张图片都是像素值的矩阵。 彩色图片具有三个通道，所谓的RGB图片，也就是在红色、绿色和蓝色三个颜色的矩阵，每个像素值都是[0,255]的值。灰度图片只有一个通道，所以只有一个矩阵。 卷积层CNN由卷积而得名，那么什么是卷积呢？先来看图片 首先我们有如下的5*5的矩阵，它的每个像素值都由0或者1组成 再来一张如下的3*3的矩阵 用这个33的矩阵对以上55的矩阵进行卷积，可以表示为如下的过程， 上述过程可以描述成，33的矩阵首先定位到55的矩阵的左上角，两者覆盖重叠到的部位进行点乘再求和，得到的值是新的矩阵的左上角第一个元素，接着33矩阵向右移动一个像素，也叫一个stride，进行下一次卷积操作，知道遍历完毕，得到一个新的矩阵。在CNN中，这个33的矩阵叫做滤波器或者卷积核，得到的新的矩阵叫做特征映射。可以说，卷积核的作用就是从原图片中发现特征。所以对于同一张图片，我们可以用不同的卷积核去操作，得到完全不同的特征映射。考虑如下的图片， 我们用不同的卷积核进行操作，得到的结果如下， 另一个例子如下， 如图，两个非常相似但是方向不同的卷积核对同一张图片进行卷积，得到的不同的结果。在训练过程中，CNN会自动学习到这些卷积核的参数，但是我们还有一些超参数需要手动确定，如卷积核个数、大小以及网络的结构等等。而特征映射的大小由三个参数来确定， Depth：也就是卷积核的个数，我们用几个卷积核进行操作，那么就可以得到多少个重叠着的特征映射，也就是特征映射的深度。 Stride：卷积时每步移动的大小，容易得知，步子越大，那么卷积的次数也就越小，那么特征映射矩阵也就越小。 Zero-Padding：指的是是否在原图像周围补上零再进行卷积，，这样也是控制特征映射大小的方法，带有Padding和不带有Padding的卷积分别叫做宽卷积和窄卷积。 非线性映射(ReLU)ReLU函数是什么？初一看它的全称Rectified Linear Unit好像特别高大上，其实它的函数形式非常简单，就是 $$output = max(0, input)$$ 也就是将原输入的负值全部替换为0。顺便一说，深度学习业界充斥着这种现象，如多层感知机与神经网络、甚至是Deep Learning这个称号本身都是这样改过来的“好名字”，不得不说，起个好名字对于成功也是非常主要的啊！ 话说回来，用ReLU处理过的图片的效果如下： 除ReLU之外，还有sigmoid函数和tanh双曲函数等非线性变换，但是ReLU被证明表现更加，所以也就更常用。 池化层池化(pooling)也就是信号处理领域的下采样Downsampling，其作用是对特征映射保留主要信息的同时进行降维。根据计算方式的不同可以有Max、Average、Sum等多种pooling方式。 以上展示了使用2*2的窗口对经过卷积和非线性映射的矩阵的Max pooling操作。这里的stride是2个像素，对每个区域采用最大值。在网络中，pooling是对每个特征映射单独操作的，于是我们会得到相同数目的输出。 下图展示了pooling操作， pooling操作显著地减小了输入的空间大小，除此之外， 使得输入更加小，更加容易计算 减小网络的复杂程度，有利于对抗过拟合 使得网络对于扰动等噪音更加鲁棒，因为微小的扰动对于pooling结果没有影响 有益于物体探测的准确性 全连接层经过两轮的卷积-ReLU-pooling操作，接下来就是标准的全连接神经网络结构了，所谓全连接，也就是上一层的每个神经元都与下一层的每个神经元有连接，倒数第二层的输出是每个类的概率，再经过一个softmax层进行归一化处理，我们得到了最终的输出：和为1的每个分类的概率。 训练过程上面把CNN拆开了讲，接下来我们把各个组件合起来看一下CNN的训练过程。比如我们的输入是一张船的图片，而输出则是属于船的概率是1，如[0,0,1,0]。 那么训练过程如下 步骤1：以随机值初始化卷积核和权重参数 步骤2：网络得到输入，进行前向的计算（卷积、ReLU和pooling）得到最终各分类的概率，假设是[0.1, 0.3, 0.2, 0.4]，因为参数都是随机的，那么结果肯定也是随机的 步骤3：利用平方误差计算错误的损失函数大小 步骤4：反向传播计算error对于各个参数的梯度，对参数进行更新，以减小错误程度 注意到我们只更新卷积核的参数以及全连接的权重，其余的参数已经在训练之前指定 在训练过程中，网络逐渐“学习”到如何正确分类图片，这是通过逐渐修改更新参数得到的 步骤5：对于所有训练集内的图片重复步骤2-4 CNN的可视化 CNN在人脸识别内的应用，如上图所示。对于CNN的原理，结合上图我们可以进行如下的解释：CNN的卷积核由下到上是逐渐探测复杂的特征的，比如第一层卷积核的特征映射是简单的明暗线段纹理，第二层就可以得到曲线等复杂一些的特征，到了第三层探测的特征已经是人脸的形状轮廓了，后面的层对于前面层的特征进行组合变化而得到更加复杂的组合。这就类似单层感知机连异或函数都无法表示，多层感知机组合起来也就是神经网络却可以拟合任意复杂的函数的原理一样。 这里有一个很棒的关于CNN原理的展示demo，如图 参考 An Intuitive Explanation of Convolutional Neural Networks","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankchen.xyz/tags/Deep-Learning/"},{"name":"CNN","slug":"CNN","permalink":"http://frankchen.xyz/tags/CNN/"}]},{"title":"Mac上安装Kaggle Docker 镜像的方法","slug":"Install-Kaggle-Docker-on-Mac","date":"2017-02-12T18:31:56.000Z","updated":"2017-02-17T06:40:55.000Z","comments":true,"path":"2017/02/13/Install-Kaggle-Docker-on-Mac/","link":"","permalink":"http://frankchen.xyz/2017/02/13/Install-Kaggle-Docker-on-Mac/","excerpt":"作为数据科学业界独占鳌头的竞赛网站Kaggle为我们提供了简便的一键安装的Docker镜像，使得我们可以方便的在本地使用与Kaggle网站在线完全一致的环境，那么在Mac上如何安装和使用Kaggle的Docker镜像呢？","text":"作为数据科学业界独占鳌头的竞赛网站Kaggle为我们提供了简便的一键安装的Docker镜像，使得我们可以方便的在本地使用与Kaggle网站在线完全一致的环境，那么在Mac上如何安装和使用Kaggle的Docker镜像呢？ 安装Docker参考Docker官网的教程可以方便的下载安装Docker在Mac上。 拉取镜像安装好Docker并运行成功后，参考这里，在终端用如下这行命令docker run --rm -it kaggle/python，就会下载kaggle的python环境Docker镜像到本地，大小数G，如果出现握手失败等提示可以尝试挂VPN或者用Daocloud的服务来加速。 jupyter notebook设置我们装好环境后当然不是只是为了在命令行内使用python环境，必须用jupyter notebook！那么怎么设置才能在本地的浏览器上连接容器内运行的内核呢？参考这里，在你的.bashrc或者.bash_profile文件内添加如下数行， 12345678910kpython()&#123; docker run -v $PWD:/tmp/working -w=/tmp/working --rm -it kaggle/python python &quot;$@&quot;&#125;ikpython() &#123; docker run -v $PWD:/tmp/working -w=/tmp/working --rm -it kaggle/python ipython&#125;kjupyter() &#123; (sleep 3 &amp;&amp; open &quot;http://$(docker-machine ip docker):8888&quot;)&amp; docker run -v $PWD:/tmp/working -w=/tmp/working -p 8888:8888 --rm -it kaggle/python jupyter notebook --no-browser --ip=&quot;0.0.0.0&quot; --notebook-dir=/tmp/working&#125; 别忘了source .bashrc，之后就可以用kjupyter命令来运行jupyter notebook，只要进入命令行中提供的网址如http://0.0.0.0:8888/?token=16185adf197d30dc82e9b88508a5ac585e6fa072c682117d就可进入jupyter notebook。","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://frankchen.xyz/tags/Data-Science/"},{"name":"Docker","slug":"Docker","permalink":"http://frankchen.xyz/tags/Docker/"}]},{"title":"Note for SoloLearn SQL","slug":"Note-for-SoloLearn-SQL","date":"2017-01-17T08:11:48.000Z","updated":"2017-01-17T15:57:03.000Z","comments":true,"path":"2017/01/17/Note-for-SoloLearn-SQL/","link":"","permalink":"http://frankchen.xyz/2017/01/17/Note-for-SoloLearn-SQL/","excerpt":"本文主要关于SQL的基本语法的笔记，来自SoloLearn。","text":"本文主要关于SQL的基本语法的笔记，来自SoloLearn。 基本概念介绍数据库数据库数据库是指以利于容易地连接、有效管理和更新的方式来管理的一系列的数据。数据库由储存相关联的信息的表格组成，举例来说，假设我们需要建立一个如YouTube的网站，那么我们需要数据库来存储视频信息、用户名与密码以及评论等等。 数据库表格数据库表格用类似Excel的方式存储和展示数据，数据库通常由许多的表格构成，例如想象一个由名字和电话号码构成的表格。 primary keyprimary key是表格中特殊的一列，主要特性是 每行都有独特的primary key 不能为null 例如下图中，ID是个primary key的好选择，因为可能会有重名的情况。 什么是SQL我们已经理解了什么是数据库，那么理解什么是SQL就很简单了。SQL 全称是结构化序列语言（Structured Query Language）。SQL用于连接和操作数据库，而MySQL指的是一种能理解SQL的程序语言。可以这么说，SQL是一种标准，而有许多遵循这个版本却自带许多特性的程序语言的实现。 SQL可以： 插入、更新、删除数据库里的记录 创建新的数据库、表格、存储程序和外观 从数据库取回数据 SQL语句 SELECT基本SQL语句SHOW DATABASES：返回服务器上所有数据库的序列 SHOW TABLES：返回当前数据库上的所有表格 SHOW COLUMNS FROM customers：返回选定表格内的列信息 SELECT 语句12SELECT column_listFROM table_name 在制定表格内选中一列或者多列 语法规则多行语句SQL允许同时运行多句语句 12SELECT FirstName FROM customers;SELECT City FROM customers; 大小写不敏感SQL是大小写不敏感的，如下三行是同样的效果 123select City from customers;SELECT City FROM customers;sElEct City From customers; 忽略空白whitespace和多行被忽略，以下语句是ok的 1234SELECT CityFROM customers; 选择多列选择多列以逗号分隔，可以同时选择多列 12SELECT FirstName, LastName, CityFROM customers; 选择所有列SELECT * FROM customers; DISTINCT和LIMITDISTINCT关键词DISTINCT关键词指排除重复项12SELECT DISTINCT column_name1, column_name2FROM table_name; LIMIT关键词指定返回的子集大小 123SELECT column listFROM table_nameLIMIT [number of records]; 如选择前五项 12SELECT ID, FirstName, LastName, CityFROM customers LIMIT 5; 选择从3后面的4项 12SELECT ID, FirstName, LastName, City FROM customers LIMIT 3, 4; 排序全名当要操作拥有相同列名字的多个表格时，可以使用全名如 123SELECT City FROM customers;SELECT customers.City FROM customers; Order ByOrder By用于与SELECT来排序数据用 Firstname排序 12SELECT * FROM customersORDER BY FirstName; 多列排序12SELECT * FROM customersORDER BY LastName, Age; 先用LastName排序，在LastName相同的情况下，用Age排序 Filtering, Functions, SubqueriesWHERE语句WHERE语句WHERE语句类似一个过滤器的作用123SELECT column_listFROM table_nameWHERE condition; SQL操作符比较操作符和逻辑操作符用于WHERE语句内 BETWEEN 操作符用于选中一定范围内的值 123SELECT column_name(s)FROM table_nameWHERE column_name BETWEEN value1 AND value2; Text 值需要用单引号123SELECT ID, FirstName, LastName, CityFROM customersWHERE City = &apos;New York&apos;; AND OR逻辑操作符逻辑操作符用于操作两个布尔值，返回true、false、或者null 123SELECT ID, FirstName, LastName, AgeFROM customersWHERE Age &gt;= 30 AND Age &lt;= 40; 12SELECT * FROM customersWHERE City = &apos;New York&apos; OR City = &apos;Chicago&apos;; 结合AND &amp; OR123SELECT * FROM customersWHERE City = &apos;New York&apos;AND (Age=30 OR Age=35); IN NOT ININ操作符用于需要将一列与许多值比较的情况，如用OR语句 1234SELECT * FROM customersWHERE City = &apos;New York&apos;OR City = &apos;Los Angeles&apos;OR City = &apos;Chicago&apos;; 用IN语句替代可简写为12SELECT * FROM customersWHERE City IN (&apos;New York&apos;, &apos;Los Angeles&apos;, &apos;Chicago&apos;); NOT IN就是反面情况 12SELECT * FROM customersWHERE City NOT IN (&apos;New York&apos;, &apos;Los Angeles&apos;, &apos;Chicago&apos;); Custom 列CONCAT语句CONCAT语句用于联合多个Text值，返回字符串 SELECT CONCAT(FirstName, &#39;, &#39; , City) FROM customers; AS 操作符AS 操作符用于生成新的列 12SELECT CONCAT(FirstName,&apos;, &apos;, City) AS new_columnFROM customers; 算数操作符 / 如将Salary自增50012SELECT ID, FirstName, LastName, Salary+500 AS SalaryFROM employees; 函数UPPER LOWER函数用于转换为大小写格式 12SELECT FirstName, UPPER(LastName) AS LastNameFROM employees; SQRT 、 AVGSQRT返回平方根 12SELECT Salary, SQRT(Salary)FROM employees; AVG返回均值 1SELECT AVG(Salary) FROM employees; SUM函数返回列的加和 1SELECT SUM(Salary) FROM employees; 子语句子语句是指含有其他语句的语句，假设我们需要选中全部薪水大于均值的人，那么可能需要先获取均值 SELECT AVG(Salary) FROM employees; 再 123SELECT FirstName, Salary FROM employeesWHERE Salary &gt; 3100ORDER BY Salary DESC; 其实可以结合起来 123SELECT FirstName, Salary FROM employeesWHERE Salary &gt; (SELECT AVG(Salary) FROM employees)ORDER BY Salary DESC; LIKE和MINLIKE函数LIKE用于在WHERE里指定条件 123SELECT column_name(s)FROM table_nameWHERE column_name LIKE pattern; 例如，选择所有A开头的人 12SELECT * FROM employeesWHERE FirstName LIKE &apos;A%&apos;; 选择所有s结尾名字的人12SELECT * FROM employeesWHERE LastName LIKE &apos;%s&apos;; MIN函数用于选择最小值 SELECT MIN(Salary) AS Salary FROM employees; JOIN、表操作合并表格对于以下两个表格 可用以下语句结合 1234SELECT customers.ID, customers.Name, orders.Name, orders.AmountFROM customers, ordersWHERE customers.ID=orders.Customer_IDORDER BY customers.ID; 结果 Join的类型可以用“小名”来简化join操作 1234SELECT ct.ID, ct.Name, ord.Name, ord.AmountFROM customers AS ct, orders AS ordWHERE ct.ID=ord.Customer_IDORDER BY ct.ID; JOIN类型 INNER JOIN LEFT JOIN RIGHT JOIN INNER JOIN等同于JOIN 123SELECT column_name(s)FROM table1 INNER JOIN table2ON table1.column_name=table2.column_name; 关系如图 LEFT JOIN 返回所有左边表格的行，甚至是在右边表格没有符合的也是如此。 基本语法为 123SELECT table1.column1, table2.column2...FROM table1 LEFT OUTER JOIN table2ON table1.column_name = table2.column_name; 举例来说，考虑以下两个依次是customers和items， 123SELECT customers.Name, items.NameFROM customers LEFT OUTER JOIN itemsON customers.ID=items.Seller_id; 结果 如果用RIGHT JOIN 123SELECT table1.column1, table2.column2...FROM table1 RIGHT OUTER JOIN table2ON table1.column_name = table2.column_name; 结果是 UNION当需要合并相似的表格时，可以用到UNION and UNION ALL，两者主要区别是前者会丢弃重复项。 如考虑以下两个表格 使用语句123SELECT ID, FirstName, LastName, City FROM FirstUNIONSELECT ID, FirstName, LastName, City FROM Second; 结果为 而使用UNION ALL 123SELECT ID, FirstName, LastName, City FROM FirstUNION ALLSELECT ID, FirstName, LastName, City FROM Second; 结果为 INSERT语句插入语句INSERT语句用于在表格插入一条数据 12INSERT INTO table_nameVALUES (value1, value2, value3,...); 也可以指定插入列 12INSERT INTO table_name (column1, column2, column3, ...,columnN) VALUES (value1, value2, value3,...valueN); 亦可只插入指定列 12INSERT INTO table_name (column1, column2, column3, ...,columnN) VALUES (value1, value2, value3,...valueN); UPDATE、DELETE语句UPDATE语句用于更改表格 123UPDATE table_nameSET column1=value1, column2=value2, ...WHERE condition; 例如 123UPDATE EmployeesSET Salary=5000WHERE ID=1; 也可以同时更改数项 123UPDATE EmployeesSET Salary=5000, FirstName=&apos;Robert&apos;WHERE ID=1; DELETE用于删除数据 12DELETE FROM table_name WHERE condition; 创建表格CREATE TABLE 用于创建表格 基本语法 12345678CREATE TABLE table_name(column_name1 data_type(size),column_name2 data_type(size),column_name3 data_type(size),....columnN data_type(size)); column_names指定列名 data_type指定存储数据的类型如int size指定最大长度 如 1234567CREATE TABLE Users( UserID int, FirstName varchar(100), LastName varchar(100), City varchar(100)); varchar指的是字符类型 数据类型数值INT - 带符号或者非符号integerFLOAT(M,D) - 带符号浮点数M是展示长度，D是小数点位DOUBLE(M,D) - 带符号long类型浮点数M是展示长度，D是小数点位 日期和时间DATE - YYYY-MM-DD 格式DATETIME - YYYY-MM-DD HH:MM:SS 格式TIMESTAMP - 从 1970年1月1日午夜开始计算的时间长度TIME - Stores the time in HH:MM:SS format. 字符串CHAR(M) - 固定长度的字符串，M是长度，最大为255 byte.VARCHAR(M) - 可变长度字符串，M是最大长度BLOB - “二进制大型对象” Binary Large Objects， 用于存储图片等TEXT - 大规模的text数据 指定primary key 12345678CREATE TABLE Users( UserID int, FirstName varchar(100), LastName varchar(100), City varchar(100), PRIMARY KEY(UserID)); NOT NULL 和 AUTO_INCREMENTSQL约束NOT NULL - 强制列不能含有NULLUNIQUE - 不允许列中出现重复值PRIMARY KEY - 强制表格对于特定的列接受数据并创建单独的加速索引CHECK - 通过逻辑表达式来判断值是否有效DEFAULT - 往表格插入数据时，若某列未指定值，则自动插入默认值 例如，如下语句指name列不允许出现NULL name varchar(100) NOT NULL AUTO INCREMENT当新一条记录加入时，自动生成某数，通常从1开始，每次新纪录加入，自增1。 12UserID int NOT NULL AUTO_INCREMENT,PRIMARY KEY (UserID) ALTER DROP RANAMEALTER用于对表格中的列进行添加、删除和编辑操作，也可以对表格的约束进行添加或移除。 ALTER TABLE People ADD DateOfBirth date; DROP用于删除整列 12ALTER TABLE PeopleDROP COLUMN DateOfBirth; 要删除整个表格 DROP TABLE People; RENAME用于列改名或者对表格改名 将People的FirstName列改名为name 12ALTER TABLE PeopleCHANGE FirstName name varchar(100); 将整个表格改名 RENAME TABLE People TO Users; 视图为了更加方便的组织和操作数据库，经常需要使用视图。视图包含行和列，就像一个真实的表。视图中的字段就是来自一个或多个数据库中的真实的表中的字段。我们可以向视图添加 SQL 函数、WHERE 以及 JOIN 语句，我们也可以提交数据，就像这些来自于某个单一的表。 创建语句如 1234CREATE VIEW view_name ASSELECT column_name(s)FROM table_nameWHERE condition; 假设一表格，如 我们要创建一个视图来展示职工的名字和薪水 123CREATE VIEW List ASSELECT FirstName, SalaryFROM Employees; 接下来可以类似展示表格一样展示视图 SELECT * FROM List; 可以用以下语句来更新视图 1234CREATE OR REPLACE VIEW view_name ASSELECT column_name(s)FROM table_nameWHERE condition; 例如 123CREATE OR REPLACE VIEW List ASSELECT FirstName, LastName, SalaryFROM Employees; 丢弃视图DROP VIEW List;","categories":[],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://frankchen.xyz/tags/SQL/"},{"name":"Note","slug":"Note","permalink":"http://frankchen.xyz/tags/Note/"}]},{"title":"Note of Recommendation System in Action","slug":"Action","date":"2017-01-15T16:19:00.000Z","updated":"2017-01-15T16:25:13.000Z","comments":true,"path":"2017/01/16/Action/","link":"","permalink":"http://frankchen.xyz/2017/01/16/Action/","excerpt":"本文主要关于项亮的《推荐系统实践》的笔记。","text":"本文主要关于项亮的《推荐系统实践》的笔记。 推荐系统的评测方式以下也是一项最终能上线的推荐算法的依次测试顺序， 离线评测 将数据集分为训练集和测试集，离线对算法进行评测 用户调查 在上线之前通过调查得到用户满意度的信息 在线评测 进行AB测试，对比推荐算法指标 推荐系统的评测指标 满意度 是推荐系统的最重要标准 无法离线计算，只能通过用户调查和在线实验得到 可通过停留时间、点击率和转化率来统计 预测准确度 最重要的离线测试标准 主要方法是，在离线数据集内的训练集训练的结果与测试集对比，比较重合度。 又分为评分预测和Top N推荐 对于评分预测，有均方根误差（RMSE）和平均绝对误差（MAE）计算两种方式，相差不大，前者对于偏离项惩罚大，后者对于评分取整的情况会降低误差。 对于Top N推荐，通常通过准确率和招呼率来评测 假设$R(u)$和$T(u)$分别是训练集和测试集上的推荐，那么两者的交集长度除以$R(u)$的长度就是precision，交集长度除以$T(u)$就是recall。 评分预测关注预测用户看了电影后会给电影什么样的评分，而Top N是找到用户最有可能感兴趣的电影 覆盖率coverage 粗略定义为推荐系统发掘的物品占全部物品的比率，精确定义为物品流行度与全部物品流行度的比率 可通过信息熵和基尼系数来计算 两者都是计算不同物品流行度之间的平衡度 推荐系统一般具有马太效应：强者更强 多样性 要覆盖满足用户广大的兴趣，可以用不同的物品相似度度量函数来定义不同的多样性 多样性和相似性是trade off的，需要达到一定的平衡，让推荐效果最好 新颖性 推荐用户未见过的物品，最简单的是推荐流行度低的物品 难点在于不牺牲精度的前提下提高多样性和新颖性 惊喜度 推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。 信任度 用户对推荐系统的信任度高，能增加用户与推荐系统的交互 提升用户对推荐系统的信任度的方法 增加推荐系统的透明度：对用户的推荐需要进行解释 提高用户的社交关系进行推荐 实时性 实时更新追踪用户的行为和状态的变化 能够将新加入的物品推荐给用户 鲁棒性 反作弊性 作弊方法 行为注入攻击 评分系统攻击：大批给某物品打高分 如何对抗作弊 采用作弊代价高的行为作为推荐系统采纳的数据 使用数据前进行检测、清理 商业目标 不同利益方关注不同","categories":[],"tags":[{"name":"Recommendation System","slug":"Recommendation-System","permalink":"http://frankchen.xyz/tags/Recommendation-System/"}]},{"title":"Sort Algorithms of Linear Time Complexity","slug":"Sort-Algorithms-of-Linear-Time-Complexity","date":"2017-01-13T12:05:31.000Z","updated":"2017-01-14T10:48:52.000Z","comments":true,"path":"2017/01/13/Sort-Algorithms-of-Linear-Time-Complexity/","link":"","permalink":"http://frankchen.xyz/2017/01/13/Sort-Algorithms-of-Linear-Time-Complexity/","excerpt":"常用排序算法如归并、快排、堆排序等基于比较的排序方法都是不能突破$O(n \\log n)$时间复杂度的，本文着重介绍几种线性时间复杂度的排序方法。","text":"常用排序算法如归并、快排、堆排序等基于比较的排序方法都是不能突破$O(n \\log n)$时间复杂度的，本文着重介绍几种线性时间复杂度的排序方法。 基于比较的排序为何不能突破$O(n \\log n)$时间复杂度？因为$N$个数有$N!$个可能的排列情况，也就是说基于比较的排序算法的判定树有$N!$个叶子结点，比较次数至少为$\\log(N!)= O(N \\log N)$ (斯特林公式) 而线性复杂度的排序方法通常来说有计数排序、桶排序和基数排序三种。 计数排序计数排序（Counting sort）是一种稳定的线性时间排序算法，Θ(n + k)的时间复杂度。n 是待排序数组的大小，k 是辅助数组 count 的大小。 因为它并不是基于比较的的排序算法，所以它没有O(NlogN)的下限。并且在一定的条件下，使用该算法比使用快排能带来更好的效率。例如在基数排序中就运用了计数排序，因为它只需要额外的10个元素大小的辅助数组，线性的效率，并保证了稳定性。 原理计数排序的主要思想是将待排序元素的值作为下标，利用辅助数组 count 记录所有的元素出现的次数。即 count[i] 的值代表元素 i 在原始数组中出现的次数。这样，原始数组的所有元素就被有有序地记录下来。 当然，只知道某个元素出现的次数是不足以排序的。仔细观察 count 数组，发现对 count 数组进行累加（count[i] += count[i-1]）后，count[i] 的含义就变成了原始数组 i 前面有 count[i]个数是不大于它的。最后就可以根据 count 数组中的排名去构造一个已排序的数组了。 下面以原始数组d = {8, 13, 0, 3, 20, 16, 9, 7, 11, 5} 去演示上述的过程： 或者是数组中有重复元素的情况： 特点计数排序虽然效率高，却有其受限的条件。 只能对[0, K]区间的非负整数进行排序。当 K 很大时，空间复杂度会变得很大。所以，如果是对于[0, 100]这样的区间，计数排序是个好的选择。考虑到空间复杂度的优化，也可以用原始数组中的 (最大值 - 最小值 + 1) 作为 count 数组的大小。 参考代码1234567891011121314151617181920212223242526/* 数组 d 的元素只能位于[0, K)区间的非负整数，O(n + K)*/void counting_sort(int d[], int n)&#123; int max = 0, min = Integer.MAX_VALUE; for (int i = 0; i &lt; n; i++) &#123; if (max &lt; d[i]) max = d[i]; if (min &gt; d[i]) min = d[i]; &#125; max++; int[] count = new int[max - min; //优化空间 int[] ans = new int[n]; for (int i = 0; i &lt; max; ++i) count[i] = 0; for (int i = 0; i &lt; n; ++i) count[d[i] - min]++; for (int i = 1; i &lt; max; ++i) count[i] += count[i - 1]; //这里必须逆序循环，不然会造成排序结果失去稳定性。 for (int i = n - 1; i &gt;= 0; --i) ans[ --count[d[i]- min] ] = d[i]; return ans;&#125; 基数排序桶排序","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankchen.xyz/tags/Algorithm/"},{"name":"Java","slug":"Java","permalink":"http://frankchen.xyz/tags/Java/"}]},{"title":"Polymorphism of Java","slug":"Polymorphism-of-Java","date":"2017-01-13T10:15:27.000Z","updated":"2017-01-13T14:49:38.000Z","comments":true,"path":"2017/01/13/Polymorphism-of-Java/","link":"","permalink":"http://frankchen.xyz/2017/01/13/Polymorphism-of-Java/","excerpt":"最近面试被问到Java的多态怎么理解，自己含含糊糊的感觉自己知道点含义可是就是具体怎么说打不出来，尴尬死。接下来打算把三大特性：封装、继承、多态都好好的写下来，以参考。本文先写关于Java的多态吧。","text":"最近面试被问到Java的多态怎么理解，自己含含糊糊的感觉自己知道点含义可是就是具体怎么说打不出来，尴尬死。接下来打算把三大特性：封装、继承、多态都好好的写下来，以参考。本文先写关于Java的多态吧。 多态，用一句话来说，就是事物在运行过程中存在不同的状态。多态的存在有三个前提: 要有继承关系 子类要重写父类的方法 父类引用指向子类对 实验我们先定义一个父类Animal 12345678910111213141516class Animal&#123; int num = 10; int age = 20; public void eat()&#123; System.out.println(\"动物在吃饭\"); &#125; public stastic void sleep()&#123; System.out.println(\"动物在睡觉\"); &#125; public void run()&#123; System.out.println(\"动物在奔跑\"); &#125;&#125; 再定义一个子类Cat 1234567891011121314151617class Cat extends Animal&#123; int num = 30; int age = 40; String name = \"Tom\"; public void eat()&#123; System.out.println(\"猫在吃饭\"); &#125; public stastic void sleep()&#123; System.out.println(\"猫在睡觉\"); &#125; public void catchMouse()&#123; System.out.println(\"猫在抓老鼠\"); &#125;&#125; 再来一个测试 12345678910111213class Demo_Test&#123; public stastic void main(String[] args)&#123; Animal am = new Cat(); am.eat(); am.sleep(); am.run(); System.out.println(am.num); System.out.println(am.age); am.catchMouse(); System.out.println(am.name); &#125;&#125; 先来看看上面是否满足三个条件： 子类Cat继承了父类Animal； 子类重写了父类的eat()、sleep()方法，其中前者是非静态，后者是静态方法； 在堆内存中开辟了子类Cat()对象，并把存在于栈内存中的Animal()引用指向这个对象。 然后我们看一下运行结果，应该是 猫在吃饭动物在睡觉动物在奔跑1020 那么为什么呢？ 子类重写了父类的非静态成员方法，对象am.eat()的输出结果是“猫在吃饭”； 子类重写了父类的静态成员方法，对象am.sleep()的输出结果是“动物在睡觉”； 未被子类重写的方法am.run()的输出结果是“动物在奔跑”。 那么可以总结出多态成员访问的特点： 成员变量：编译看左边(父类),运行看左边(父类) 成员方法：编译看左边(父类)，运行看右边(子类)。动态绑定 静态方法：编译看左边(父类)，运行看左边(父类)(静态和类相关，算不上重写，所以，访问还是左边的) 只有非静态的成员方法,编译看左边,运行看右边，而以上的这个过程就叫做多态的向上转型。 多态的缺点多态也存在缺点，也就是无法尝试调用子类特有的方法，如上面未展示am.catchMouse();System.out.println(am.name);两行代码都是会报错的，因为这是子类特有的成员方法和成员属性。 如果我们需要调用子类的特有成员方法和成员属性，那么应该将这个父类强制转换为子类类型，如 1234567891011121314151617181920class Demo_Test &#123; public static void main(String[] args) &#123; Animal am = new Cat(); am.eat(); am.sleep(); am.run();// am.catchMouse();// System.out.println(am.name); System.out.println(am.num); System.out.println(am.age); System.out.println(\"------------------------------\"); Cat ct = (Cat)am; ct.eat(); ct.sleep(); ct.run(); ct.catchMouse(); &#125; &#125; 执行强制类型转换Cat ct = (Cat)am后，这时ct就指向堆里最先创建的Cat()对象了，自然能使用Cat类的一切成员方法和成员属性了。这也是多态的魅力，为了使用子类的某些方法不需要重新再开辟内存。以上就是多态中的向下转型。 通俗解释花木兰替父从军大家都知道花木兰替父从军的例子，花木兰替父亲花弧从军。那么这时候花木兰是子类，花弧是父类。花弧有自己的成员属性年龄，姓名，性别。花木兰也有这些属性，但是很明显二者的属性完全不一样。花弧有自己的非静态成员方法‘骑马杀敌’，同样花木兰也遗传了父亲一样的方法‘骑马杀敌’。花弧还有一个静态方法‘自我介绍’，每个人都可以问花弧姓甚名谁。同时花木兰还有一个自己特有的非静态成员方法‘涂脂抹粉’。但是，现在花木兰替父从军，女扮男装。这时候相当于父类的引用（花弧这个名字）指向了子类对象（花木兰这个人），那么在其他类（其他的人）中访问子类对象（花木兰这个人）的成员属性（姓名，年龄，性别）时，其实看到的都是花木兰她父亲的名字（花弧）、年龄（60岁）、性别（男）。当访问子类对象（花木兰这个人）的非静态成员方法（骑马打仗）时，其实都是看到花木兰自己运用十八般武艺在骑马打仗。当访问花木兰的静态方法时（自我介绍），花木兰自己都是用她父亲的名字信息在向别人作自我介绍。并且这时候花木兰不能使用自己特有的成员方法‘涂脂抹粉’。—–多态中的向上转型那么终于一将功成万骨枯，打仗旗开得胜了，花木兰告别了战争生活。有一天，遇到了自己心爱的男人，这时候爱情的力量将父类对象的引用（花弧这个名字）强制转换为子类对象本来的引用（花木兰这个名字），那么花木兰又从新成为了她自己，这时候她完全是她自己了。名字是花木兰，年龄是28，性别是女，打仗依然那样生猛女汉子，自我介绍则堂堂正正地告诉别人我叫花木兰。OMG！终于，终于可以使用自己特有的成员方法‘涂脂抹粉’了。从此，花木兰完全回到了替父从军前的那个花木兰了。并且和自己心爱的男人幸福的过完了一生。—–多态中的向下转型。 参考出处 作者：程序狗链接：https://www.zhihu.com/question/30082151/answer/120520568来源：知乎","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://frankchen.xyz/tags/Java/"},{"name":"OOP","slug":"OOP","permalink":"http://frankchen.xyz/tags/OOP/"}]},{"title":"Map 0f Java","slug":"Map-0f-Java","date":"2017-01-13T05:53:14.000Z","updated":"2017-01-13T11:37:57.000Z","comments":true,"path":"2017/01/13/Map-0f-Java/","link":"","permalink":"http://frankchen.xyz/2017/01/13/Map-0f-Java/","excerpt":"整理关于Java的Map相关的知识，最近面试有被问到关于此类数据结构的知识，故写下来总结一番。","text":"整理关于Java的Map相关的知识，最近面试有被问到关于此类数据结构的知识，故写下来总结一番。 List和Map都是java.util里面有关集合的数据结构类。Map主要的功能是元素对（键值对）的形式，每个键都映射到一个类。接下来我们介绍Map的分类、初始化、遍历的方式、排序方法以及常用API。 Map类Java中的Map类可分为三种： 通用Map：用于在应用程序内管理映射，通常在java.utils内 HashMap、 Hashtable、 Properties、 LinkedHashMap、 IdentityHashMap、 TreeMap、WeakHashMap、 ConcurrentHashMap 专用Map：不必我们自己创建，而是通过其他应用程序访问 java.util.jar.Attribute、javax.print.attribute.standard.PrinterStateReasons、java.security.Provider、java.awt.RenderingHints、javax.swing.UIDefaults 自定义Map 常用类型的区别 HashMap 最常用的Map，速度快，非同步（不支持线程同步） 允许一条键为null：多了覆盖 允许多条值为null TreeMap 用Iterator遍历的时候返回以key为序的序列 故不允许key为空 非同步 HashTable 类似HashMap，但是写入速度慢 因为支持线程同步：同一时刻只能有一线程能写HashTable LinkedHashMap 保持了插入顺序，Iterator遍历时先进先出 key、value都允许为空 用法1234567891011//建立HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();//寻值map.get(\"key1\");//插入map.put(\"key2\", \"value2\");//移除map.remove(\"key1\");//清空map.clear();// Map的遍历初始化123Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();map.put(\"key1\", \"value1\");map.put(\"key2\", \"value2\"); 分别使用增强for循环和Iterator来遍历 增强for循环遍历KeySet遍历123for (String key: map.keySet())&#123; System.out.println(key + \": \" + map.get(key));&#125; entrySet遍历12for (Map.Entry&lt;String, String&gt; entry: map.entrySet())&#123; System.out.println(entry.getKey() + \": \" + entry.getValue()); Iterator遍历KeySet遍历12345Iterator&lt;String&gt; iterator = map.keySet().iterator();while (iterator.hasNext())&#123; String key = iterator.next(); System.out.println(key + \": \" + map.get(key));&#125; entrySet遍历12345Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = map.entrySet().iterator();while (iterator.hasNext()) &#123; Map.Entry&lt;String, String&gt; entry = iterator.next(); System.out.println(entry.getKey() + \"：\" + entry.getValue());&#125; 遍历性能就实验比较， 增强for循环使用方便，但性能较差，不适合处理超大量级的数据。 迭代器的遍历速度要比增强for循环快很多，是增强for循环的2倍左右。 使用entrySet遍历的速度要比keySet快很多，是keySet的1.5倍左右。 排序TreeMap排序，若是升序只需要按序输出即可，降序需要改写比较器： 123456789101112Map&lt;String, String&gt; map = new TreeMap&lt;String, String&gt;(new Comparator&lt;String&gt;() &#123; public int compare(String obj1, String obj2) &#123; return obj2.compareTo(obj1);// 降序排序 &#125;&#125;);map.put(\"a\", \"c\");map.put(\"b\", \"b\");map.put(\"c\", \"a\");for (String key : map.keySet()) &#123; System.out.println(key + \" ：\" + map.get(key));&#125; HashMap\\HashTable\\LinkedHashMap排序 1234567891011121314151617Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();map.put(\"a\", \"c\");map.put(\"b\", \"b\");map.put(\"c\", \"a\");// 通过ArrayList构造函数把map.entrySet()转换成listList&lt;Map.Entry&lt;String, String&gt;&gt; list = new ArrayList&lt;Map.Entry&lt;String, String&gt;&gt;(map.entrySet());// 通过比较器实现比较排序Collections.sort(list, new Comparator&lt;Map.Entry&lt;String, String&gt;&gt;() &#123; public int compare(Map.Entry&lt;String, String&gt; mapping1, Map.Entry&lt;String, String&gt; mapping2) &#123; return mapping1.getKey().compareTo(mapping2.getKey()); &#125;&#125;);for (Map.Entry&lt;String, String&gt; mapping : list) &#123; System.out.println(mapping.getKey() + \" ：\" + mapping.getValue());&#125; 常用API method 作用 clear() 从 Map 中删除所有映射 remove(Object key) 从 Map 中删除键和关联的值 put(Object key, Object value) 将指定值与指定键相关联 putAll(Map t) 将指定 Map 中的所有映射复制到此 map entrySet() 返回 Map 中所包含映射的 Set 视图。Set 中的每个元素都是一个 Map.Entry 对象，可以使用 getKey() 和 getValue() 方法（还有一个 setValue() 方法）访问后者的键元素和值元素 keySet() 返回 Map 中所包含键的 Set 视图。删除 Set 中的元素还将删除 Map 中相应的映射（键和值） values() 返回 map 中所包含值的 Collection 视图。删除 Collection 中的元素还将删除 Map 中相应的映射（键和值） get(Object key) 返回与指定键关联的值 containsKey(Object key) 如果 Map 包含指定键的映射，则返回 true containsValue(Object value) 如果此 Map 将一个或多个键映射到指定值，则返回 true isEmpty() 如果 Map 不包含键-值映射，则返回 true size() 返回 Map 中的键-值映射的数目","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://frankchen.xyz/tags/Java/"},{"name":"Data Structure","slug":"Data-Structure","permalink":"http://frankchen.xyz/tags/Data-Structure/"}]},{"title":"To be a great commander","slug":"good commander","date":"2017-01-11T07:19:05.000Z","updated":"2017-01-11T07:26:06.000Z","comments":true,"path":"2017/01/11/good commander/","link":"","permalink":"http://frankchen.xyz/2017/01/11/good commander/","excerpt":"近来偶然读到林彪元帅关于如何进行军事工作写的工作总结，不光是军事，应用到日常工作、做事也是很好的指导，这是林彪不到30岁时候写下的文字，顿时明白为何人家23岁就可以当军长的原因。","text":"近来偶然读到林彪元帅关于如何进行军事工作写的工作总结，不光是军事，应用到日常工作、做事也是很好的指导，这是林彪不到30岁时候写下的文字，顿时明白为何人家23岁就可以当军长的原因。 杨成武回忆：1936年12月,西安事变爆发，红军大学第一期毕业。即将奔赴前线，林彪找我谈话，给我留下很深的印象。我过去当政委，还想干老本行。但军 委经过全面考虑,确定我当师长。林彪说毛主席要我和你谈一次话，讲一下怎样当好师长的问题。谈话时罗瑞卿也在场,杨成武认真作了记录。林彪讲了九条,这也 是他自己在战争中的体会： 一、要勤快。不勤快的人办不好事情，不能当好军事指挥员。应该自己干的事情一定要亲自过目，亲自动手。比如，应该上去看的山头就要爬上去，应该了解的情况 就要及时了解，应该检查的问题就要严格检查。不能懒，军事指挥员切忌懒，因为懒会带来危险，带来失败。比方说，一个军事指挥员，到了宿营地就进房子，搞水 洗脸洗脚，搞鸡蛋煮面吃，吃饱了就睡大觉。他对住的村子有多大，在什么位置，附近有几个山头周围有几条道路，敌情怎么样，群众条件怎么样，可能发生什么情 况，部队到齐了没有，哨位在什么地方，发生紧急情况时的处置预案如何，都不过问，都不知道。这样，如果半夜三更发生了情况，敌人来个突然袭击，就没有办法 了。到那种时候，即使平时很勇敢的指挥员，也会束手无策，只好三十六计，跑为上计，结果，变成一个机会主义者。机会主义和打败仗，常常是因为没有思想准 备，没有组织准备，工作没有做到家，懒的结果。因此，不论大小指挥员都要勤快，要不惜走路，不怕劳累，要多用脑子，要做到心到、眼到、口到、脚到、手到。 事情没有做好以前，不能贪闲。贪闲就隐伏着犯错误的根子。什么事都要心中有底，“凡事预则立，不预则废”。雷打不动的干部，牛皮糖式的干部，不管有多大本 事，都不是好干部。 二、要摸清上级的意图。对上级的意图要真正理解，真正融会贯通，真正认识自己所受领的任务在战役、战斗全局中的 地位和作用。这样，才能充分发挥自己的主观能动性；才能打破框框，有敢于和善于在新情况中找到新办法的创造性；才能有大勇，才能决心强、决心狠，敢于彻底 胜利，有强烈的吞掉敌人的企图和雄心。指挥员的勇敢集中表现在歼敌决心的坚定顽强上面。指挥员的大勇建立在革命的最高自觉性和正确理解上级意图的基础上 面。 三、要调查研究。对于敌情、地形、部队的情况和社会情况，要经常做到心中有数。要天天摸，天天琢磨，不能间断。这样做，不能看 作是重复，实际上这不是重复，而是不断深化不断提高的过程，是取得正确认识的必不可少的手段。平时积累掌握的情况越多，越系统，在战时，特别是在紧张复杂 的情况下，就越沉着，越有办法。急中生智的“智”，才有基础。因此，调查研究工作要贯串在各项工作中，要贯串在每一次战役、战斗的整个过程，反对打莽撞 仗、糊涂仗，反对急性病，反对不亲自动手做调查研究的懒汉作风。特别是敌情，必须切实摸透。因为敌情是活的，敌人必然会极力隐蔽、伪装他们的真实企图和行 动。要尽一切可能不间断地侦察，查清敌人的部署和动向，看他扮演什么角色？是主角还是配角？是主力还是非主力？是骄兵还是败兵？能集中多大兵力向我们进攻 和阻挡我们的进攻。查明敌主官的特性，看他惯用和擅长用什么战法，根据他当前的企图判断他可能采用什么打法，等等。只要摸清了敌情、我情、地形的底，决心 就快，就硬，就坚定。就不会被任何假象所迷惑，就不会被任何困难所吓住。如果情况不清，就会犹豫不决，举棋不定，坐失良机，或者勉强下了决心，一遇风吹草 动，听到畏难叫苦和不正确的建议，就容易动摇，可能一念之差，前功尽弃。 四、要有个活地图。指挥员和参谋必须熟悉地图，要经常读地 图。熟读地图可以产生见解，产生智慧，产生办法，产生信心。读的方法是把图挂起来，搬个凳子坐下来，对着地图看，从大的方向到活动地区，从地区全貌到每一 地段的地形特点，从粗读到细读，逐块逐块地读，用红蓝铅笔把主要的山脉、河流、城镇、村庄、道路标划出来，边读，边划，等到地图差不多快划烂了，也就差不 多把地图背熟了，背出来了。在熟读地图的基础上，要亲自组织有关指挥员和参谋对作战地区和战场进行实地勘察，核正地图，把战场的地形情况和敌我双方的兵力 部署都装至脑子里去，做到闭上眼睛面前就有一幅鲜明的战场图影，离开地图也能指挥作战。这样，在你死我活、瞬息万变的战斗情况下，可以比敌人来得快，争取 先机，先敌一着，掌握主动，稳操胜券。 五、要把各方面的问题想够想透。每一次战役、战斗的组织，要让大家提出各种可能出现的问题， 要让大家来找答案，而且要从最坏的最严重的情况来找答案。把所有提出来的问题都回答了，再没有问题没有回答的了，这样，打起仗来才不会犯大错误，万一犯了 错误，也比较容易纠正。没有得到答案的问题，不能因为想了很久想不出来就把它丢开，留下一个疙瘩。如果这样，是很危险的，在紧要关头，这个疙瘩很可能冒出 来，就会使你们心中无数，措手不及。当然，在战争环境中，要考虑的问题很多，不可能一次都提完，也不可能一次都回答完，整个战役、战斗的过程，就是不断提 出问题和不断回答问题的过程。有时脑子很疲劳，有的问题可能立即回答不了。这时，除了好好地和别人商量以外，就好好地睡一觉，睡好了，睡醒了，头脑清醒 了，再躺在床上好好想一想，就可能开窍，可能想通了，回答了，解决了。总之，对每一个问题不能含糊了事。问题回答完了，战役、战斗的组织才算完成。 六、要及时下达决心。在什么样的情况下可以下决心打呢？指挥员必须以最大努力组织战役、战斗的准备工作，力求确有把握才动手，不打无把握之仗。但是任何 一次战斗都不可能完全具备各种条件，不可能有百分之百的把握。一般说有百分之七十左右的把握，就很不错了，就要坚决地打，放手地打。不足的条件，要通过充 分发挥人的因素的作用，依靠人民群众的力量，充分发挥人民军队特有的政治上的优势，充分发挥指战员的智慧和英勇顽强的战斗作风来弥补，以主观努力来创造条 件，化冒险性为创造性，取得胜利。 七、要有一个很好的很团结的班子。领导班子思想认识要一致，行动要协调、合拍，要雷厉风行，要有革命英雄主义的气概。都要勤快，都千方百计地办好事情，完成任务。不互相扯皮，不互相干扰，不抱旁观者的态度。如果领导班子不好，人多不但无用，反而有害。 八、要有一个很好的战斗作风。有好的战斗作风的部队才能打好仗，打胜仗。好的战斗作风首先是不叫苦，抢着去担负最艰巨的任务，英勇顽强，不怕牺牲，猛打 猛冲猛追。特别是要勇于穷追。因为把敌人打垮以后，追击是解决战斗、扩大战果、彻底歼灭敌人最关键的一招。在追击时，要跑步追，快步追，走不动的扶着拐棍 追，就是爬、滚，也要往前追，只有抓住敌人，才能吃掉敌人。好的战斗作风要靠平时养成，要靠实际锻炼，要在紧张、残酷的战斗中才能锻炼出来。不敢打硬仗、 恶仗的部队，让他打几次就打出来了，因为已经见识过硬仗、恶仗的场面，有了体会，有了经验，知道怎么打了，百炼成钢就是这个道理。做工作也要有好的作风， 说了就要做，说到那里做到那里，要做得干脆利索，要一竿子插到底，一点不含糊，不做好不撒手。好的作风的养成，关键在于干部。强将手下无弱兵，干部的作风 怎么样，部队的作风就会怎么样。因此，首先要抓好干部，要干部做出样子，影响带动部队。只要干部作风好，指挥好战斗，多打胜仗，即使是新建的部队或者原来 基础较弱的部队，也会很快打出好作风来，像铁锤一样，砸到那里，那里就碎。 九、要重视政治，亲自做政治工作。部队战斗力的提高 要靠平时坚强的党的领导、坚强的政治工作。连队的支部一定要建设好，支部的工作要做活，就是要把所有党团员的革命劲头鼓得足足的，充分发挥他们的模范作 用、带头作用，通过他们把全连带动起来，通过他们去做政治工作，提高全体指战员的阶级觉悟。有了坚强的党支部的领导，有了坚强的政治工作，就可以做到一呼 百应，争先恐后，不怕牺牲，前赴后继。战术、技术也要练好，特别是技术，如果枪打不准，战场上就不能消灭敌人，就不能解决战斗。因此，军事训练不能马虎， 党政工作要领导好训练。艺高人胆大，胆大艺更高，部队有了高度的无产阶级觉悟，有了好的战斗作风，再加上过硬的作战本领，就如虎添翼，就可以无敌于天下。","categories":[],"tags":[{"name":"life","slug":"life","permalink":"http://frankchen.xyz/tags/life/"}]},{"title":"logistic regression的公式手推相关","slug":"logistic-regression","date":"2017-01-09T07:56:59.000Z","updated":"2017-01-09T13:25:11.000Z","comments":true,"path":"2017/01/09/logistic-regression/","link":"","permalink":"http://frankchen.xyz/2017/01/09/logistic-regression/","excerpt":"本文有关logistic regression的公式相关的手推，包括假说函数、极大似然估计以及梯度下降算法。","text":"本文有关logistic regression的公式相关的手推，包括假说函数、极大似然估计以及梯度下降算法。 Hypothesis首选我们要明确，logistic regression虽然叫“回归”，但是它却是用来做分类的，也是最最常用的分类机器学习算法。其假说模型为： $$h_{\\theta}(x) = g(\\theta ^T x)$$ 其中$g(z) = \\frac{1}{1+e^{-z}}$，so $$h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta ^T x}}$$ 可以看到，函数主体和线性回归一样，都是样本与参数的内积，但是逻辑回归是用来打分的，也就是判断样本是正负例的概率，需要$g(z)$也就是sigmoid或者logistics函数来将这个内积映射到一个区间(0,1)，所以实际上 $$h_{\\theta}(x) = p(y=1 | x;\\theta)$$ 以上只是得出了样本点是正例的概率，到底预测它是正例还是负例，我们还需要一个decision boundary，例如 $$h_{\\theta}(x) \\ge 0.5 \\rightarrow y=1 \\h_{\\theta}(x) \\lt 0.5 \\rightarrow y=0$$ 由于sigmoid函数的形状我们容易得到 $$\\theta ^T x \\ge 0 \\rightarrow y=1 \\\\theta ^T x \\lt 0 \\rightarrow y=0$$ 当然，decision boundary属于假说函数的一部分，不一定就是0.5. 极大似然估计二元分类可以看做是一个伯努利分布，又叫做0-1分布，上面提到$$p(y=1 | x;\\theta) = h_{\\theta}(x)$$ 那么自然有 $$p(y=0 | x;\\theta) = 1 - h_{\\theta}(x)$$ 那么总体分布也就是得到一个观测值的概率可以用Trick写成$$p(y | x;\\theta) = (h_{\\theta}(x))^y (1 - h_{\\theta}(x^i))^{(1-y)}$$ 因为每一个样本和其他样本是独立同分布的，把它们的概率相乘得到似然函数， $$L(\\theta) = \\prod_{i=1}^{m}((h_{\\theta}(x^i))^{y^i} (1 - h_{\\theta}(x^i))^{(1-y^i)})$$为计算方便，取对数，得到 $$l(\\theta) = \\sum_{i=1}^{m} (y^i \\log(h_{\\theta}(x^i)) +(1-y^i) (1 - h_{\\theta}(x^i)))$$ Cost Function既然逻辑回归是做预测的，那么损失函数就是预测错了多少的度量咯。比如我们就可以统计到底预测错了多少个，来当做损失函数，但是呢，这样的函数肯定是非凸的，不好优化，所以实际采用的损失函数是这样的 $$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m} (Cost(h_{\\theta}(x^i)), y^i)$$ 其中$$Cost(h_{\\theta}(x), y) = -\\log(h_{\\theta}(x)) \\ \\ if \\ \\ y=1 \\;Cost (h_{\\theta}(x), y) = -\\log(1 - h_{\\theta}(x)) \\ \\ if \\ \\ y=0$$ 可简写成： $$Cost(h_{\\theta}(x), y) = - y \\log(h_{\\theta}(x)) - (1-y) \\log(1 - h_{\\theta}(x))$$ 函数图像如下 可以直观的看到，就是概率预测的和标签越接近惩罚越小，反之越大。当然，这里讲的只是二元分类，标签不是0就是1. 最后，逻辑回归的损失函数为 $$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} (y^i \\log(h_{\\theta}(x^i)) + (1-y^i) \\log (1-h_{\\theta}(x^i))$$ 优化过程梯度下降更新算法 $$\\theta_j = \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$ 其中， $$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = - \\frac{1}{m} \\sum{i=1}^{m} (y^i \\log(g(z)) + (1-y^i) \\log (1-g(z))$$ $$= - \\frac{1}{m} \\sum_{i=1}^{m} ( \\frac{y^i}{g(z)} \\frac{\\partial g(z)}{\\theta_j} - \\frac{1-y^i}{1-g(z)} \\frac{\\partial g(z)}{\\theta_j})$$ $$= - \\frac{1}{m} \\sum_{i=1}^{m} \\frac{y^i - g(z)}{g(z)(1-g(z))} \\frac{\\partial g(z)}{\\theta_j}$$ 又$$\\frac{\\partial g(z)}{\\theta_j} = g(z)(1-g(z)x_j^i$$ 代入上式可得$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} =\\frac{1}{m} \\sum_{i=1}^{m}(h_{\\theta}(x^i)-y^i)x_j^i$$这就得到了逻辑回归的梯度下降更新式子$$\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}(h_{\\theta}(x^i)-y^i)x_j^i$$是不是感觉和线性回归的式子一模一样？哈哈，很神奇吧！不过注意两者的假说函数$h_{\\theta}(x)$是不同的，区别在这儿。 牛顿法优化除了梯度下降，其实还有许多可以对逻辑回归做优化的方法，例如牛顿法，这里有待下次更新。","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankchen.xyz/tags/Algorithm/"}]},{"title":"linear regression 的公式手推相关","slug":"linear-regression","date":"2017-01-06T13:55:30.000Z","updated":"2017-01-09T07:58:11.000Z","comments":true,"path":"2017/01/06/linear-regression/","link":"","permalink":"http://frankchen.xyz/2017/01/06/linear-regression/","excerpt":"本文有关多元linear Regression的损失函数从极大似然估计的角度的推导以及梯度下降算法。纯手打，^^","text":"本文有关多元linear Regression的损失函数从极大似然估计的角度的推导以及梯度下降算法。纯手打，^^ 损失函数我们的模型为 $$h_{\\theta}(x) = \\theta_0 + \\theta_1x + \\theta_2 x^2 … = \\theta ^ TX$$ 似然函数为： $$y^i = \\theta ^ T x^i + \\epsilon ^ i$$ ，残差用$\\epsilon$表示，对于每一个样本点的残差$\\epsilon^i$，（注意以下标号中上标i表示第i个样本点），有： $$\\epsilon^i = y^i - h_{\\theta}(x^i) = y^i - \\theta ^ T x^i$$ 那么我们有假设所有的残差根据中心极限定理，其密度函数$p(\\epsilon^i)$符合均值为0，方差为$\\sigma^2$的高斯分布，即 $$p(\\epsilon^i) = \\frac{1}{\\sqrt {2\\pi} \\sigma} \\exp (-\\frac{(\\epsilon^i)^2}{2 \\sigma ^2}) \\= \\frac{1}{\\sqrt {2\\pi} \\sigma} \\exp (-\\frac{(y^i - \\theta ^ T x^i)^2}{2 \\sigma ^2})$$ 那么，似然函数就是所有样本点的出现的概率乘积，即 $$L(\\theta) = \\prod_{i=1}^{m} p(\\epsilon^i) \\$$ $$= \\prod_{i=1}^{m} \\frac{1}{\\sqrt {2\\pi} \\sigma} \\exp (-\\frac{(y^i - \\theta ^ T x^i)^2}{2 \\sigma ^2})$$ 我们最终是求$\\theta$的最优解，于是对似然函数取对数， $$l(\\theta) = \\log \\left(\\prod_{i=1}^{m} \\frac{1}{\\sqrt {2\\pi} \\sigma} \\exp (-\\frac{(y^i - \\theta ^ T x^i)^2}{2 \\sigma ^2})\\right) \\$$ $$= m \\log \\frac{1}{\\sqrt {2\\pi} \\sigma} - \\frac{1}{2 \\sigma ^2} \\sum _{i=1}^{m} (y^i - \\theta ^ T x^i)^2$$ 扔掉常数项，就得到了线性回归的目标函数或error function， $$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i) ^ 2$$ 最小二乘法以上可以把目标函数写成这样的形式： $$J(\\theta) = \\frac{1}{2} (X \\theta - y)^T (X \\theta - y)$$ 对向量$\\theta$求导， $$\\frac{\\partial J(\\theta)}{\\partial \\theta} = X^T(X \\theta - y)$$ 直接令上式为零，得到 $$\\theta = (X^TX)^{-1} X^Ty$$ 最小二乘法解线性回归理论很漂亮，但是因为复杂度的原因实际上不常用。因为矩阵求逆的复杂度为$O(n^3)$，很慢。另外一种解释是通常X不是满秩的，譬如生物学科里面的数据，特征有成千上万个，数据却少得可怜，这种通常是解不出特定的解的。 梯度下降算法常用的还是梯度下降法。以上得到了损失函数为 $$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i) ^ 2$$ 那么，通过对$\\theta$的各分量$\\theta_i$根据梯度更新其值即可 $$\\theta_j = \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$ 而上式中 $$\\frac {\\partial J(\\theta)}{\\partial \\theta_i} =$$ $$\\frac{1}{2m} \\sum_{i=1}^m \\frac{\\partial(h_{\\theta}(x^i) - y^i) ^ 2}{\\partial \\theta_j}$$ $$= \\frac{2}{2m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i) \\frac{\\partial h_{\\theta}(x^i)}{\\partial \\theta_j}$$ 又上式中 $$\\frac{\\partial h_{\\theta}(x^i)}{\\partial \\theta_j} =\\frac{\\partial (\\theta_0 x_0^i + \\theta_1 x_1^i + \\cdots+ \\theta_j x_j^i + \\cdots)}{\\partial \\theta_j} \\= \\frac{\\partial ( \\theta_j x_j^i )}{\\partial \\theta_j}= x_j^i$$ 一步步代上去可得 $$\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m {(h_{\\theta}(x^i) - y^i)} x_j^i$$ 这就是梯度下降法更新多元线性回归的公式啦！^_^ PS：有细心的同学可能发现上面的式子中添加了$\\frac{1}{m}$，可能有疑惑，想问有什么区别。回答：其实是一样的，因为在机器学习中，我们并不关心最优化的时候，目标函数的极值是多少，而我们只关注，目标函数取极值的时候，参数的值是多少。所以因为这个原因，我们经常为了式子简便而对目标函数做缩放，比如上面的取对数，对数的底是e还是2根本没有区别。","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankchen.xyz/tags/Algorithm/"}]},{"title":"AliPay RedEnvelope Hacker","slug":"AliPay-RedEnvelope-Hacker","date":"2016-12-25T15:16:53.000Z","updated":"2016-12-25T16:03:14.000Z","comments":true,"path":"2016/12/25/AliPay-RedEnvelope-Hacker/","link":"","permalink":"http://frankchen.xyz/2016/12/25/AliPay-RedEnvelope-Hacker/","excerpt":"最近支付宝继续其逆天改命的追赶超越微信的社交梦之路，上线了狂拽酷炫吊炸天的AR红包玩法，顿时刷爆了一波朋友圈，但是这热闹后面也隐藏着一些问题甚至是致命Bug或者危机。","text":"最近支付宝继续其逆天改命的追赶超越微信的社交梦之路，上线了狂拽酷炫吊炸天的AR红包玩法，顿时刷爆了一波朋友圈，但是这热闹后面也隐藏着一些问题甚至是致命Bug或者危机。 AR红包一上线，毁誉参半，有媒体人产品方面人士表示“这是支付宝在对抗微信的战场上，难得的漂亮反击”，但是。。。同时很快有天才找到了作弊的方法，所谓实现了“早上试了一下，现在已经实现在家收红包的局面了”，方法其实类似我之后提到的，只是他用的是PS。。。方法是： 截图放到ps里，建立和黑色条纹等宽的长条若干； 黑色条纹然后往上移动一点，覆盖住显示图片的区域，再复制一张截图置于顶层建立剪贴蒙板，把位置错来来看； 扫一下就可以领取红包了 以上来自知乎用户@Chain 很快我又找到了代码版本，是用php实现的，数行代码即可搞定（为了跑成功花了半小时学习如何安装并跑出php的Hello World，囧~）： 123456789101112131415161718192021222324252627282930313233343536&lt;?phpfunction imagecropper($source_path)&#123; $source_info = getimagesize($source_path); $source_width = $source_info[0]; $source_height = $source_info[1]; $source_mime = $source_info['mime']; $oldimg = imagecreatefrompng($source_path); $base_width=340; $base_height=6; $baseimage = imagecreatetruecolor($base_width, $base_width);// 苹果图啊 $beginx = 150; $beginy = 444; $thisimage = imagecreatetruecolor($base_width, $base_height); imagecopy($baseimage, $oldimg, 0, 0, $beginx, $beginy, $base_width, $base_height); for ($i=0;$i&lt;30;$i++)&#123; imagecopy($baseimage, $oldimg, 0, $i*($base_height*2)-$base_height, $beginx, ($beginy+($i*($base_height*2))), $base_width, $base_height); imagecopy($baseimage, $oldimg, 0, $i*($base_height*2), $beginx, ($beginy+($i*($base_height*2))), $base_width, $base_height); &#125; header('Content-Type: image/png'); imagepng($baseimage);&#125;imagecropper(\"http://127.0.0.1/frank/img/IMG_0734.png\")?&gt; 效果如下图： 算法的原理是：使用imagecreatetruecolor创建的新图像，利用imagecopy将指定坐标（截图中的线索图片的方框的左上角）开始的横条部分逐步依次由上到下复制到新图像上，以此达到用黑色条纹上下方的相似图像遮盖条纹的目的。当然原作者的iPhone肯定不是我这个型号，所以很显然这个开始复制的点需要测量手机截图的实际像素距离才可以确定（又花了半个小时测量我手机的截图的线索图片方框左上角的坐标，囧~）。 从这个实验的过程我也看到php作为与浏览器息息相关的语言的优势，上面的程序是每次运行都把新图片显示在浏览器里面，所以每次收到新截图都可以只需刷新一下浏览器就可以在屏幕上显示出处理后的图片了。 运行此php脚本的方法是将其放在xampp的htdocs目录下，开启Apache服务，再在浏览器中访问127.0.0.1/index.php 即可（index.php即脚本名）。 其实从AR红包上线以来就有许多问题，譬如 如果线索图片是工卡、校园卡或者桌椅、纸巾等附近的人也可以拍到类似的物品或者含有类似logo物品的图片也是可以领取的，这就涉及到定位精准度等问题，不过我认为这也是玩法的乐趣所在，让人民群众喜闻乐见的作弊也是买点之一 而像上面提到的直接处理图片就可以作弊肯定就是大问题了，这说明阿里哦不，是蚂蚁金服对于图片的处理的算法急需改进，肯定不可能是这种错位一下图片就可以作弊的程度，这里我有个想法是加入类似于人脸识别里的判断是真人还是相片的模块，通过这个排除不是真实物品的利用屏幕上的图片来作弊的行为。 好了，暂时先写这么多，祝圣诞快乐！ 参考资料 支付宝红包破解算法","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankchen.xyz/tags/Algorithm/"}]},{"title":"pointer and ++--","slug":"pointer-and","date":"2016-12-11T08:50:45.000Z","updated":"2016-12-11T08:57:08.000Z","comments":true,"path":"2016/12/11/pointer-and/","link":"","permalink":"http://frankchen.xyz/2016/12/11/pointer-and/","excerpt":"本文主要关于演示指针与自增自减运算符结合的时候的运算顺序。","text":"本文主要关于演示指针与自增自减运算符结合的时候的运算顺序。 表达式 含义 *p++或*(p++) 自增前表达式的值是*p，然后自增p (*p)++ 自增前表达式的值是*p，然后自增*p *++p或*(++p) 先自增p，自增后表达式的值是*p ++*p或++(*p) 先自增*p，自增后表达式的值是*p 原理是自增和*在一起时结合顺序由右到左，比如*p++，先运算p++，也就是p当前的值，再运算*p对寻址，再对p自增。 注意以上所有的表达式返回的值都是*p，只不过对分别是对p和*p的自增操作。","categories":[],"tags":[{"name":"C++","slug":"C","permalink":"http://frankchen.xyz/tags/C/"}]},{"title":"QuickSort in C & Python","slug":"QuickSort-in-C","date":"2016-12-02T11:55:35.000Z","updated":"2016-12-10T07:36:15.000Z","comments":true,"path":"2016/12/02/QuickSort-in-C/","link":"","permalink":"http://frankchen.xyz/2016/12/02/QuickSort-in-C/","excerpt":"本文主要关于演示C语言中的快排算法。","text":"本文主要关于演示C语言中的快排算法。 大名鼎鼎的QuickSort–快速排序算法，如此的高效、快速而优雅，今天让我们用图片和文字形象地展示一下它吧！ 快排是一种分治算法，其主要思想在于通过选取一个基准Pivot将序列分为左右两个子序列，其中左序列的数都小于等于Pivot，右边的都大于等于Pivot，再在这两个子序列上递归调用快排算法即可。 而理解快排算法的核心就在于分解这个步骤，这里我们先给出代码，再用图片展示步骤。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859//// main.c// QuickSort//// Created by frankchen on 12/2/16.// Copyright © 2016 frankchen. All rights reserved.//#include &lt;stdio.h&gt;#define N 5void QuickSort(int s[],int low,int high);void swap(int s[],int i,int j);int main(void)&#123; int a[N], i; printf(\"Enter %d numbers to be sorted: \", N); for (i=0; i&lt;N; i++) scanf(\"%d\", &amp;a[i]); QuickSort(a, 0, N-1); printf(\"In sorted order: \\n\"); for (i=0; i&lt;N; i++) printf(\"%d \", a[i]); printf(\"\\n\"); return 0;&#125;//交换数组中的两个元素void swap(int s[],int i,int j)&#123; int temp; temp=s[i]; s[i]=s[j]; s[j]=temp;&#125;void QuickSort(int s[], int low, int high)&#123; int i, pivot;//记录界限的位置 if(low &lt; high)//只有数组中元素大于1时才操作 &#123; pivot = low;//选取第一个元素作为基准 for(i=low+1; i&lt;=high; i++) &#123; if (s[i] &lt; s[low]) swap(s, ++pivot, i); &#125; swap(s, low, pivot);//基准元与界限交换 QuickSort(s, low, pivot-1); QuickSort(s, pivot+1, high); &#125;&#125; 以下是Python版本： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class QuickSort(object): def __init__(self, l): ''' 初始化 ''' self.l = l def __str__(self): ''' 打印排序后的list ''' return 'list\\t:%s' % (self.l) def swap(self, s, i, j): ''' 交换位置 ''' tmp = 0 tmp = s[i] s[i] = s[j] s[j] = tmp def QuickSort(self, s, low, high): i = 0; pivot = 0 if low &lt; high: pivot = low i = low + 1 for i in range(i, high+1): if s[i] &lt; s[low]: pivot += 1 self.swap(s, pivot, i) self.swap(s, low, pivot) self.QuickSort(s, low, pivot-1) self.QuickSort(s, pivot+1, high) return s def getList(): ''' 读取文件内的数存入数组 ''' MyFile = \"QuickSort.txt\" l = [] with open(MyFile) as f: for line in f.readlines(): l.append(int(line)) return ldef Sort(): ''' 排序 ''' # 创建QuickSort对象 l = getList() p = QuickSort(l) N = len(l) p.QuickSort(l, 0, N-1) return pif __name__ == \"__main__\": p = Sort() print p 为了排序整个序列，只需要调用QuickSort(R，0，n-1)即可完成对R[0..n-1]的排序。 这里选取了第一个元素作为基准，如果输入是随机序列，那么没什么大碍，但是如果输入是预排序或者逆序的，那么会产生坏的排序，使得时间复杂度提高，随机选取基准是一种可行的方法，另外一种是选取开头、中间点和末位点的中位数。 ```","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankchen.xyz/tags/Algorithm/"}]},{"title":"Workflow for automatic extract and jump to the true URL of 91porn.com","slug":"Workflow-for-automatic-extract-and-jump-to-the-true-URL-of-91porn-com","date":"2016-11-27T16:41:22.000Z","updated":"2017-01-13T09:49:17.000Z","comments":true,"path":"2016/11/28/Workflow-for-automatic-extract-and-jump-to-the-true-URL-of-91porn-com/","link":"","permalink":"http://frankchen.xyz/2016/11/28/Workflow-for-automatic-extract-and-jump-to-the-true-URL-of-91porn-com/","excerpt":"主要讲述自己开发的Alfred Workflow 91url：自动获取某网站的真实视频地址并跳转的应用。","text":"主要讲述自己开发的Alfred Workflow 91url：自动获取某网站的真实视频地址并跳转的应用。 相信许多朋友都爱看91的视频，但是免费用户的每日有限的观看次数总是让人意犹未尽。。。于是FreeGet 免费福利资讯应运而生。。。但是重复的从浏览器窗口点击复制url，切换窗口、粘贴再点击。。。有什么办法可以让众狼友的双手解放出来呢？作为程序员的我当然不能忍，所以耗费一下午的时间开发了这个Workflow：91url，能够自动获取当前的浏览器窗口的网址并提交到FreeGet网站，并用火狐浏览器打开真实视频网址（抱歉是0.1版本，功能受限，不足之处待之后版本解决）。使用方法非常简单，只要在我的Github下载并拖到Alfred 的Workflow即可，另外还需要安装Python的自动化网页测试工具包selenium： 1pip install selenium 然后浏览器处于91视频的页面的时候，用command + 空格 输入91url并enter即可(当然也可以修改HotKey来触发)： 之后91url将自动打开火狐浏览器并跳转至FreeGet获取真实网址， 如上图，点击即可播放或者下载，so，enjoy！","categories":[],"tags":[{"name":"Old Driver","slug":"Old-Driver","permalink":"http://frankchen.xyz/tags/Old-Driver/"}]},{"title":"Devide and Conquer Counting Inversions with Python","slug":"Devide-and-Conquer-Counting-Inversions-with-Python","date":"2016-11-25T08:55:15.000Z","updated":"2017-01-13T09:48:56.000Z","comments":true,"path":"2016/11/25/Devide-and-Conquer-Counting-Inversions-with-Python/","link":"","permalink":"http://frankchen.xyz/2016/11/25/Devide-and-Conquer-Counting-Inversions-with-Python/","excerpt":"本文主要内容关于python归并排序求逆序数的算法。","text":"本文主要内容关于python归并排序求逆序数的算法。 解决问题：给定有限长度的无重复乱序数组，求其逆序数个数。简单来说我们可以用两个循环来解决，但是复杂度为$O(N^2)$，若利用归并排序，复杂度就降为了$O(N \\log(N))$。以下给出一个实现， 1234567891011121314151617181920212223242526272829303132333435363738394041class Ivrs_num(object): def __init__(self, filename): self.count = 0 self.filename = filename def load_txt(self): '''载入txt文件保存为list''' int_list = [] with open(self.filename) as f: for line in f: int_list.append(int(line)) return int_list def merge(self, ListA, ListB): '''将两个已经排序好的数组合并成新的排序好的数组 顺便计算逆序数''' newList = [] while ListA and ListB: if int(ListA[0]) &gt; int(ListB[0]): self.count += len(ListA) newList.append(ListB.pop(0)) else: newList.append(ListA.pop(0)) return newList + ListA + ListB def merge_sort(self, A): '''归并排序''' if len(A) == 1: return A else: middle = len(A) // 2 return self.merge(self.merge_sort(A[:middle]), self.merge_sort(A[middle:])) def count_ivrs(self): data = self.load_txt() self.merge_sort(data) return self.countif __name__ == \"__main__\": ivrs = Ivrs_num(\"IntegerArray.txt\") print ivrs.count_ivrs()","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://frankchen.xyz/tags/Algorithm/"}]},{"title":"Understanding EM algorithm","slug":"Understanding-EM-algorithm","date":"2016-11-18T13:54:19.000Z","updated":"2017-01-13T09:49:08.000Z","comments":true,"path":"2016/11/18/Understanding-EM-algorithm/","link":"","permalink":"http://frankchen.xyz/2016/11/18/Understanding-EM-algorithm/","excerpt":"本文主要内容为用一个简短的例子解释EM算法。","text":"本文主要内容为用一个简短的例子解释EM算法。 EM算法是聚类中常用的机器学习算法，但是相比喜闻乐见的k-means算法，大家可能对于EM算法的了解可能没有那么直观深入，所以本文主要利用一个简单的例子，在对比k-means算法的过程中，帮助大家建立一个清晰明确的对EM算法的理解。 当我们在聚类的领域谈论k-means算法时，分组的概念是非常清晰直观的–每个样本点只属于它离得最近的那个中心点所属的分组。如果给出一些样本点和中心，那么我们很容易给这些点打标签。 但是对于EM算法而言，分组的概念就么那么直观了（因为EM算法考虑每个样本点都有一定的概率属于所有的分组）。在我们介=引入EM算法之前，我们先复习一下k-means里面的分组概念。 k-means里面的分组概念假设我们有如下的三个样本点（2D平面情况下的点）： 数据集 X Y 点0 10 5 点1 2 1 点2 3 7 如果在上面的数据集上跑k-means算法，假定中心点如下： 中心 X Y A 3 4 B 6 3 C 4 6 使用欧几里得距离，可以分配聚类如下 距离 群组A中心 群组B中心 群组C中心 群组分配 点0 7.071 4.472 6.083 群组B 点1 3.162 4.472 5.385 群组A 点2 3.000 5.000 1.414 群组C 到此，我们可以给出一个回答：到底把一个点归类到一个群组意味着什么？举例来说，点1只被分配到分组A，也就是点1的100%属于群组A并且0%属于群组B和群组C。那么根据这个定义，我们可以得到如下表格， 群组A 群组B 群组C 点0 0 1 0 点1 1 0 0 点2 0 0 1 成员个数 1 1 1 注意到，对于一个群组来说，其得到的每一行的和都是1（因为一个点的百分比就是之和就是1）；另外，每一列的和就是这个群组的成员点的个数。推而广之： 每一行的和总是1，因为一个点的百分比之和是1 每一列的和就是分配到这个群组的点的个数 EM算法中的群组分配到此为止，我们做的似乎不错，但是存在一个问题，我们这里每个点都只能属于一个群组，不过这一点有时候不是那么合理，比如点0分配到群组B似乎没什么问题，但是比如点1距离群组A和群组B的距离差别不那么大，那么只因为距离群组A近那么一点点就只把它分配到群组A，难道没什么问题吗？我们可能更加想表达这样一种概念：点1更可能属于群组A，但是我们也想保留点1也有一些可能属于群组B的不确定性。 距离 群组A中心 群组B中心 群组C中心 群组分配 点0 7.071 4.472 6.083 群组B 点1 3.162 4.472 5.385 群组A 点2 3.000 5.000 1.414 群组C 我们如何表述这种不确定性呢？这就是分组权重的由来。分组权重表达了一个点有多可能属于某个群组。比如之前的形式是这样的， 群组A 群组B 群组C 点0 0 1 0 点1 1 0 0 点2 0 0 1 成员个数 1 1 1 把上表的0和1换成分数（不要质疑这些分数怎么来的，等下会给出合理解释，：）） 群组A 群组B 群组C 点0 0.007 0.938 0.055 点1 0.812 0.154 0.034 点2 0.234 0.016 0.750 软计数 1.053 1.108 0.839 上表中，点0以93.8%的概率属于群组B，同时下一行中，点1以81.2%的概率属于群组A。这些分数也就是分组权重。比如，点0属于分组A的权重就是0.7%。 和k-means算法中每个点只能属于一个群组不同，这里每个点都以某种程度地属于每一个群组。例如，93.8%的点0属于群组，其余的属于其他的群组。 那么，在上面的分配矩阵里面，每一行和每一列的含义如下， 每一行的和都是1，因为每个点都100%属于所有的群组，将这一些部分加起来和必定是1 每一列的和是这个群组的“软计数”，它代表着所有的点属于这个群组的百分比之和 所有的群组的软计数之和就是样本点的总数 步骤E：给定分组参数计算分组权重分组权重是怎么来的？来自给定分组的分布的时候，我们观察样本点可能是什么。而EM算法中的每一个群组都由一个分组权重，一个均值向量，一个协方差矩阵构成。其中，均值表示群组的中心，协方差体现群组的范围，而分组权重体现了样本点与此分组的关联程度。所有，每个群组都由一个多变量高斯分布所定义。 回顾上面的例子，加上一个不确定度的椭圆，如下， 图中每个椭圆表示着协方差矩阵，在这个例子中，每个群组都是用的是对角协方差矩阵[[3,0],[0,3]]。因为对角线外元素都是0，所有图中的椭圆实际上看起来是圆。并且，由于并没有什么理由认为哪个群组比其他群组更加重要，那么我们定义每个群组的分组权重都是1/3。 那么，点0属于群组A的概率是多少？这里用来衡量潜在的高斯分布的标准是概率密度函数（probability density function，PDF），使用scipy.stats.multivariate_normal.pdf可计算出来： 12print multivariate_normal.pdf([10,5], mean=[3,4], cov=[[3,0],[0,3]])&gt;&gt;&gt; 1.275199678019219e-05 我们还要将这个概率乘以分组权重才行， 12print 1/3.*multivariate_normal.pdf([10,5], mean=[3,4], cov=[[3,0],[0,3]])&gt;&gt;&gt; 4.2506655934e-06 点0属于群组A的似然值是4.251e-6。单看看不出什么，我们还要依次计算群组B和群组C的值。分别计算B和C的PDF并且乘以分组权重： 12print 1/3.*multivariate_normal.pdf([10,5], mean=[6,3], cov=[[3,0],[0,3]])&gt;&gt;&gt; 0.000630854709005 12print 1/3.*multivariate_normal.pdf([10,5], mean=[4,6], cov=[[3,0],[0,3]])&gt;&gt;&gt; 3.71046481027e-05 那么，总结一下， 群组A 群组B 群组C 点(10,5)的PDF乘以分组权重 4.251e-6 6.309e-4 3.710e-5 很明显，群组B的似然值是最高的，这容易理解因为点0距离群组B中心的距离最近。 似然值看起来太小了，我么可以对其做一个归一化：都除以所有群组的似然值的和得到百分比值。 点0 群组A 群组B 群组C 总和 似然值 4.251e-6 6.309e-4 3.710e-5 6.722e-4 似然值，处以总和 4.251e-6 / 6.722e-4 = 0.007 6.309e-4 / 6.722e-4 = 0.938 3.710e-5 / 6.722e-4 = 0.055 - 最下一行就是点0分组责任！注意到因为归一化，这一行的总和是1。小结一下： 对于每一个点我们计算了其高斯分布的PDF值，通过使用高斯分布的均值和协方差计算得到 将PDF乘以分组权重，得到似然度 将似然程度归一化 依次，我们可以得到点1和点2的矩阵，这里略去不表，最后将三个点的矩阵汇总得到以下， Responsibility matrix Cluster A Cluster B Cluster C Data point 0 0.007 0.938 0.055 Data point 1 0.812 0.154 0.034 Data point 2 0.234 0.016 0.750 Soft counts 1.053 1.108 0.839 步骤M：给定分组责任计算分组参数现在我们手里已经有了分组的责任了，我们可以依据这些重新更新参数：分组权重、均值和协方差。虽然我们开始是乱猜的这些参数，但是通过更新我们可以得到一些更好的估计。 分组权重。群组的相对重要程度由它的软计数来决定。由于分组权重必须和为1，所有这里也需要做归一化， Cluster A Cluster B Cluster C Sum Soft counts 1.053 1.108 0.839 3.000 Soft counts, divided by the sum 1.053 / 3.000 = 0.351 1.108 / 3.000 = 0.369 0.839 / 3.000 = 0.280 - 归一化后的软计数，就是分组权重的新的估计。 均值。使用分组责任计算所有点坐标的百分比， 12345678[Weighted sum of data points for cluster A]= [Fraction of data point 0 represented in cluster A] * [data point 0] + [Fraction of data point 1 represented in cluster A] * [data point 1] + [Fraction of data point 2 represented in cluster A] * [data point 2]= 0.007*[data point 0] + 0.812*[data point 1] + 0.234*[data point 2]= 0.007*(10,5) + 0.812*(2,1) + 0.234*(3,7)= (0.063,0.035) + (1.624,0.812) + (0.702,1.638)= (2.396,2.485) 再除以软计数： 123[mean of cluster A]= (2.396,2.485)/1.053= (2.275,2.360) 类似地计算B和C的均值，得到如下， New means X Y Cluster A 2.275 2.360 Cluster B 8.787 4.473 Cluster C 3.418 6.626 让我们画出均值的之前和之后的估计。注意到群组A的均值移动地更加靠近点1，因为群组A是主要的，类似的也发生在B和C上， 协方差。 协方差也是由分数来计算的，但是这里需要矩阵形式，所以实际上是由向量叉乘得到的， $$x_i - \\hat{\\mu}_k$$ 假设上面是d维向量，计算其与自身的内积， $$(x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^T$$ 对012每个点都根据其坐标和A的均值计算协方差矩阵，与分组责任相乘： 12345678910111213[Weighted sum of outer products]= [Fraction of data point 0 represented in cluster A] * [outer product for data point 0] + [Fraction of data point 1 represented in cluster A] * [outer product for data point 1] + [Fraction of data point 2 represented in cluster A] * [outer product for data point 2]= 0.007*[[59.676,20.394], [20.394,6.970]] + 0.812*[[0.076,0.374], [0.374,1.850]] + 0.234*[[0.526,3.364], [3.364,21.530]]= [[0.602, 1.234], [1.234, 6.589]][New covariance for cluster A]= [[0.602,1.234], [1.234,6.589]]/1.053= [[0.572,1.172], [1.172,6.257]] 对于BC重复以上，得到， New covariances Cluster A [[0.572,1.172], [1.172,6.257]] Cluster B [[8.132,3.606], [3.606,2.004]] Cluster C [[3.078,-0.518], [-0.518,1.581]] 长于一口气！这么多的计算，我们到底更新的效果是什么？首先，所有的群组都有了更小的不确定度，从图上来看，椭圆变得更小了；其次，群组改变了形状来适应数据，比如群组B向着点1的方向延长了。总之，每次均值和协方差更新以后，群组都改变形状以更好地表示数据里的模式。 EM算法：交替步骤E和步骤M现在我们对于分组的参数有了更好地估计，那么我们可以回过头去计算分组的责任，于是我们可以得到一个更好的参数的估计。实际上，我们可以交替着使用步骤E和步骤M来逐步提高分组的性能。 参考资料 华盛顿大学机器学习：聚类与检索","categories":[],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://frankchen.xyz/tags/Machine-Learning/"}]},{"title":"Way to solving the .DS_Store problem of Mac","slug":"Way-to-solving-the-DS-Store-problem-of-Mac","date":"2016-11-17T07:25:30.000Z","updated":"2017-01-13T09:17:33.000Z","comments":true,"path":"2016/11/17/Way-to-solving-the-DS-Store-problem-of-Mac/","link":"","permalink":"http://frankchen.xyz/2016/11/17/Way-to-solving-the-DS-Store-problem-of-Mac/","excerpt":"本文主要讲述如何解决Mac OS下面的.DS_Store文件的问题。","text":"本文主要讲述如何解决Mac OS下面的.DS_Store文件的问题。 在Mac OS上，.DS_Store文件 是 Desktop Services Store 的简称，是用来存储文件夹的显示属性的自定义属性的隐藏文件，：比如文件图标的摆放位置，如文件的图标位置或背景色，相当于Windows的desktop.ini，删除以后的副作用就是这些信息的失去，不过总体而言影响不大。 最近上手的Mac，.DS_Store 第一次烦到我，是在Github上folk了人家的项目自己提交了想Pull Request 的时候出现的问题，我明明只修改了一个文件，为什么在每个文件夹下面都出现了这么些个奇怪的 .DS_Store ？？删除了这些文件以后再第二次提交的时候又会出现（期间我用了Finder），谷歌以后才发现这原来是系统自动生成的，要想它不出现，除非只用Shell不用Finder。。。 接着找资料发现如下办法， 安装ASEPSIS OS X 10.11以前的版本直接安装即可 OS X 10.11需要关闭SIP（System Integrity Protection）再使用命令touch ~/.no-asepsis-os-restriction 新建文件再安装 重启系统 然后可以用以下指令删除mac上所有的.DS_Store： 123find ~ -name &quot;.DS_Store&quot; -delete或者find &lt;your path&gt; -name &quot;.DS_Store&quot; -delete 参考资料 https://www.zhihu.com/question/20345704 https://zh.wikipedia.org/wiki/.DS_Store","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/categories/tutorial/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://frankchen.xyz/tags/Mac/"},{"name":".DS_Store","slug":"DS-Store","permalink":"http://frankchen.xyz/tags/DS-Store/"}]},{"title":"A efficiency comparison between while for generator comprehension - Python while、for、生成器、列表推导等语句的执行效率对比","slug":"2016-11-11-a-efficiency-comparison-between-while-slash-for-slash-generator-slash-comprehension","date":"2016-11-11T14:43:17.000Z","updated":"2016-11-11T16:04:04.000Z","comments":true,"path":"2016/11/11/2016-11-11-a-efficiency-comparison-between-while-slash-for-slash-generator-slash-comprehension/","link":"","permalink":"http://frankchen.xyz/2016/11/11/2016-11-11-a-efficiency-comparison-between-while-slash-for-slash-generator-slash-comprehension/","excerpt":"python中，同一个功能的实现，可以用多种语句来实现，比如说:while语句、for语句、生成器、列表推导、内置函数等实现，然而他们的效率并不一样。这里有一个小程序来测试它们执行的效率。","text":"python中，同一个功能的实现，可以用多种语句来实现，比如说:while语句、for语句、生成器、列表推导、内置函数等实现，然而他们的效率并不一样。这里有一个小程序来测试它们执行的效率。 12345678910111213141516171819202122232425262728293031323334353637383940414243import sys nums = 100 #每个函数实现的都是从1-100的自然数的绝对值添加进一个listdef while_Statement(): #while loop实现 res = [] x = 0 while nums &gt; x: x += 1 res.append(abs(x)) def for_Statement(): #for loop实现 res = [] for x in range(nums): res.append(abs(x)) def generator_Expression(): #生成器实现 res = list(abs(x) for x in range(nums)) def list_Comprehension(): #列表解析式实现，对比生成器实现可以发现两者区别只有中括号与小括号 res = [abs(x) for x in range(nums)] def map_Function(): #map函数（内置函数）实现 res = map(abs, range(nums)) if __name__=='__main__': import timeit #用timeit模块来测试 print sys.version funcs = [while_Statement, for_Statement, generator_Expression, list_Comprehension, map_Function] for func in funcs: print func.__name__.ljust(20),': ',timeit.timeit(\"func()\", setup=\"from __main__ import func\") 测试结果:&gt;&gt;&gt; 2.7.10 (default, Oct 23 2015, 19:19:21) [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)]while_Statement : 16.0008671284for_Statement : 12.9374330044generator_Expression : 10.133701086list_Comprehension : 7.86516404152map_Function : 5.24277615547 以上用timeit模块两种测试方式测试了若干组数字，得出的结果是执行内置函数最快，其次就是列表推导，再其次生成器和for循环，while循环最慢。最快的使用内置函数的方法要比使用最慢的while快两倍以上。 简单分析下原因: 内置函数比如说map,filter,reduce基本上都是用C语言来实现的,所以速度是最快的， 列表推导内的迭代在解释器内是以C语言的速度运行的(一般是for循环的两倍,对大型文件操作而言 列表推导效果尤其明显)，相比较for循环代码是在PVM步进运行要快的多 for循环里面含range()，相对速度也会快些， while语句是纯粹用Python代码写成，所以速度最慢。 综上，所以函数式编程最好使用内置函数，然后才考虑使用列表推导或for循环。最好不用while循环。 ##参考资料 Python while、for、生成器、列表推导等语句的执行效率测试 PythonSpeed/PerformanceTips Python 性能小贴士 (第1部分)","categories":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"}]},{"title":"精确率、召回率、F1 值、ROC/AUC 、PRC各自的优缺点是什么？","slug":"metric","date":"2016-09-21T14:15:48.000Z","updated":"2017-01-13T09:47:10.000Z","comments":true,"path":"2016/09/21/metric/","link":"","permalink":"http://frankchen.xyz/2016/09/21/metric/","excerpt":"精确率、召回率、F1 值、ROC、AUC、PRC都是机器学习模型的常用评价标准，那么它们的区别和联系以及各自应用场景是什么呢？ 这些指标的含义1234Precision：P=TP/(TP+FP)Recall：R=TP/(TP+FN)F1-score：2/(1/P+1/R)ROC/AUC：TPR=TP/(TP+FN), FPR=FP/(FP+TN) TP —— True Positive （真正, TP）被模型预测为正的正样本；可以称作判断为真的正确率 TN —— True Negative（真负 , TN）被模型预测为负的负样本 ；可以称作判断为假的正确率 FP ——False Positive （假正, FP）被模型预测为正的负样本；可以称作误报率 FN—— False Negative（假负 , FN）被模型预测为负的正样本；可以称作漏报率","text":"精确率、召回率、F1 值、ROC、AUC、PRC都是机器学习模型的常用评价标准，那么它们的区别和联系以及各自应用场景是什么呢？ 这些指标的含义1234Precision：P=TP/(TP+FP)Recall：R=TP/(TP+FN)F1-score：2/(1/P+1/R)ROC/AUC：TPR=TP/(TP+FN), FPR=FP/(FP+TN) TP —— True Positive （真正, TP）被模型预测为正的正样本；可以称作判断为真的正确率 TN —— True Negative（真负 , TN）被模型预测为负的负样本 ；可以称作判断为假的正确率 FP ——False Positive （假正, FP）被模型预测为正的负样本；可以称作误报率 FN—— False Negative（假负 , FN）被模型预测为负的正样本；可以称作漏报率True Positive Rate（真正率 , TPR）或灵敏度（sensitivity） 也就是召回率 Recall TPR = TP \\/（TP + FN） 正样本预测结果数 \\/ 正样本实际数 True Negative Rate（真负率 , TNR）或特指度（specificity） TNR = TN \\/（TN + FP） 负样本预测结果数 \\/ 负样本实际数 False Positive Rate （假正率, FPR） FPR = FP \\/（FP + TN） 被预测为正的负样本结果数 \\/负样本实际数 False Negative Rate（假负率 , FNR） FNR = FN \\/（TP + FN） 被预测为负的正样本结果数 \\/ 正样本实际数 假定一个具体场景作为例子： 假如某个班级有男生80人,女生20人,共计100人.目标是找出所有女生. 现在某人挑选出50个人,其中20人是女生,另外还错误的把30个男生也当作女生挑选出来了. 作为评估者的你需要来评估(evaluation)下他的工作。 相关(Relevant),正类 无关(NonRelevant),负类 被检索到(Retrieved) true positives(TP 正类判定为正类,例子中就是正确的判定”这位是女生”) false positives(FP 负类判定为正类,”存伪”,例子中就是分明是男生却判断为女生,当下伪娘横行,这个错常有人犯) 未被检索到(Not Retrieved) false negatives(FN 正类判定为负类,”去真”,例子中就是,分明是女生,这哥们却判断为男生–梁山伯同学犯的错就是这个) true negatives(TN 负类判定为负类,也就是一个男生被判断为男生,像我这样的纯爷们一准儿就会在此处) 通过这张表,我们可以很容易得到这几个值: TP=20 FP=30 FN=0 TN=50 精确率： 策略命中的所有相关样本\\/策略命中的所有样本 Precision = TP \\/ (TP + FP) _注：正确率\\/准确率（accuracy）和 精度（precision）不是一个概念_ _正确率是我们最常见的评价指标，accuracy = （TP+TN）\\/(P+N)，这个很容易理解，就是被分对的样本数除以所有的样本数_ 召回率：策略命中的所有相关样本\\/所有的相关样本（包括策略未被命中的） Recall = TP \\/ (TP + FN) F1-score(F1-分数)：2×准确率×召回率\\/（准确率+召回率），是模型准确率和召回率的一种加权平均，它的最大值是1，最小值是0。（详细介绍见下） ROC(_receiver operating characteristic curve_): ROC曲线的横坐标为_false positive rate_（FPR,假正率），纵坐标为_true positive rate_（TPR，真正率，召回率） PRC(_precision recall curve_): PRC曲线的横坐标为召回率_Recall_，纵坐标为准确率_Precision_。 AUC:被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。故AUC与PRC是同一概念。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在0.5和1之间。 指标的评价标准ROC与AUCROC（receiver operating characteristic curve）也就是下图中的曲线，其关注两个指标 True Positive Rate ( TPR ) = TP \\/ [ TP + FN] ，TPR代表能将正例分对的概率 False Positive Rate( FPR ) = FP \\/ [ FP + TN] ，FPR代表将负例错分为正例的概率 在ROC 空间中，每个点的横坐标是FPR，纵坐标是TPR，这也就描绘了分类器在TP（真正的正例）和FP（错误的正例）间的trade-off。ROC的主要分析工具是一个画在ROC空间的曲线——ROC curve。我们知道，对于二值分类问题，实例的值往往是连续值，我们通过设定一个阈值，将实例分类到正类或者负类（比如大于阈值划分为正类）。因此我们可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve。ROC curve经过（0,0）（1,1），实际上(0, 0)和(1, 1)连线形成的ROC curve实际上代表的是一个随机分类器。一般情况下，这个曲线都应该处于(0, 0)和(1, 1)连线的上方。如图所示。 用ROC curve来表示分类器的performance很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。 于是Area Under roc Curve(AUC)就出现了。顾名思义，AUC的值就是处于ROC curve下方的那部分面积的大小。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。 同时我们也看里面也上了AUC也就是是面积。一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting（比如图中0.2到0.4可能就有问题，但是样本太少了），这个时候调模型可以只看AUC，面积越大一般认为模型越好。 PRC再说PRC， precision recall curve。和ROC一样，先看平滑不平滑（蓝线明显好些），在看谁上谁下（同一测试集上），一般来说，上面的比下面的好（绿线比红线好）。F1（下面介绍）当P和R接近就也越大，一般会画连接(0,0)和(1,1)的线，线和PRC重合的地方的F1是这条线最大的F1（光滑的情况下），此时的F1对于PRC就好象AUC对于ROC一样。一个数字比一条线更方便调模型。 以上两个指标用来判断模型好坏，但是有时候模型没有单纯的谁比谁好（比如图二的蓝线和青线），那么选择模型还是要结合具体的使用场景。 下面是两个场景： 地震的预测 对于地震的预测，我们希望的是RECALL非常高，也就是说每次地震我们都希望预测出来。这个时候我们可以牺牲PRECISION。情愿发出1000次警报，把10次地震都预测正确了；也不要预测100次对了8次漏了两次。 嫌疑人定罪 基于不错怪一个好人的原则，对于嫌疑人的定罪我们希望是非常准确的。及时有时候放过了一些罪犯（recall低），但也是值得的。 对于分类器来说，本质上是给一个概率，此时，我们再选择一个CUTOFF点（阀值），高于这个点的判正，低于的判负。那么这个点的选择就需要结合你的具体场景去选择。反过来，场景会决定训练模型时的标准，比如第一个场景中，我们就只看RECALL=99.9999%（地震全中）时的PRECISION，其他指标就变得没有了意义。 PRC比ROC更有效的特殊情况在正负样本分布得极不均匀(highly skewed datasets)的情况下，PRC比ROC能更有效地反应分类器的好坏。 例如上图中图(a)ROC曲线中的两个模型看似非常接近左上角，但是对应着同一点的图(b)中的PRC曲线中我们可以发现，在召回率为0.8时准确率只有0.05之少，这是由于正负样本比例失衡造成的。 再如下图： 在testing set出现imbalance时ROC曲线能保持不变，而PR则会出现大变化。引用图(Fawcett, 2006)，(a)(c)为ROC，(b)(d)为PR，(a)(b)样本比例1:1，(c)(d)为1:10。 F1信息检索、分类、识别、翻译等领域两个最基本指标是召回率(Recall Rate)和准确率(Precision Rate)，召回率也叫查全率，准确率也叫查准率。 图表表示如下： 准确率和召回率是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下准确率高、召回率就低，召回率低、准确率高，当然如果两者都低，那是什么地方出问题了。一般情况，用不同的阀值，统计出一组不同阀值下的精确率和召回率，这就是PRC曲线。如下图： 如果是做搜索，那就是保证召回的情况下提升准确率；如果做疾病监测、反垃圾，则是保准确率的条件下，提升召回。 所以，在两者都要求高的情况下，可以用F1来衡量。 F1 = 2 * P * R / (P + R) 总结 AUC是ROC的积分（曲线下面积），是一个数值，一般认为越大越好，数值相对于曲线而言更容易当做调参的参照。 ROC相比PRC在正负样本比例悬殊时具有保持单调性的特性，学术论文在假定正负样本均衡的时候多用ROC\\/AUC。 实际工程更多存在数据标签倾斜问题一般使用F1。","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/categories/tutorial/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://frankchen.xyz/tags/Machine-Learning/"}]},{"title":"PDF click return","slug":"2016-09-07-pdf-click-return","date":"2016-09-07T02:57:05.000Z","updated":"2016-11-11T13:50:33.000Z","comments":true,"path":"2016/09/07/2016-09-07-pdf-click-return/","link":"","permalink":"http://frankchen.xyz/2016/09/07/2016-09-07-pdf-click-return/","excerpt":"","text":"在阅读PDF文档时，如果点击了文档中的链接，跳转到另一页，想再快速返回原来的页面，使用alt+左箭头的组合键即可。 根据情况的不同，有时需要使用若干次才能返回原来的页面，这个组合键就相当于回退，如果点击了链接到了新的页面，在新的页面上又移动了页面，如向上或向下浏览，那么至少要使用2次或2次以上alt+左箭头组合键才能终点原链接处。 而想要再次回退就使用alt+右箭头组合进行操作即可。","categories":[],"tags":[{"name":"tips","slug":"tips","permalink":"http://frankchen.xyz/tags/tips/"}]},{"title":"如何解决Linux 下 zip 文件解压乱码","slug":"Linux zip encoding","date":"2016-07-16T04:11:14.000Z","updated":"2016-11-11T13:50:33.000Z","comments":true,"path":"2016/07/16/Linux zip encoding/","link":"","permalink":"http://frankchen.xyz/2016/07/16/Linux zip encoding/","excerpt":"由于zip格式中并没有指定编码格式，Windows下生成的zip文件中的编码是GBK/GB2312等，因此，导致这些zip文件在Linux下解压时出现乱码问题，因为Linux下的默认编码是UTF8。目前网上流传一种unzip -O cp936的方法，但一些unzip是没有-O这个选项的。","text":"由于zip格式中并没有指定编码格式，Windows下生成的zip文件中的编码是GBK/GB2312等，因此，导致这些zip文件在Linux下解压时出现乱码问题，因为Linux下的默认编码是UTF8。目前网上流传一种unzip -O cp936的方法，但一些unzip是没有-O这个选项的。 在ubuntu下的安装命令是 sudo apt-get install p7zip convmv 安装完之后，就可以用7za和convmv两个命令完成解压缩任务。 LANG=C 7za x your-zip-file.zip convmv -f GBK -t utf8 --notest -r . 第一条命令用于解压缩，而LANG=C表示以US-ASCII这样的编码输出文件名，如果没有这个语言设置，它同样会输出乱码，只不过是UTF8格式的乱码(convmv会忽略这样的乱码)。第二条命令是将GBK编码的文件名转化为UTF8编码，-r表示递归访问目录，即对当前目录中所有文件进行转换。 作者：Latm Ake链接：https://www.zhihu.com/question/20523036/answer/35225920来源：知乎著作权归作者所有，转载请联系作者获得授权。","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/categories/tutorial/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"}]},{"title":"RECURRENT NEURAL NETWORK TUTORIAL, PART 4 – IMPLEMENTING A GRU/LSTM RNN WITH PYTHON AND THEANO","slug":"2016-06-22-recurrent-neural-network-tutorial-part-4-implementing-a-gru-slash-lstm-rnn-with-python-and-theano","date":"2016-06-22T13:42:51.000Z","updated":"2017-01-13T09:47:09.000Z","comments":true,"path":"2016/06/22/2016-06-22-recurrent-neural-network-tutorial-part-4-implementing-a-gru-slash-lstm-rnn-with-python-and-theano/","link":"","permalink":"http://frankchen.xyz/2016/06/22/2016-06-22-recurrent-neural-network-tutorial-part-4-implementing-a-gru-slash-lstm-rnn-with-python-and-theano/","excerpt":"本文中，我们将学习关于LSTM (Long Short Term Memory)网络和 GRUs (Gated Recurrent Units)的知识。LSTM最初由Sepp Hochreiter and Jürgen Schmidhuber于1997年提出，如今是深度学习自然语言处理领域最流行的模型。GRU，出现于2014年，是LSTM的简化版，与LSTM有许多相似的特性。","text":"本文中，我们将学习关于LSTM (Long Short Term Memory)网络和 GRUs (Gated Recurrent Units)的知识。LSTM最初由Sepp Hochreiter and Jürgen Schmidhuber于1997年提出，如今是深度学习自然语言处理领域最流行的模型。GRU，出现于2014年，是LSTM的简化版，与LSTM有许多相似的特性。 ##LSTM 网络 在第三部分我们提到了梯度消失问题妨碍了标准的RNN学习长期依赖问题。LSTM被设计于用gate结构解决梯度消失问题。为了理解这个机制，我们来看看LSTM如何计算隐藏状态$$s_t$$（其中小圆圈代表Hadamard product，即同型矩阵各元素对应相乘，不同于矩阵点乘）。 式子看起来复杂，一步一步来理解实则简单。首先，LSTM的一层代表的只是另一种计算隐藏层的方法。之前我们计算了隐藏状态$$s_t = tanh(Ux_t + WS_{s-1})$$。对于当前的单元，输入是$$t$$时刻的$$x_t$$，而$$s_{t-1}$$是之前的隐藏状态，输出是新的隐藏状态$$s_t$$。其实，LSTM做的事情是完全一样的，只不过换了种方式，这也是理解LSTM的核心。我们可以把LSTM单元看作是黑盒子，给予其当前输入和之前的隐藏状态，它可以输出下一个隐藏状态。 把这个牢记于心，我们开始来阐述LSTM如何计算隐藏状态。关于这一点，详细可看这篇文章，这里我们只作简短描述： $$i,f,o$$分别被称为输入门、遗忘门和输出门。注意到，它们具有相同的等式，只是参数矩阵不同。它们之所以被称为“门”，是因为sigmoid函数将向量值压缩到0和1之间，再与另一个向量相乘，我们因此决定向量的多少“通过”。输入门决定当前输入计算出来的状态的多少成分被通过，遗忘门决定之前的状态有多少可以被保留到之后，输出门决定当前的状态有多少被传送到外层的网络（高层网络和下一时刻）。这些门的维度都是$$d_s$$，即隐藏层的大小。 $$g$$是一个候选的隐藏状态，基于当前的输入和之前的隐藏状态计算而出。其与vanilla RNN的计算等式相同，只是把参数$$U,W$$改名为$$U^g,W^g$$。和RNN的直接将$$g$$作为心的隐藏状态不同，我们将其通过一个输入门来决定保留它的多少成分。 $$c_t$$是单元的内部的记忆，它由之前的记忆$$c_{t-1}$$通过遗忘门再加上新计算出来的隐藏状态$$g$$通过输入门计算得出。因此，它代表了旧的记忆与新的输入的结合。我们可以选择全部忽略旧的记忆（遗忘门全部置零），或者忽略全部的计算出的新状态（输入门全部置零），但是通常来说，我们可能更希望介于两者之间。 给定记忆$$c_t$$，我们最终通过让记忆和输出门相乘计算出输出隐藏层状态$$s_t$$。在网络内，不是所有的内部记忆都与其他单元使用的隐藏状态有关。 换一种说法，我们可以将标准的RNN看作是特殊的LSTM，如果我们将遗忘门全部置零，输入门全部置一，输出门全部置一，我们就几乎得到一个标准的RNN。通过门机制，LSTM可以操作记忆从而解决长期依赖问题。 注意到，还有许多的LSTM变种，一种添加上“猫眼”结构，它的门同时取决于之前的隐藏状态$$s_{t-1}$$和之前的内部状态$$c_{t-1}$$。 LSTM: A Search Space Odyssey 实验观察了许多不同的LSTM机制。 ##GRU网络 GRU的理念类似于LSTM，其等式如下： GRU拥有两个门，称为重置门$$r$$和更新门$$z$$。直观来说，重置门决定如何联合新的输入和之前的记忆，而更新门决定留下多少之前的记忆。如果将重置门全部置一并且更新门全部置零，那么我们又得到了我们原始的RNN了。GRU的解决长期依赖的理念和LSTM基本类似，以下是一些不同之处： 两个门VS三个门 GRU不处理内层记忆$$c_t$$ 输入门和遗忘门被组合成更新门$$z$$,重置门$$r$$直接连接之前的隐藏状态。因此， 计算输出是不加上第二个非线性变换 ##GRU VS LSTM 如今你认识了两个对抗梯度消失的模型","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/categories/tutorial/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankchen.xyz/tags/Deep-Learning/"},{"name":"RNN","slug":"RNN","permalink":"http://frankchen.xyz/tags/RNN/"}]},{"title":"RECURRENT NEURAL NETWORKS TUTORIAL, PART 3 – BACKPROPAGATION THROUGH TIME AND VANISHING GRADIENTS","slug":"2016-06-22-recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients","date":"2016-06-22T07:42:14.000Z","updated":"2017-01-13T09:47:07.000Z","comments":true,"path":"2016/06/22/2016-06-22-recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/","link":"","permalink":"http://frankchen.xyz/2016/06/22/2016-06-22-recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/","excerpt":"在之前的部分我们实现了RNN，但是并未深入探究时间反向传播算法，本文将对这一点作详细说明。我们将了解关于梯度消失问题的知识，它促使了LSTM和GRU的出现，而这两者都是NLP领域非常常见的模型。 ##BACKPROPAGATION THROUGH TIME (BPTT) 首先我们回顾一下RNN的基本等式：","text":"在之前的部分我们实现了RNN，但是并未深入探究时间反向传播算法，本文将对这一点作详细说明。我们将了解关于梯度消失问题的知识，它促使了LSTM和GRU的出现，而这两者都是NLP领域非常常见的模型。 ##BACKPROPAGATION THROUGH TIME (BPTT) 首先我们回顾一下RNN的基本等式： 我们也定义了损失函数（交叉熵）： 在这里，$$y_t$$是 $$t$$时刻的正确的词语， $$\\tilde{y_t}$$ 是我们的预测。因为我们把一整个序列（句子）当做是输入，那么错误等同于每个时间step（词语）的错误的和。 ![](/images/2016/06/rnn-bptt1.png） 需要注意，我们的目标是计算基于参数$$U, V, W$$错误的梯度，并且通过SGD来学习到好的参数。类似于我们将错误相加的做法，对于一个训练样本，我们将各个时间点的梯度相加。 $$\\frac{\\partial{E}}{\\partial{W}} = \\sum_{t} \\frac{\\partial{E_t}}{\\partial{W}}$$ 我们使用链式求导法则来计算这些梯度，这就是反向传播算法:从错误处反向计算。以下我们使用$$E_3$$作为例子。 其中，$$z_3 = V s_3$$，并且$$\\otimes$$指的是向量外积。在这里我们需要注意到，$$\\frac{\\partial{E_3}}{\\partial{V}}$$只取决于当前时刻的值$$\\tilde{y_3}, y_3, s_3$$。如果你明确了这一点，那么计算$$V$$的梯度只是矩阵计算罢了。 但是，对于$$\\frac{\\partial{E_3}}{\\partial{W}}$$和$$V$$就不一样了。我们写出链式法则： 可以看到，$$s_3 = \\tanh(U x_t + W s_2)$$取决于$$s_2$$，而$$s_2$$又取决于$$W$$和$$s_1$$，以此类推。所以我们计算$$W$$的偏导，我们不能把$$s_2$$当做一个常量，相反我们需要一遍遍的应用链式法则： 我们把每个时间点对于梯度贡献综合起来。换句话说，因为$$W$$在直到我们需要输出的时刻都被用到，所以我们需要计算$$t=3$$时刻直到$$t=0$$时刻： 这其实和深度前馈神经网络里的标准的反向传播算法是类似的。主要的不同点在于我们把每个时间点的$$W$$的梯度综合起来。传统的神经网络的不同层之间不共享参数，于是我们也不需要综合什么。但是在我看来，BPTT只不过是在没有展开的RNN上的标准BP算法的别名罢了。类似于标准的BP算法，你也可以定义一个徳塔项来表示反向传播的内容。例如：$$\\delta_{2}^{(3)} = \\frac{\\partial{E_3}}{\\partial{z_2}} = \\frac{\\partial{E_3}}{\\partial{s_3}} \\frac{\\partial{s_3}}{\\partial{s_2}} \\frac{\\partial{s_2}}{\\partial{z_2}}$$，其中$$z_2 = Ux_2 + Ws_1$$。以此类推。 代码实现BPTT如下：123456789101112131415161718192021222324def bptt(self, x, y): T = len(y) # Perform forward propagation o, s = self.forward_propagation(x) # We accumulate the gradients in these variables dLdU = np.zeros(self.U.shape) dLdV = np.zeros(self.V.shape) dLdW = np.zeros(self.W.shape) delta_o = o delta_o[np.arange(len(y)), y] -= 1. # For each output backwards... for t in np.arange(T)[::-1]: dLdV += np.outer(delta_o[t], s[t].T) # Initial delta calculation: dL/dz delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2)) # Backpropagation through time (for at most self.bptt_truncate steps) for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]: # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step) # Add to gradients at each previous step dLdW += np.outer(delta_t, s[bptt_step-1]) dLdU[:,x[bptt_step]] += delta_t # Update delta for next step dL/dz at t-1 delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2) return [dLdU, dLdV, dLdW] 从代码你也可以观察到RNN不容易训练：由于序列比较长，每次传播需要计算很多层，实践上通常许多人的做法是将传播限制在几步之内。 ##梯度消失问题 之前的部分我们已经讲述了关于RNN的长期依赖问题。为了解这一点，我们来观察上面我们计算过的梯度。 注意到是对于自身的链式法则，如。再注意到，我们这里计算的是向量对向量的偏导，其结果是一个雅可比矩阵。因此可以改写上面的梯度式为： 式子中的雅克比矩阵的二范数具有上限1.这导致激活函数tanh（或者sigmoid）映射所有的值到[-1,1]，那么偏导也被限制在1（对于sigmoid是$$\\frac{1}{4} $$）。 图中我们可以看到，在tanh函数两边，梯度都接近于0.这导致它之前的梯度也接近于0，那么，与矩阵中的数字多次相乘使得梯度迅速减小，直到接近消失。院出的时间点对于现在的影响接近于0，这就是长期依赖问题的原因。但是，长期依赖问题并不只是对于RNN出现，深度神经网络都具有这个问题，只不过RNN经常要处理长序列问题，所以网络层数很多，这个问题就经常出现了。 容易想到的是，与梯度消失问题对应的是，梯度爆炸问题。当雅克比矩阵中的数值较大时，容易出现这个问题。但是，通常来说，对于梯度爆炸，业界关注并不太多。有两个原因，其一，梯度爆炸发生时通常容易发现，因为可能导致程序崩溃之类的后果；其二，通常为梯度设置上限是应对梯度爆炸的简便做法。 那么怎么应对梯度弥散问题呢？首先，更好的初始化权重可以减少梯度弥散的效果；其次，使用正则化；更加通用的方法是使用ReLU作为激活函数，其梯度要么是1要么是0，所以更少的可能出现梯度弥散的问题。另外，更加流行的做法则是使用 Long Short-Term Memory (LSTM)或者Gated Recurrent Unit (GRU)，LSTM出现于1997年，也许是NLP领域近期最流行的网络结构。GRU，出现于2014年，是LSTM的简化版本。两种网络都是为了应对梯度弥散和长期依赖问题。我们将会在之后的教程中涉及它们。","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/categories/tutorial/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankchen.xyz/tags/Deep-Learning/"},{"name":"RNN","slug":"RNN","permalink":"http://frankchen.xyz/tags/RNN/"}]},{"title":"Vim 练级攻略一","slug":"2016-06-22-vim-lian-ji-gong-lue-1","date":"2016-06-22T02:50:35.000Z","updated":"2016-11-11T13:50:33.000Z","comments":true,"path":"2016/06/22/2016-06-22-vim-lian-ji-gong-lue-1/","link":"","permalink":"http://frankchen.xyz/2016/06/22/2016-06-22-vim-lian-ji-gong-lue-1/","excerpt":"你想以最快的速度学习人类史上最好的文本编辑器VIM吗？你先得懂得如何在VIM幸存下来，然后一点一点地学习各种戏法。 Vim: the Six Billion Dollar editor Better, Stronger, Faster. 学习Vim并且其会成为你最后一个使用的文本编辑器。没有比这个更好的文本编辑器了，非常地难学，但是却不可思议地好用。","text":"你想以最快的速度学习人类史上最好的文本编辑器VIM吗？你先得懂得如何在VIM幸存下来，然后一点一点地学习各种戏法。 Vim: the Six Billion Dollar editor Better, Stronger, Faster. 学习Vim并且其会成为你最后一个使用的文本编辑器。没有比这个更好的文本编辑器了，非常地难学，但是却不可思议地好用。我建议下面这四个步骤： 存活 感觉良好 觉得更好，更强，更快 使用VIM的超能力 当你走完这篇文章，你会成为一个vim的 superstar。 在开始学习以前，我需要给你一些警告： 学习vim在开始时是痛苦的。 需要时间 需要不断地练习，就像你学习一个乐器一样。 不要期望你能在3天内把vim练得比别的编辑器更有效率。 事实上，你需要2周时间的苦练，而不是3天。 ##第一级 – 存活 安装Vim 启动Vim 什么也不干！请先阅读 当你安装好一个编辑器后，你一定会想在其中输入点什么东西，然后看看这个编辑器是什么样子。但vim不是这样的，请按照下面的命令操作： 启动Vim后，vim在 Normal 模式下。 让我们进入 Insert 模式，请按下键 i 。(你会看到vim左下角有一个–insert–字样，表示，你可以以插入的方式输入了） 此时，你可以输入文本了，就像你用“记事本”一样。 如果你想返回 Normal 模式，请按 ESC 键。 现在，你知道如何在 Insert 和 Normal 模式下切换了。下面是一些命令，可以让你在 Normal 模式下幸存下来（箭头不代表按键）： i → Insert 模式，按 ESC 回到 Normal 模式. x → 删当前光标所在的一个字符。 :wq → 存盘 + 退出 (:w 存盘, :q 退出) （陈皓注：:w 后可以跟文件名） dd → 删除当前行，并把删除的行存到剪贴板里 p → 粘贴剪贴板 推荐: hjkl (强例推荐使用其移动光标，但不必需) →你也可以使用光标键 (←↓↑→). 注: j 就像下箭头。 :help → 显示相关命令的帮助。你也可以就输入 :help 而不跟命令。（注：退出帮助需要输入:q） 你能在vim幸存下来只需要上述的那5个命令，你就可以编辑文本了，你一定要把这些命令练成一种下意识的状态。于是你就可以开始进阶到第二级了。 当是，在你进入第二级时，需要再说一下 Normal 模式。在一般的编辑器下，当你需要copy一段文字的时候，你需要使用 Ctrl 键，比如：Ctrl-C。也就是说，Ctrl键就好像功能键一样，当你按下了功能键Ctrl后，C就不在是C了，而且就是一个命令或是一个快键键了，在VIM的Normal模式下，所有的键就是功能键了。这个你需要知道。 标记: 下面的文字中，如果是 Ctrl-λ我会写成 . 以 : 开始的命令你需要输入 回车，例如 — 如果我写成 :q 也就是说你要输入 :q. ##第二级 – 感觉良好 上面的那些命令只能让你存活下来，现在是时候学习一些更多的命令了，下面是我的建议：（注：所有的命令都需要在Normal模式下使用，如果你不知道现在在什么样的模式，你就狂按几次ESC键） 各种插入模式 a → 在光标后插入 o → 在当前行后插入一个新行 O → 在当前行前插入一个新行 cw → 替换从光标所在位置后到一个单词结尾的字符 简单的移动光标 0 → 数字零，到行头 ^ → 到本行第一个不是blank字符的位置（所谓blank字符就是空格，tab，换行，回车等） $ → 到本行行尾 g_ → 到本行最后一个不是blank字符的位置。 /pattern → 搜索 pattern 的字符串（注：如果搜索出多个匹配，可按n键到下一个） 拷贝/粘贴 （注：p/P都可以，p是表示在当前位置之后，P表示在当前位置之前） P → 粘贴 yy → 拷贝当前行当行于 ddP Undo/Redo u → undo &lt;C-r&gt; → redo 打开/保存/退出/改变文件(Buffer) :e → 打开一个文件 :w → 存盘 :saveas → 另存为 :x， ZZ 或 :wq → 保存并退出 (:x 表示仅在需要时保存，ZZ不需要输入冒号并回车) :q! → 退出不保存 :qa! 强行退出所有的正在编辑的文件，就算别的文件有更改。 :bn 和 :bp → 你可以同时打开很多文件，使用这两个命令来切换下一个或上一个文件。（注：我喜欢使用:n到下一个文件） 花点时间熟悉一下上面的命令，一旦你掌握他们了，你就几乎可以干其它编辑器都能干的事了。但是到现在为止，你还是觉得使用vim还是有点笨拙，不过没关系，你可以进阶到第三级了。下一篇我们将介绍第三个和第四个阶段。 (注：图片中的U83R 1337指的是德语中的uber leet，合起来就是Super Cool的意思，所以图片中文字指的是Super cool use Vim!)","categories":[{"name":"Vim","slug":"Vim","permalink":"http://frankchen.xyz/categories/Vim/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/tags/tutorial/"}]},{"title":"Vim 练级攻略二","slug":"2016-06-22-vim-lian-ji-gong-er","date":"2016-06-22T02:50:35.000Z","updated":"2016-11-11T13:50:33.000Z","comments":true,"path":"2016/06/22/2016-06-22-vim-lian-ji-gong-er/","link":"","permalink":"http://frankchen.xyz/2016/06/22/2016-06-22-vim-lian-ji-gong-er/","excerpt":"第三级 – 更好，更强，更快先恭喜你！你干的很不错。我们可以开始一些更为有趣的事了。在第三级，我们只谈那些和vi可以兼容的命令。 更好下面，让我们看一下vim是怎么重复自己的： . → (小数点) 可以重复上一次的命令 N → 重复某个命令N次","text":"第三级 – 更好，更强，更快先恭喜你！你干的很不错。我们可以开始一些更为有趣的事了。在第三级，我们只谈那些和vi可以兼容的命令。 更好下面，让我们看一下vim是怎么重复自己的： . → (小数点) 可以重复上一次的命令 N → 重复某个命令N次 下面是一个示例，找开一个文件你可以试试下面的命令： 2dd → 删除2行 3p → 粘贴文本3次 100idesu [ESC] → 会写下 “desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu “ . → 重复上一个命令—— 100 “desu “. → 重复 3 次 “desu” (注意：不是 300，你看，VIM多聪明啊). ###更强 你要让你的光标移动更有效率，你一定要了解下面的这些命令，千万别跳过。 NG → 到第 N 行 （注：注意命令中的G是大写的，另我一般使用 : N 到第N行，如 :137 到第137行） gg → 到第一行。（注：相当于1G，或 :1） G → 到最后一行。 按单词移动： w → 到下一个单词的开头。 e → 到下一个单词的结尾。 ![](/images/2016/06/word_moves.jpg) &gt; 如果你认为单词是由默认方式，那么就用小写的e和w。默认上来说，一个单词由字母，数字和下划线组成（注：程序变量） &gt; 如果你认为单词是由blank字符分隔符，那么你需要使用大写的E和W。（注：程序语句） 下面，让我来说说最强的光标移动： % : 匹配括号移动，包括 (, {, [. （注：你需要把光标先移到括号上） * 和 #: 匹配光标当前所在的单词，移动光标到下一个（或上一个）匹配单词（*是下一个，#是上一个） 更快你一定要记住光标的移动，因为很多命令都可以和这些移动光标的命令连动。很多命令都可以如下来干： &lt;start position&gt;&lt;command&gt;&lt;end position&gt; 例如 0y$ 命令意味着： 0 → 先到行头 y → 从这里开始拷贝 $ → 拷贝到本行最后一个字符 你也可以输入 ye，从当前位置拷贝到本单词的最后一个字符。 你也可以输入 y2/foo 来拷贝2个 “foo” 之间的字符串。 还有很多时间并不一定你就一定要按y才会拷贝，下面的命令也会被拷贝： d (删除 ) v (可视化的选择) gU (变大写) gu (变小写) 等等 （注：可视化选择是一个很有意思的命令，你可以先按v，然后移动光标，你就会看到文本被选择，然后，你可能d，也可y，也可以变大写等） 第四级 – Vim 超能力你只需要掌握前面的命令，你就可以很舒服的使用VIM了。但是，现在，我们向你介绍的是VIM杀手级的功能。下面这些功能是我只用vim的原因。 在当前行上移动光标: 0 ^ $ f F t T , ; 0 → 到行头 ^ → 到本行的第一个非blank字符 $ → 到行尾 g_ → 到本行最后一个不是blank字符的位置。 fa → 到下一个为a的字符处，你也可以fs到下一个为s的字符。 t, → 到逗号前的第一个字符。逗号可以变成其它字符。 3fa → 在当前行查找第三个出现的a。 F 和 T → 和 f 和 t 一样，只不过是相反方向。 还有一个很有用的命令是 dt” → 删除所有的内容，直到遇到双引号—— “。 ###区域选择 &lt;action&gt;a&lt;object&gt; 或 &lt;action&gt;i&lt;object&gt; 在visual 模式下，这些命令很强大，其命令格式为 &lt;action&gt;a&lt;object&gt; 和 &lt;action&gt;i&lt;object&gt; action可以是任何的命令，如 d (删除), y (拷贝), v (可以视模式选择)。 object 可能是： w 一个单词， W 一个以空格为分隔的单词， s 一个句字， p 一个段落。也可以是一个特别的字符：”、 ‘、 )、 }、 ]。 假设你有一个字符串 (map (+) (“foo”)).而光标键在第一个 o 的位置。 vi” → 会选择 foo. va” → 会选择 “foo”. vi) → 会选择 “foo”. va) → 会选择(“foo”). v2i) → 会选择 map (+) (“foo”) v2a) → 会选择 (map (+) (“foo”)) ###块操作: 块操作，典型的操作： 0 I– [ESC] ^ → 到行头 → 开始块操作 → 向下移动 (你也可以使用hjkl来移动光标，或是使用%，或是别的) I– [ESC] → I是插入，插入“–”，按ESC键来为每一行生效。 ###自动提示： 和 在 Insert 模式下，你可以输入一个词的开头，然后按 或是，自动补齐功能就出现了…… 宏录制： qa 操作序列 q, @a, @@如下 qa 把你的操作记录在寄存器 a。 于是@a 会replay被录制的宏。 @@是一个快捷键用来replay最新录制的宏。 示例：在一个只有一行且这一行只有“1”的文本中，键入如下命令： qaYpq→ qa 开始录制 Yp 复制行. 增加1. q 停止录制. @a → 在1下面写下 2 @@ → 在2 正面写下3 现在做 100@@ 会创建新的100行，并把数据增加到 103. ###可视化选择： v,V,&lt;C-v&gt; 前面，我们看到了 的示例 （在Windows下应该是），我们可以使用 v 和 V。一但被选好了，你可以做下面的事： J → 把所有的行连接起来（变成一行） &lt; 或 &gt; → 左右缩进 = → 自动给缩进 （注：这个功能相当强大，我太喜欢了） 在所有被选择的行后加上点东西： &lt;C-v&gt; 选中相关的行 (可使用 j 或 或是 /pattern 或是 % 等……) $ 到行最后 A, 输入字符串，按 ESC。 ###分屏: :split 和 vsplit. 下面是主要的命令，你可以使用VIM的帮助 :help split. 你可以参考本站以前的一篇文章VIM分屏。 :split → 创建分屏 (:vsplit创建垂直分屏) &lt;C-w&gt;&lt;dir&gt; : dir就是方向，可以是 hjkl 或是 ←↓↑→ 中的一个，其用来切换分屏。 _ (或 |) : 最大化尺寸 (| 垂直分屏) + (或 -) : 增加尺寸 ##结束语 上面是作者最常用的90%的命令。 我建议你每天都学1到2个新的命令。 在两到三周后，你会感到vim的强大的。 有时候，学习VIM就像是在死背一些东西。 幸运的是，vim有很多很不错的工具和优秀的文档。 运行vimtutor直到你熟悉了那些基本命令。 其在线帮助文档中你应该要仔细阅读的是 :help usr_02.txt. 你会学习到诸如 !， 目录，寄存器，插件等很多其它的功能。 学习vim就像学弹钢琴一样，一旦学会，受益无穷。","categories":[{"name":"Vim","slug":"Vim","permalink":"http://frankchen.xyz/categories/Vim/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/tags/tutorial/"}]},{"title":"RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO","slug":"2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano","date":"2016-06-15T09:54:18.000Z","updated":"2017-01-13T09:47:18.000Z","comments":true,"path":"2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/","link":"","permalink":"http://frankchen.xyz/2016/06/15/2016-06-15-recurrent-neural-networks-tutorial-part-2-implementing-a-rnn-with-python-numpy-and-theano/","excerpt":"本文将用Python实现完整的RNN，并且用Theano来优化。 语言模型我们的目标是使用RNN建立一个语言模型。以下我们举例说明什么是语言模型。例如，你说了一句包括$$m$$个词语的句子，语言模型可以为这句话出现的概率打分： $$P(w_1,\\cdots,w_m) = \\prod_{i=1}^m P(w_i \\mid w_1,\\cdots,w_{i-1})$$","text":"本文将用Python实现完整的RNN，并且用Theano来优化。 语言模型我们的目标是使用RNN建立一个语言模型。以下我们举例说明什么是语言模型。例如，你说了一句包括$$m$$个词语的句子，语言模型可以为这句话出现的概率打分： $$P(w_1,\\cdots,w_m) = \\prod_{i=1}^m P(w_i \\mid w_1,\\cdots,w_{i-1})$$ 每一个词语的概率都取决于它之前的所有的词的概率。 这样的模型有什么用处呢？ 可以用于机器翻译或者语音识别中的正确句子打分 以概率生成新的句子 注意到在上面的公式内，我们使用了所有的之前的词的概率，实际上这在计算和存储时的耗费都是巨大的，通常而言只会取2~4个词左右。 预处理并训练数据1.标记化原始的文本需要被标记化，例如需要把文本标记为句子，句子标记为词语，并且还需要处理标点符号。我们将使用NLTK的word_tokenize\\sent_tokenize方法。 2.移除低频词移除低频词不管是对于训练和预测都是有帮助的。这里我们设置一个上限vocabulary_size为8000，出现次数少于它的词都会被替换为UNKNOWN_TOKEN输入，而当输出是UNKNOWN_TOKEN时，它将被随机替换为一个不在词表内的词，亦或者持续预测直到不出现UNKNOWN_TOKEN为止。 3.放置句子开始和结束标记为了解句子的开始和结束，我们把SENTENCE_START放置在句子开头，并且把SENTENCE_END放置在句子结尾。 4.建立训练数据的矩阵RNN的输入和输出都是向量而不是字符串，我们需要把词与向量一一对应，通过index_to_word和word_to_index。比如一个训练的例子$$x$$为[0, 179, 341, 416]（注意到其中每个元素都是长度为vocabulary_size的one-hot向量，所以$$x$$实际上是一个矩阵），那么其label-$$y$$为[179, 341, 416, 1]，注意到我们的目标是预测下一个词，所以$$y$$就是$$x$$移动一位，并添加上最后的一个元素（预测词）的结果，其中SENTENCE_START和SENTENCE_END分别为0和1. 123456789101112131415161718192021222324252627282930313233343536373839404142vocabulary_size = 8000unknown_token = \"UNKNOWN_TOKEN\"sentence_start_token = \"SENTENCE_START\"sentence_end_token = \"SENTENCE_END\"# Read the data and append SENTENCE_START and SENTENCE_END tokensprint \"Reading CSV file...\"with open('data/reddit-comments-2015-08.csv', 'rb') as f: reader = csv.reader(f, skipinitialspace=True) reader.next() # Split full comments into sentences sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader]) # Append SENTENCE_START and SENTENCE_END sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]print \"Parsed %d sentences.\" % (len(sentences))# Tokenize the sentences into wordstokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]# Count the word frequenciesword_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))print \"Found %d unique words tokens.\" % len(word`_freq.items())# Get the most common words and build index_to_word and word_to_index vectorsvocab = word_freq.most_common(vocabulary_size-1)index_to_word = [x[0] for x in vocab]index_to_word.append(unknown_token)word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])print \"Using vocabulary size %d.\" % vocabulary_sizeprint \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])# Replace all words not in our vocabulary with the unknown tokenfor i, sent in enumerate(tokenized_sentences): tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]print \"\\nExample sentence: '%s'\" % sentences[0]print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]# Create the training dataX_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences]) 以下是一个训练样本：1234567x:SENTENCE_START what are n't you understanding about this ? ![0, 51, 27, 16, 10, 856, 53, 25, 34, 69]y:what are n't you understanding about this ? ! SENTENCE_END[51, 27, 16, 10, 856, 53, 25, 34, 69, 1] 接下来我们开始建立RNN。 ##建立RNN 总结一下我们的RNN模型的形式。初始输入$$x$$是一个代表一条句子的矩阵，每一时刻的输入$$x_t$$是一个代表一个词语的向量，每一时刻的输出$$o_t$$也是一个向量，其中每个元素代表词表内每一个词被预测的概率。 RNN中的等式：$$s_t = tanh(Ux_t + Ws_{t-1})o_t = softmax(Vs_t)$$ 介绍一下各个变量的维度：假设我们的词表大小$$C$$为8000，隐藏层大小$$H$$为100，可以把隐藏层大小理解为网络内存的大小，内存越大，网络能记忆的信息也越多，但是也要耗费更多的代价来计算。综上，我们的模型参数维度为： $$x_t \\in \\Bbb{R}^{8000} \\ o_t \\in \\Bbb{R}^{8000} \\ s_t \\in \\Bbb{R}^{100} \\ U_t \\in \\Bbb{R}^{100 \\times 8000} \\ V_t \\in \\Bbb{R}^{8000 \\times 100} \\ W_t \\in \\Bbb{R}^{100 \\times 100} \\$$ 其中$$U，V，W$$是网络的参数，根据上面的等式，我们需要学习$$2HC+H^2$$个参数，由于$$x_t$$是稀疏的one-hot向量，所以其与$$U$$的乘积一步即可算出，$$W$$和$$S_t$$的维度都比较小，所以最繁琐的计算就是$$VS_t$$了。 ###初始化 通过Numpy实现第一个版本，对$$U，V，W$$的初始化比较tricky，通常是把它们的初始值置于$$[-\\frac{1}{\\sqrt n},\\frac{1}{\\sqrt n}]$$较好。 1234567891011class RNNNumpy: def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4): # Assign instance variables self.word_dim = word_dim self.hidden_dim = hidden_dim self.bptt_truncate = bptt_truncate # Randomly initialize the network parameters self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim)) self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim)) self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim)) 其中word_dim是词表大小，hidden_dim是隐藏层大小。 ###前向计算 以下是前向计算（预测词语的概率）的过程： 1234567891011121314151617def forward_propagation(self, x): # The total number of time steps T = len(x) # During forward propagation we save all hidden states in s because need them later. # We add one additional element for the initial hidden, which we set to 0 s = np.zeros((T + 1, self.hidden_dim)) s[-1] = np.zeros(self.hidden_dim) # The outputs at each time step. Again, we save them for later. o = np.zeros((T, self.word_dim)) # For each time step... for t in np.arange(T): # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector. s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1])) o[t] = softmax(self.V.dot(s[t])) return [o, s]RNNNumpy.forward_propagation = forward_propagation 我们同时返回输出及隐藏层状态，隐藏层状态之后会用于梯度计算。$$o_t$$是词表内每个词的概率，但是有时我们需要直接预测出概率最高的词： 123456def predict(self, x): # Perform forward propagation and return index of the highest score o, s = self.forward_propagation(x) return np.argmax(o, axis=1)RNNNumpy.predict = predict 尝试输出：12345678910111213141516171819np.random.seed(10)model = RNNNumpy(vocabulary_size)o, s = model.forward_propagation(X_train[10])print o.shapeprint o(45, 8000)[[ 0.00012408 0.0001244 0.00012603 ..., 0.00012515 0.00012488 0.00012508] [ 0.00012536 0.00012582 0.00012436 ..., 0.00012482 0.00012456 0.00012451] [ 0.00012387 0.0001252 0.00012474 ..., 0.00012559 0.00012588 0.00012551] ..., [ 0.00012414 0.00012455 0.0001252 ..., 0.00012487 0.00012494 0.0001263 ] [ 0.0001252 0.00012393 0.00012509 ..., 0.00012407 0.00012578 0.00012502] [ 0.00012472 0.0001253 0.00012487 ..., 0.00012463 0.00012536 0.00012665]] 对上面句子（包括45个单词）中的每个词，模型都预测了8000个概率值。因为模型参数这时候是随机初始值，所以预测也是随机的。接下来我们给出预测的词的位置：1234567predictions = model.predict(X_train[10])print predictions.shapeprint predictions(45,)[1284 5221 7653 7430 1013 3562 7366 4860 2212 6601 7299 4556 2481 238 2539 21 6548 261 1780 2005 1810 5376 4146 477 7051 4832 4991 897 3485 21 7291 2007 6006 760 4864 2182 6569 2800 2752 6821 4437 7021 7875 6912 3575] 接下来我们计算损失。 ###计算损失 我们使用交叉熵函数作为损失函数。若我们有$$N$$个训练样本（text中的词语数），$$C$$个类别（词表大小），预测是$$o$$，label是$$y$$，那么损失计算为： $$L(y,o) = - \\frac{1}{N} \\sum_{n \\in N} y_n \\log o_n$$ 损失函数计算的是我们的预测oo与正确的词yy的差距的大小。通过以下计算损失： 123456789101112131415161718def calculate_total_loss(self, x, y): L = 0 # For each sentence... for i in np.arange(len(y)): o, s = self.forward_propagation(x[i]) # We only care about our prediction of the \"correct\" words correct_word_predictions = o[np.arange(len(y[i])), y[i]] # Add to the loss based on how off we were L += -1 * np.sum(np.log(correct_word_predictions)) return Ldef calculate_loss(self, x, y): # Divide the total loss by the number of training examples N = np.sum((len(y_i) for y_i in y)) return self.calculate_total_loss(x,y)/NRNNNumpy.calculate_total_loss = calculate_total_lossRNNNumpy.calculate_loss = calculate_loss 让我们稍微检验一下，如果我们词表大小为$$C$$，那么开始时每个词被随机预测的概率为$$\\frac{1}{C}$$，那么损失为$$L = - \\frac{1}{C} N \\log \\ \\frac{1}{C} = \\log C$$：123 Limit to 1000 examples to save timeprint \"Expected Loss for random predictions: %f\" % np.log(vocabulary_size)print \"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]) 很接近。这里我们应当记住，如果data很大，那么计算损失会变得非常耗费时间。 ###通过SGD和BPTT（BACKPROPAGATION THROUGH TIME ）训练RNN 给定训练样本$$(x,y)$$我们需要计算梯度$$\\frac{\\partial L}{\\partial {U}},\\frac{\\partial L}{\\partial {V}},\\frac{\\partial L}{\\partial {W}}$$。通过以下代码实现BPTT：12345678910111213141516171819202122232425def bptt(self, x, y): T = len(y) # Perform forward propagation o, s = self.forward_propagation(x) # We accumulate the gradients in these variables dLdU = np.zeros(self.U.shape) dLdV = np.zeros(self.V.shape) dLdW = np.zeros(self.W.shape) delta_o = o delta_o[np.arange(len(y)), y] -= 1. # For each output backwards... for t in np.arange(T)[::-1]: dLdV += np.outer(delta_o[t], s[t].T) # Initial delta calculation delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2)) # Backpropagation through time (for at most self.bptt_truncate steps) for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]: # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step) dLdW += np.outer(delta_t, s[bptt_step-1]) dLdU[:,x[bptt_step]] += delta_t # Update delta for next step delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2) return [dLdU, dLdV, dLdW]RNNNumpy.bptt = bptt 接下来我们测试梯度。 ###测试梯度 实现BP算法过程中，测试梯度是一个良好的习惯。通过以下公式： $$\\frac{\\partial L}{\\partial{\\theta}} \\approx \\lim_{h \\rightarrow0} \\frac{J(\\theta + h)-J(\\theta - h)}{2h}$$ 使用以上的公式来测试梯度，同样，由于需要计算所以的参数，梯度测试也是很耗时间的，在部分数据上测试是比较好的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def gradient_check(self, x, y, h=0.001, error_threshold=0.01): # Calculate the gradients using backpropagation. We want to checker if these are correct. bptt_gradients = self.bptt(x, y) # List of all parameters we want to check. model_parameters = ['U', 'V', 'W'] # Gradient check for each parameter for pidx, pname in enumerate(model_parameters): # Get the actual parameter value from the mode, e.g. model.W parameter = operator.attrgetter(pname)(self) print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)) # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ... it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: ix = it.multi_index # Save the original value so we can reset it later original_value = parameter[ix] # Estimate the gradient using (f(x+h) - f(x-h))/(2*h) parameter[ix] = original_value + h gradplus = self.calculate_total_loss([x],[y]) parameter[ix] = original_value - h gradminus = self.calculate_total_loss([x],[y]) estimated_gradient = (gradplus - gradminus)/(2*h) # Reset parameter to original value parameter[ix] = original_value # The gradient for this parameter calculated using backpropagation backprop_gradient = bptt_gradients[pidx][ix] # calculate The relative error: (|x - y|/(|x| + |y|)) relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient)) # If the error is to large fail the gradient check if relative_error &gt; error_threshold: print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix) print \"+h Loss: %f\" % gradplus print \"-h Loss: %f\" % gradminus print \"Estimated_gradient: %f\" % estimated_gradient print \"Backpropagation gradient: %f\" % backprop_gradient print \"Relative Error: %f\" % relative_error return it.iternext() print \"Gradient check for parameter %s passed.\" % (pname)RNNNumpy.gradient_check = gradient_check# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.grad_check_vocab_size = 100np.random.seed(10)model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)model.gradient_check([0,1,2,3], [1,2,3,4]) 接下来我们实现SGD。 ###实现SGD 通过两步实现： sdg_step计算计算梯度对每个batch更新 外部循环迭代整个训练数据并且调整学习率 123456789def numpy_sdg_step(self, x, y, learning_rate): # Calculate the gradients dLdU, dLdV, dLdW = self.bptt(x, y) # Change parameters according to gradients and learning rate self.U -= learning_rate * dLdU self.V -= learning_rate * dLdV self.W -= learning_rate * dLdWRNNNumpy.sgd_step = numpy_sdg_step 12345678910111213141516171819202122232425262728Outer SGD Loop# - model: The RNN model instance# - X_train: The training data set# - y_train: The training data labels# - learning_rate: Initial learning rate for SGD# - nepoch: Number of times to iterate through the complete dataset# - evaluate_loss_after: Evaluate the loss after this many epochsdef train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5): # We keep track of the losses so we can plot them later losses = [] num_examples_seen = 0 for epoch in range(nepoch): # Optionally evaluate the loss if (epoch % evaluate_loss_after == 0): loss = model.calculate_loss(X_train, y_train) losses.append((num_examples_seen, loss)) time = datetime.now().strftime('%Y-%m-%d %H:%M:%S') print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss) # Adjust the learning rate if loss increases if (len(losses) &gt; 1 and losses[-1][1] &gt; losses[-2][1]): learning_rate = learning_rate * 0.5 print \"Setting learning rate to %f\" % learning_rate sys.stdout.flush() # For each training example... for i in range(len(y_train)): # One SGD step model.sgd_step(X_train[i], y_train[i], learning_rate) num_examples_seen += 1 完成。我们来测试一下训练耗费多长的时间： 123np.random.seed(10)model = RNNNumpy(vocabulary_size)%timeit model.sgd_step(X_train[10], y_train[10], 0.005) 可以看到在我的机器上需要SGD的每一步需要180ms，这代表整个训练将耗费数天甚至更多。我们可以通过许多的方法来加速这一过程，如改善代码和调整模型。这里我们希望使用GPU来加速。在这之前，我们先测试一下SGD的效果： 1234np.random.seed(10)# Train on a small subset of the data to see what happensmodel = RNNNumpy(vocabulary_size)losses = train_with_sgd(model, X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1) 123456789102016-06-13 16:59:46: Loss after num_examples_seen=0 epoch=0: 8.9874252016-06-13 16:59:56: Loss after num_examples_seen=100 epoch=1: 8.9762702016-06-13 17:00:06: Loss after num_examples_seen=200 epoch=2: 8.9602122016-06-13 17:00:16: Loss after num_examples_seen=300 epoch=3: 8.9304302016-06-13 17:00:26: Loss after num_examples_seen=400 epoch=4: 8.8622642016-06-13 17:00:37: Loss after num_examples_seen=500 epoch=5: 6.9135702016-06-13 17:00:46: Loss after num_examples_seen=600 epoch=6: 6.3024932016-06-13 17:00:56: Loss after num_examples_seen=700 epoch=7: 6.0149952016-06-13 17:01:06: Loss after num_examples_seen=800 epoch=8: 5.8338772016-06-13 17:01:16: Loss after num_examples_seen=900 epoch=9: 5.710718 看起来，SGD起到了效果。 ##通过Theano和GPU训练 123enp.random.seed(10)model = RNNTheano(vocabulary_size)%timeit model.sgd_step(X_train[10], y_train[10], 0.005) 这次一次SGD步骤耗费为73.7ms。 这里我们直接使用训练好的的参数：123456from utils import load_model_parameters_theano, save_model_parameters_theanomodel = RNNTheano(vocabulary_size, hidden_dim=50)# losses = train_with_sgd(model, X_train, y_train, nepoch=50)# save_model_parameters_theano('./data/trained-model-theano.npz', model)load_model_parameters_theano('./data/trained-model-theano.npz', model) ###生成语句 使用如下生成语句：123456789101112131415161718192021222324def generate_sentence(model): # We start the sentence with the start token new_sentence = [word_to_index[sentence_start_token]] # Repeat until we get an end token while not new_sentence[-1] == word_to_index[sentence_end_token]: next_word_probs = model.forward_propagation(new_sentence) sampled_word = word_to_index[unknown_token] # We don't want to sample unknown words while sampled_word == word_to_index[unknown_token]: samples = np.random.multinomial(1, next_word_probs[-1]) sampled_word = np.argmax(samples) new_sentence.append(sampled_word) sentence_str = [index_to_word[x] for x in new_sentence[1:-1]] return sentence_strnum_sentences = 10senten_min_length = 7for i in range(num_sentences): sent = [] # We want long sentences, not sentences with one or two words while len(sent) &lt; senten_min_length: sent = generate_sentence(model) print \" \".join(sent) 得到结果为： no to blame their stuff go at all . consider via under gear but equal every game . no similar work on the ui birth a ce nightmare . the challenging what is absolutely hard . me do you research getting +2 . ugh is much good , no . me so many different lines hair . probably not very a bot or gain . correct this is affected so why ? register but a grown gun environment . 看起来还不错！不过还是有一些缺陷，这些都是由于vanilla RNN不能解决长期依赖问题导致的。下一篇我们将讨论BPTT并且关注梯度消失/爆炸问题。","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/categories/tutorial/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankchen.xyz/tags/Deep-Learning/"},{"name":"RNN","slug":"RNN","permalink":"http://frankchen.xyz/tags/RNN/"}]},{"title":"ubuntu下修改DNS并且避免重启失效的方法","slug":"2016-06-07-edit-dns-of-ubuntu","date":"2016-06-07T10:19:51.000Z","updated":"2016-11-11T13:50:33.000Z","comments":true,"path":"2016/06/07/2016-06-07-edit-dns-of-ubuntu/","link":"","permalink":"http://frankchen.xyz/2016/06/07/2016-06-07-edit-dns-of-ubuntu/","excerpt":"","text":"安装好Ubuntu之后设置了静态IP地址，再重启后就无法解析域名。想重新设置一下DNS，打开/etc/resolv.conf： 123cat /etc/resolv.conf# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)# DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN 也就是说，这个文件是resolvconf程序动态创建的，不要直接手动编辑，修改将被覆盖。 试了试，重启果然失效了，若不解决每次重启都得修改DNS，那多麻烦啊！ 还好找到如下办法： 修改/etc/resolvconf/resolv.conf.d/base（这个文件默认是空的），在其内插入： 12nameserver 8.8.8.8nameserver 8.8.4.4 保存后执行：[sudo] resolvconf -u,即可。","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/categories/tutorial/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://frankchen.xyz/tags/Linux/"}]},{"title":"RECURRENT NEURAL NETWORKS TUTORIAL, PART 1 – INTRODUCTION TO RNNS","slug":"2016-06-06-recurrent-neural-networks-tutorial-part-1-introduction-to-rnns","date":"2016-06-06T11:18:44.000Z","updated":"2017-01-13T09:47:17.000Z","comments":true,"path":"2016/06/06/2016-06-06-recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/","link":"","permalink":"http://frankchen.xyz/2016/06/06/2016-06-06-recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/","excerpt":"RNN，也即是递归神经网络，是许多NLP任务的流行处理模型。本部分中将简介RNN。 本部分主要实现此模型– recurrent neural network based language model，模型有两个作用： 可以基于出现的概率对句子进行打分，可以对于语法和语义正确性进行评估，从而应用于机器翻译等领域。 可以依据概率生成新的语料。","text":"RNN，也即是递归神经网络，是许多NLP任务的流行处理模型。本部分中将简介RNN。 本部分主要实现此模型– recurrent neural network based language model，模型有两个作用： 可以基于出现的概率对句子进行打分，可以对于语法和语义正确性进行评估，从而应用于机器翻译等领域。 可以依据概率生成新的语料。 ##什么是RNN RNN背后的核心理念是利用序列的信息。传统的神经网络常常假设输入（输出）是独立于彼此的，这对于某些应用来说是不可行的，例如NLP任务，如果你需要预测下一个单词是什么，你不可能不用到之前的单词的信息。RNN之中的recurrent指的就是循环往复的意思，网络对于序列数据的每个元素进行同样的操作，网络的输出取决于之前的计算。另一种理解RNN的方法则是把它们看成是有记忆的网络，记忆收集从开始到现在被考虑的信息。理论上RNN可以利用任意时间长度的信息，但是实际上这比较困难。以下是RNN的典型结构： 以上的结构将RNN展开成完整的网络。例如，我们需要处理一个5层的序列，那么就需要展开成5层：每层对应一个词。 $$x_t$$是$$t$$时刻的输入 $$s_t$$是$$t$$时刻的隐藏状态，代表网络的“记忆”，计算公式为$$s_t = f(Ux_t + Ws_{t-1})$$，其中$$f$$通常是tanh函数或者RELU函数。 $$o_t$$是$$t$$时刻的输出，如果我们想预测下一个词，那么计算公式为$$o_t = softmax(Vs_t)$$，指的是在整个词表内词的概率值。 对于以上有几点需要提示： 我们可以把隐藏状态$$s_t$$看成是网络的“记忆”，其捕捉到在之前所有时间的信息。输出$$o_t$$只取决于在$$t$$时刻的记忆。而在实际上，由于长期依赖问题，$$s_t$$很难捕捉到很长时间以前的信息。 RNN每个step中的参数（$$U,V,W$$）是相同的，这使得学习的代价减小许多。 取决于实际任务，我们可能并不需要每个step都有输入和输出。 ##RNN的实际应用以下是RNN在NLP领域的一些实际应用。 ###语言模型与生成语句在给定之前词语的情况下我们希望产生出下一个词语是什么。语言模型的作用就是让我们可以衡量一个句子的可能性，这对于机器翻译是很重要的（可能性越高的更可能正确）。而语言模型的另一个作用则是预测下一个词语是什么，我们可以通过在输出的概率词汇中采样得到，。对于语言模型，输入是一序列的词语（每一个词语都是one-hot表示），输出则是一序列的预测的词语。在训练时我们设置$$o_t = x_{t + 1}$$，因为这一时刻的输出就是下一时刻的输入。 关于语言模型与生成语句的论文： Recurrent neural network based language model Extensions of Recurrent neural network based language model Generating Text with Recurrent Neural Networks ###机器翻译 机器翻译类似于语言模型，其输入为需要翻译的句子，输出则是翻译的目标语言的句子。与语言模型不同的是，我们在输入整个句子之后才输出。 关于机器翻译的论文： A Recursive Recurrent Neural Network for Statistical Machine Translation Sequence to Sequence Learning with Neural Networks Joint Language and Translation Modeling with Recurrent Neural Networks ###语言识别 给予一段说话人语言的声音序列，我们预测出一序列的带有概率的语言成分的序列。 相关论文： Towards End-to-End Speech Recognition with Recurrent Neural Networks ###生成图片描述 与CNN结合，RNN可以为无标签的图片生成描述，模型甚至可以排列好这些描述成更加类似人类语言的形式。 ##训练RNN 训练RNN与典型的神经网络类似，值得注意的是Backpropagation Through Time (BPTT)，并且通过BPTT训练的vanilla RNN由于梯度弥散或者梯度爆炸，不能有效的解决长期依赖的问题。 ##其他的一些RNN 针对vanilla RNN的一些缺陷，近年来许多RNN的变体涌现出来。 双向RNN，主要输入不仅与过去的输入，并且与将来的输入有关的理念。例如我们需要预测一个句子中间缺失的词语。双向RNN的结构很简单，只是把两个RNN在顶部结合，其输出取决于了两个RNN的隐藏状态。 深度（双向）RNN，与双向RNN类似，只是每个step需要训练多层的网络，这使得模型更为强大（也需要更多的数据来训练）。 LSTM网络，对RNN的隐藏层做了改进以解决长期依赖问题，是近来流行的RNN类型。LSTM可以通过gate决定网络需要记住和遗忘多长时间之前的记忆，以此联合之前的状态、记忆和输入。对于LSTM的详细知识可以见此：理解LSTM网络。 ##结论 到此我们对于RNN有了一个基本的认识，之后我们将对其进行代码的实现。","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/categories/tutorial/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankchen.xyz/tags/Deep-Learning/"},{"name":"RNN","slug":"RNN","permalink":"http://frankchen.xyz/tags/RNN/"}]},{"title":"理解LSTM网络","slug":"2016-06-04-understanding-lstm-networks","date":"2016-06-04T09:16:15.000Z","updated":"2017-01-13T08:15:55.000Z","comments":true,"path":"2016/06/04/2016-06-04-understanding-lstm-networks/","link":"","permalink":"http://frankchen.xyz/2016/06/04/2016-06-04-understanding-lstm-networks/","excerpt":"Recurrent Neural Networks(RNN)人类是依靠自己过往的经验来学习，如同我们在读文章时，每一时刻，我们对于当前的概念的理解总要添加上对于之前获取的知识经验，总之，我们的思维是有持续性的。 传统的神经网络模型无法做到这一点，而这也是它的主要缺陷之一。 循环神经网络解决了这个难题，所谓循环，简单来说其结构允许信息在网络内留存，也即是，网络是有记忆的。","text":"Recurrent Neural Networks(RNN)人类是依靠自己过往的经验来学习，如同我们在读文章时，每一时刻，我们对于当前的概念的理解总要添加上对于之前获取的知识经验，总之，我们的思维是有持续性的。 传统的神经网络模型无法做到这一点，而这也是它的主要缺陷之一。 循环神经网络解决了这个难题，所谓循环，简单来说其结构允许信息在网络内留存，也即是，网络是有记忆的。 上图中，表示A表示神经网络的一块，在$$t$$时刻有输入$$X_t$$和输出$$h_t$$，这结构很简单，特别的是中间那一环，表示信息可以在网络内保存和传递。 以上的循环结构也可以拆解为下面的串列结构来理解： 我们可以把一个网络块的循环看成是多个相同的网络之间信息的传递，而这样的结构也就意味着，RNN天生就适合处理与序列相关的数据，如时间序列、NLP中的语言模型、音乐等。 近年来RNN的成功应用是与LSTM划不开联系的。接下来就给大家介绍LSTM网络。 ##长期依赖问题 RNN能够利用之前的信息来帮助当前的处理，这是我们对于RNN的期待。但是RNN能够做到这一点是有条件的：需要解决长期依赖问题。 举个例子，例如我们需要预测如下句子中的最后一个单词：“the clouds are in the sky”，因为有关的信息与其被需要的位置的距离不远，所以RNN可以轻易解决这个问题。 但是当预测需要更早的信息的时候呢？例如“I grew up in France… I speak fluent French.” 这时候有关的信息与其被需要的位置的距离可能很远。而此时，RNN变得很难处理这个问题。 理论上，RNN应该可以解决这类“长期依赖问题”，但是实际上却无法做到。幸运的是，LSTM可以解决这个问题！ LSTM 网络Long Short Term Memory networks，简称为LSTM，是一种可以学习长期依赖的RNN。那么，LSTM为何具有这种能力呢？ 我们先来看看标准的RNN形式： 形式很简单，tanh层接受当前时刻的输入$$X_t$$以及上一时刻传来的记忆信息，输出$$h_t$$及记忆信息。 而LSTM在标准的RNN形式下，网络增加到了四层。 此图中，黄色的框格代表神经网络层，粉红色的圆圈代表逐点运算，例如向量相加或者相乘，单条黑色的线代表向量的传输，合并的黑色的线代表向量的连接，分叉的黑色线代表向量的复制（同一向量传送到不同的方向）。 ##LSTM网络核心理念 LSTM的核心就是网络内上面的那条水平线，称为cell state $$C_t$$，类似于传送带的功能，它在整个网络内直线传送，只做一些线性的变动，它代表着的是RNN中不变的信息。 LSTM具有对cell state进行增减信息的功能，是靠如下的部件–gate完成的。 gate由一个sigmoid层和逐点乘操作构成，我们都知道sigmoid函数输出的是一个介于0和1之间的数，那么gate的作用就显而易见了：通过sigmoid层的输入决定gate的输入有多少能被输入。而LSTM具有三个gate，都是为了保持和控制cell state。 分步理解LSTMLSTM结构的第一步是决定我们应该把多少信息从cell state里面丢弃。这是有一个叫做“forget gate”的部分完成的。其接收$$h_{t-1}$$和$$x_t$$，输出一个0和1之间的数，再与上一时刻的cell state$$C_{t-1}$$做逐点乘。 下一步则是决定把多少的新信息存储在cell state里面。首先，一个称为“input gate layer”的sigmoid层决定要更新的信息的多少，接下来一个tanh层产生一个称为$$\\tilde{C}_t$$的向量，它代表了新的状态值，最后我们整合以上两者，添加到cell state里面去。 决定了要忘记多少信息并添加多少新信息之后，我们就实际来执行这一步： 最后，我们还需要决定输出的内容。输出主要来自于cell state，但是还是要通过一个tanh层并由sigmoid层决定输出多少。","categories":[{"name":"note","slug":"note","permalink":"http://frankchen.xyz/categories/note/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://frankchen.xyz/tags/RNN/"},{"name":"LSTM","slug":"LSTM","permalink":"http://frankchen.xyz/tags/LSTM/"}]},{"title":"Rephil和MapReduce： 描述长尾数据的数学模型","slug":"2016-06-04-rephil-and-mapreduce","date":"2016-06-04T07:02:32.000Z","updated":"2016-11-11T13:50:33.000Z","comments":true,"path":"2016/06/04/2016-06-04-rephil-and-mapreduce/","link":"","permalink":"http://frankchen.xyz/2016/06/04/2016-06-04-rephil-and-mapreduce/","excerpt":"Google Rephil是Google AdSense背后广告相关性计算的头号秘密武器。但是这个系统没有发表过论文。只是其作者（博士Uri Lerner和工程师Mike Yar）在2002年在湾区举办的几次小规模交流中简要介绍过。所以Kevin Murphy把这些内容写进了他的书《Machine Learning: a Probabilitic Perspecitve》里。在吴军博士的《数学之美》里也提到了Rephil。 Rephil的模型是一个全新的模型，更像一个神经元网络。这个网络的学习过程从Web scale的文本数据中归纳海量的语义——比如“apple”这个词有多个意思：一个公司的名字、一种水果、以及其他。当一个网页里包含”apple”, “stock”, “ipad”等词汇的时候，Rephil可以告诉我们这个网页是关于apple这个公司的，而不是水果。 这个功能按说pLSA和LDA也都能实现。为什么需要一个全新的模型呢？ 从2007年至今，国内外很多团队都尝试过并行化pLSA和LDA。心灵手巧的工程师们，成功的开发出能学习数万甚至上十万语义（latent topics）的训练系统。但是不管大家用什么训练数据，都会发现，得到的大部分语义（相关的词的聚类）都是非常类似，或者说“重复”的。如果做一个“去重”处理，几万甚至十万的语义，就只剩下几百几千了。 这是怎么回事？","text":"Google Rephil是Google AdSense背后广告相关性计算的头号秘密武器。但是这个系统没有发表过论文。只是其作者（博士Uri Lerner和工程师Mike Yar）在2002年在湾区举办的几次小规模交流中简要介绍过。所以Kevin Murphy把这些内容写进了他的书《Machine Learning: a Probabilitic Perspecitve》里。在吴军博士的《数学之美》里也提到了Rephil。 Rephil的模型是一个全新的模型，更像一个神经元网络。这个网络的学习过程从Web scale的文本数据中归纳海量的语义——比如“apple”这个词有多个意思：一个公司的名字、一种水果、以及其他。当一个网页里包含”apple”, “stock”, “ipad”等词汇的时候，Rephil可以告诉我们这个网页是关于apple这个公司的，而不是水果。 这个功能按说pLSA和LDA也都能实现。为什么需要一个全新的模型呢？ 从2007年至今，国内外很多团队都尝试过并行化pLSA和LDA。心灵手巧的工程师们，成功的开发出能学习数万甚至上十万语义（latent topics）的训练系统。但是不管大家用什么训练数据，都会发现，得到的大部分语义（相关的词的聚类）都是非常类似，或者说“重复”的。如果做一个“去重”处理，几万甚至十万的语义，就只剩下几百几千了。 这是怎么回事？ 如果大家尝试着把训练语料中的低频词去掉，会发现训练得到的语义和用全量数据训练得到的差不多。换句话说，pLSA和LDA模型的训练算法没有在意低频数据。 为什么会这样呢？因为pLSA和LDA这类概率模型的主要构造单元都是指数族分布（exponential family）。比如pLSA假设一个文档中的语义的分布是multinomial的，每个语义中的词的分布也是multinomial的。因为multinomial是一种典型的指数族分布，这样整个模型描述的海量数据的分布，不管哪个维度上的marginalization，都是指数族分布。在LDA中也类似——因为LDA假设各个文档中的语义分布的multinomial distributions的参数是符合Dirichlet分布的，并且各个语义中的词的分布的multinomial distributions的参数也是符合Dirichlet分布的，这样整个模型是假设数据是指数族分布的。 可是Internet上的实际数据基本都不是指数族分布的——而是长尾分布的。至于为什么是这样？可以参见2006年纽约时报排名畅销书The Long Tail: Why the Future of Business is Selling Less of More。或者看看其作者Chris Anderson的博客The Long Tail。 长尾分布的形状大致如下图所示： 其中x轴表示数据的类型，y轴是各种类型的频率，少数类型的频率很高（称为大头，图中红色部分），大部分很低，但是大于0（称为长尾，图中黄色部分）。一个典型的例子是文章中词的分布，有个具体的名字Zipf’s law，就是典型的长尾分布。而指数族分布基本就只有大头部分——换句话说，如果我们假设长尾数据是指数族分布的，我们实际上就把尾巴给割掉了。 割掉数据的尾巴——这就是pLSA和LDA这样的模型做的——那条长尾巴覆盖的多种多样的数据类型，就是Internet上的人生百态。理解这样的百态是很重要的。比如百度和Google为什么能如此赚钱？因为互联网广告收益。传统广告行业，只有有钱的大企业才有财力联系广告代理公司，一帮西装革履的高富帅聚在一起讨论，竞争电视或者纸媒体上的广告机会。互联网广告里，任何人都可以登录到一个网站上去投放广告，即使每日广告预算只有几十块人民币。这样一来，刘备这样织席贩屡的小业主，也能推销自己做的席子和鞋子。而搜索引擎用户的兴趣也是百花齐放的——从人人爱戴的陈老师苍老师到各种小众需求包括“红酒木瓜汤”（一种丰胸秘方，应该出丰胸广告）或者“苹果大尺度”（在搜索范冰冰主演的《苹果》电影呢）。把各种需求和各种广告通过智能技术匹配起来，就酝酿了互联网广告的革命性力量。这其中，理解各种小众需求、长尾意图就非常重要了。 实际上，Rephil就是这样一个能理解百态的模型。因为它把Google AdSense的盈利能力大幅提升，最终达到Google收入的一半。两位作者荣获Google的多次大奖，包括Founders’ Award。 而切掉长尾是一个很糟糕的做法。大家还记得小说《1984》里有这样一个情节吗？老大哥要求发布“新话”——一种新的语言，删掉自然英语中大部分词汇，只留下那些主流的词汇。看看小说里的人们生活的世界，让人浑身发毛，咱们就能体会“割尾巴”的恶果了。没有看过《1984》的朋友可以想象一下水木首页上只有“全站十大”，连“分类十大”都删掉之后的样子。 既然如此，为什么这类模型还要假设数据是指数族分布的呢？——实在是不得已。指数族分布是一种数值计算上非常方便的数学元素。拿LDA来说，它利用了Dirichlet和multinomial两种分布的共轭性，使得其计算过程中，模型的参数都被积分给积掉了（integrated out）。这是AD-LDA这样的ad hoc并行算法——在其他模型上都不好使的做法——在LDA上好用的原因之一。换句话说，这是为了计算方便，掩耳盗铃地假设数据是指数族分布的。 实际上，这种掩耳盗铃在机器学习领域很普遍。比如有个兄弟听了上面的故事后说：“那我们就别用概率模型做语义分析了，咱们还用矩阵分解吧？SVD分解怎么样？” 很不好意思的，当我们把SVD分解用在语义分析（称为LSA，latent semantic analysis）上的时候，我们还是引入了指数族分布假设——Gaussian assumption或者叫normality assumption。这怎么可能呢？SVD不就是个矩阵分解方法吗？确实传统SVD没有对数据分布的假设，但是当我们用EM之类的算法解决存在missing data的问题——比如LSA，还有推荐系统里的协同过滤（collaborative filtering）——这时不仅引入了Gaussian assumption，而且引入了linearity assumption。当我们用其他很多矩阵分解方法做，都存在同样的问题。 掩耳盗铃的做法怎么能存在得如此自然呢？这是因为指数族分布假设（尤其是Gaussian assumption）有过很多成功的应用，包括通信、数据压缩、制导系统等。这些应用里，我们关注的就是数据中的低频部分；而高频部分（或者说距离mean比较远的数据）即使丢掉了，电话里的声音也能听懂，压缩还原的图像也看得明白，导弹也还是能沿着“最可能”靠谱的路线飞行。我们当然会假设数据是指数族分布的，这样不仅省计算开销，而且自然的忽略高频数据，我们还鄙夷地称之为outlier或者noise。 可是在互联网的世界里，正是这些五花八门的outliers和noise，蕴含了世间百态，让数据不可压缩，从而产生了“大数据”这么个概念。处理好大数据的公司，赚得盆满钵满，塑造了一个个传奇。这里有一个听起来比较极端的说法大数据里无噪声——很多一开始频率很低，相当长尾，会被词过滤系统认为是拼写错误的queries，都能后来居上成为主流。比如“神马”，“酱紫”。 Rephil系统实现的模型是一个神经元网络模型（neural network）。它的设计的主要考虑，就是要能尽量好的描述长尾分布的文本数据和其中蕴含的语义。Rephil模型的具体技术细节因为没有在论文中发表过，所以不便在这里透露。但是Rephil模型描述长尾数据的能力，是下文将要介绍的Peacock系统的原动力，虽然两者在模型上完全不同。 Rephil系统是基于Google MapReduce构建的。如上节所述，MapReduce在用来实现迭代算法的时候，效率是比较低的。这也是Peacock要设计全新框架的原动力——使其比MapReduce高效，但同时像MapReduce一样支持fault recovery。","categories":[{"name":"note","slug":"note","permalink":"http://frankchen.xyz/categories/note/"}],"tags":[{"name":"LDA","slug":"LDA","permalink":"http://frankchen.xyz/tags/LDA/"}]},{"title":"note for CS224d:1","slug":"2016-06-04-note-for-cs224d-1","date":"2016-06-04T06:51:38.000Z","updated":"2017-01-13T09:49:32.000Z","comments":true,"path":"2016/06/04/2016-06-04-note-for-cs224d-1/","link":"","permalink":"http://frankchen.xyz/2016/06/04/2016-06-04-note-for-cs224d-1/","excerpt":"最近学习了斯坦福的CS224d课程，该课 程的主要内容是神经网络在自然语言处理领域的应用。 这里记录相关的学习笔记，大概分 成以下几个部分：word2vec，窗口分类，神经网络，循环神经网络，递归神经网络，卷积 神经网络。 ##为什么需要深度学习 传统的机器学习方法都是人为的设计特征或者表示，深度学习的目的是希望能够通过神经网络让机器自动学习到有效的特征表示，这里所说的深度学习更偏向于关注各种类型的神经网络。探索机器学习的原因主要有以下几方面：","text":"最近学习了斯坦福的CS224d课程，该课 程的主要内容是神经网络在自然语言处理领域的应用。 这里记录相关的学习笔记，大概分 成以下几个部分：word2vec，窗口分类，神经网络，循环神经网络，递归神经网络，卷积 神经网络。 ##为什么需要深度学习 传统的机器学习方法都是人为的设计特征或者表示，深度学习的目的是希望能够通过神经网络让机器自动学习到有效的特征表示，这里所说的深度学习更偏向于关注各种类型的神经网络。探索机器学习的原因主要有以下几方面： 人工设计的特征常常定义过多，不完整并且需要花费大量的时间去设计和验证 自动学习的特征容易自适应，并且可以很快的学习 深度学习提供了一个弹性的，通用的学习框架用来表征自然的，视觉的和语言的信息。 深度学习可以用来学习非监督的（来自于生文本）和有监督的（带有特别标记的文本，例如正向和负向标记） 在2006年深度学习技术开始在一些任务中表现出众，为什么现在才热起来？ 深度学习技术受益于越来越多的数据 更快的机器与更多核CPU/GPU对深度学习的普及起了很大的促进作用 新的模型，算法和idea层出不穷 通过深度学习技术提升效果首先发生在语音识别和机器视觉领域，然后开始过渡到NLP领域 深度学习在所有的NLP层次（音素、形态学、句法、语义）都得到了应用，而所有的NLP层次的表示都涉及到向量（Vectors），下面主要讲如何用向量来表示词。 ##词向量 ###语义词典 我们要如何表示一个词的意思呢？常识上，在词典中我们通过更加简单常用的词来构成例句来解释一个词的意思。那在计算机中，我们通常使用Wordnet来表示词义： 但语义词典存在如下问题： 语义词典资源很棒但是可能在一些细微之处有缺失，例如这些同义词准确吗：adept, expert, good, practiced, proficient,skillful? 会错过一些新词，几乎不可能做到及时更新: wicked, badass, nifty, crack, ace, wizard, genius, ninjia 有一定的主观倾向 需要大量的人力物力 很难用来计算两个词语的相似度 ###One-hot Representation首先我们把词表中的词从$$0到|V|−1$$进行编号，ont-hot向量把每个词表示成一个$$|V|$$维 （词表大小为$$|V|$$）的向量，该向量只有特定词的编号对应的位置为1，其他位置全部为0 。这种方法把每个词表示成独立的个体，无法通过one-hot向量直接表示出词之间的关系。解决方法是通过一个词的上下文来表示一个词。 例如,比如","categories":[{"name":"note","slug":"note","permalink":"http://frankchen.xyz/categories/note/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://frankchen.xyz/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://frankchen.xyz/tags/Deep-Learning/"}]},{"title":"char-rnn-chinese","slug":"2016-05-26-char-rnn-chinese","date":"2016-05-26T07:37:32.000Z","updated":"2016-11-11T13:50:33.000Z","comments":true,"path":"2016/05/26/2016-05-26-char-rnn-chinese/","link":"","permalink":"http://frankchen.xyz/2016/05/26/2016-05-26-char-rnn-chinese/","excerpt":"本文主要根据Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch的内容来进行试验。 #准备工作 根据原文“This code is written in Lua and requires Torch. Additionally, you need to install the nngraph and optim packages using LuaRocks”，安装以下依赖。","text":"本文主要根据Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch的内容来进行试验。 #准备工作 根据原文“This code is written in Lua and requires Torch. Additionally, you need to install the nngraph and optim packages using LuaRocks”，安装以下依赖。 ##安装Torch 使用如下的命令安装Torch 1234cd ~/curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bashgit clone https://github.com/torch/distro.git ~/torch --recursivecd ~/torch; ./install.sh 再用如下命令更新：source ~/.bashrc 出现如下画面，代表Torch已经装好！ ##安装luasudo apt-get install lua5.2 ##安装其他依赖 使用LuaRocks来安装nngraph和optim： 12luarocks install nngraphluarocks install optim 首先安装LuaRocks安装时在config部分遇到问题，参考安装Luarocks和linux下lua开发环境安装这时可能遇到安装了lua但是却提示无法找到lua.h可能是因为还需要安装liblua5.1-0-dev的缘故。使用apt-get安装luarocks后在安装nngraph时报错，需要解决 ==其实使用torch内自带的luarocks安装即可==：1sudo ~/torch/install/bin/luarocks install 因为本机只有英特尔核显，所以只打算用CPU计算，就不安装CUDA了。 #开始实验 karpathy的example实验-cpu版本###training过程 使用th train.lua --help查看一下各参数的作用：123456789101112131415161718192021222324252627282930Options -data_dir data directory. Should contain the file input.txt with input data [data/tinyshakespeare] 训练语料 -min_freq min frequent of character [0] -rnn_size size of LSTM internal state [128] -num_layers number of layers in the LSTM [2] -model for now only lstm is supported. keep fixed [lstm] -learning_rate learning rate [0.002] -learning_rate_decay learning rate decay [0.97] -learning_rate_decay_after in number of epochs, when to start decaying the learning rate [10] -decay_rate decay rate for rmsprop [0.95] -dropout dropout for regularization, used after each RNN hidden layer. 0 = no dropout [0] -seq_length number of timesteps to unroll for [50] -batch_size number of sequences to train on in parallel [50] -max_epochs number of full passes through the training data [50] -grad_clip clip gradients at this value [5] -train_frac fraction of data that goes into train set [0.95] -val_frac fraction of data that goes into validation set [0.05] -init_from initialize network parameters from checkpoint at this path [] -seed torch manual random number generator seed [123] -print_every how many steps/minibatches between printing out the loss [1] -eval_val_every every how many iterations should we evaluate on validation data? [2000] -checkpoint_dir output directory where checkpoints get written [cv] -savefile filename to autosave the checkpont to. Will be inside checkpoint_dir/ [lstm] -accurate_gpu_timing set this flag to 1 to get precise timings when using GPU. Might make code bit slower but reports accurate timings. [0] -gpuid which gpu to use. -1 = use CPU [0] -opencl use OpenCL (instead of CUDA) [0] -use_ss whether use scheduled sampling during training [1] -start_ss start amount of truth data to be given to the model when using ss [1] -decay_ss ss amount decay rate of each epoch [0.005] -min_ss minimum amount of truth data to be given to the model when using ss [0.9] 按照Github上的说明进行实验，使用原文件夹里的语料，1th train.lua -data_dir data/tinyshakespeare/shakespeare_input.txt -gpuid -1 报错：1234567891011121314 th train.lua -data_dir data/tinyshakespeare/shakespeare_input.txt -gpuid -1vocab.t7 and data.t7 do not exist. Running preprocessing...one-time setup: preprocessing input text file data/tinyshakespeare/shakespeare_input.txt/input.txt...loading text file.../home/frank/torch/install/bin/luajit: cannot open &lt;data/tinyshakespeare/shakespeare_input.txt/input.txt&gt; in mode r at /home/frank/torch/pkg/torch/lib/TH/THDiskFile.c:649stack traceback: [C]: at 0x7f9c42473540 [C]: in function &apos;DiskFile&apos; ./util/CharSplitLMMinibatchLoader.lua:201: in function &apos;text_to_tensor&apos; ./util/CharSplitLMMinibatchLoader.lua:38: in function &apos;create&apos; train.lua:118: in main chunk [C]: in function &apos;dofile&apos; ...rank/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk [C]: at 0x00405d70 这里出现了问题，因为本文是中国作者按照原karpathy的char-rnn 改写的，我认为或许使用karpathy作者的原版本教程可能会更加方便一些。于是使用As a sanity check，运行：1th train.lua -gpuid -1 这指的是使用CPU并不指定任何参数来训练example。 15:42开始训练1234567891011121314151617 th train.lua -gpuid -1loading data files...cutting off end of data so that the batches/sequences divide evenlyreshaping tensor...data load done. Number of data batches in train: 423, val: 23, test: 0vocab size: 65creating an LSTM with 2 layerssetting forget gate biases to 1 in LSTM layer 1setting forget gate biases to 1 in LSTM layer 2number of parameters in the model: 240321cloning rnncloning criterion1/21150 (epoch 0.002), train_loss = 4.19803724, grad/param norm = 5.1721e-01, time/batch = 2.3129s2/21150 (epoch 0.005), train_loss = 3.93712133, grad/param norm = 1.4679e+00, time/batch = 2.3114s3/21150 (epoch 0.007), train_loss = 3.43764434, grad/param norm = 9.5800e-01, time/batch = 2.3022s4/21150 (epoch 0.009), train_loss = 3.41313742, grad/param norm = 7.5143e-01, time/batch = 2.5311s5/21150 (epoch 0.012), train_loss = 3.33707270, grad/param norm = 6.9269e-01, time/batch = 2.4913s 到第300次迭代后，time/batch稳定在2.3s左右，也就是说，使用GPU训练这个1Mb的example，需要约14小时！次日08:24训练完毕12345621148/21150 (epoch 49.995), train_loss = 1.53254314, grad/param norm = 5.9157e-02, time/batch = 2.8658s21149/21150 (epoch 49.998), train_loss = 1.50882624, grad/param norm = 5.7123e-02, time/batch = 2.8737sdecayed learning rate by a factor 0.97 to 0.00057368183755432evaluating loss over split index 2saving checkpoint to cv/lm_lstm_epoch50.00_1.3568.t721150/21150 (epoch 50.000), train_loss = 1.46142484, grad/param norm = 5.9032e-02, time/batch = 2.8834s ###Sample过程查看help：1234567891011121314151617181920th sample.lua --helpUsage: /home/frank/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th [options] &lt;model&gt;Sample from a character-level language modelOptions &lt;model&gt; model checkpoint to use for sampling -seed random number generator&apos;s seed [123] -sample 0 to use max at each timestep, 1 to sample at each timestep [1] -primetext used as a prompt to &quot;seed&quot; the state of the LSTM using a given sequence, before we sample. [] -length max number of characters to sample [2000] 采样字符大小，最大2000 -temperature temperature of sampling [1] -gpuid which gpu to use. -1 = use CPU [0] 和训练时设置应该保持一致 -verbose set to 0 to ONLY print the sampled text, no diagnostics [1] -stop stop sampling when detected [] 先试运行一下th sample.lua cv/lm_lstm_epoch50.00_1.3568.t7 -gpuid -1生成了如下语句：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374giff,Some sweet amends, aasher, had therein, not he had wot onman&apos;s friends, for her own blow: for my men&apos;sackingly knight, it cannot hear upon yield.GLOUCESTER:How now again?iMarch, that&apos;s my arm determitness,The temper of the vrowal; ere from the groveTut wilh &apos;goom&apos;d Carulea.&apos;Mwailich, tne shy had lost in When Ied the wayThe bower to a late his body, grim on you:His opicious shames a booy, infairs,&apos;From her, I tell you, ay, as we mean himDear &apos;tis a giving o&apos; thur back&apos;s empass&apos;d,That nrost, I&apos;ll havk him cume thee for &apos;t.LAONTES:&apos;Twi&apos;l love thy sronowing.VALYRAN:Beord mocdoch him for thy follightsnnhours,But thank yours lodkes, my good journeding,His jealousisposour thee are both abomishThat noom that&apos;s easembelland. Camest, sir, morekia; one, in this highty be the unSince of a gournor on thy friendshall swowSome painon; and I, and lord, the at the kinsWise rit hable surliments. Shd, believh gone.voisted tleace:Tock him what all you di turn up to celentTo my sistinge. Frranch, good night, your child, so fatus;Aor he shall be my trueking:Come on my quarrel of the way:Methinks the letters; for this ctome-steersTad mousd my smodered pouncy tohaw up another sense tlays underttryTut bonscuration fair all purpose,then be vesegt me: do not, yet rustle cannot,But for thy mustered a dust, let meTncerfact me tresmer of his father:therefore by hanging,ANdAys, my lord: you do here in coumisant.LORD:How lond the brown!So majp me; bonch, smmily lovely blotters,When Ie my hoeaty threat and virlume these things,Make fasting garlands dfar the sack&apos;d my servictughtNot knows the crowns: one air, Aumerle,Ere wear not so nour Bidagle? What Aphark is furyTld meens them, faireyou consides to no moreIhis wantond frown and pollitueser&apos;d city.Can should put him more recounders to impudesnt poison onthet hour from hunt to Rame, supp to bereFlowerd and his friend is une dewn ao pirt,You know by join&apos;d guilty, whathout we e.ANdAys, my lord: you do here in coumisant.LORD:How lond the brown!So majp me; bonch, smmily lovely blotters,When Ie my hoeaty threat and virlume these things,Make fasting garlands dfar the sack&apos;d my servictughtNot knows the crowns: one air, Aumerle,Ere wear not so nour Bidagle? What Aphark is furyTld meens them, faire karpathy的example实验-gpu版本使用和cpu版本相同的指令，只是th train.lua -gpuid 0得到的1th sample.lua cv/lm_lstm_epoch50.00_1.3622.t7 -gpuid 0 sample为： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071--------------------------geI prithee; nor in the day of all report?Nou shall be you that lances stoson quarrel:We mits most stone, &apos;aim, upeed his forticentAhen I was, yet for her, but of lovels ClarencelAnd past her got and father there, one weepIn mine wroth nightlys, smileed. Cantleye An York,A druls marriage, that though Service hated meTheir duen of iflock, we are here in berievedand more than tanise them with sutereptHe rt in some pickle.SePSERTER:Or I have safe all thou depustere andthe before him.FRIAR LAURENCE:What art my government, cosbude, and hence; if Onfrawn? provest tor my duty?CATESBY:KARIANA:My love noe is are with t herman and his,It should she well deeauring our consent:They hang me bointed on the king, let so twoNature by my sighsing pleasing &apos;jabeThat leaven and grue, at her Richard&apos;s blood.More ends it likipenortnive, nor each of himic.SLY:How dachors, Richmend, henr dack but like it?Be long, anon since your kingdom and us,And we that aver elaunter, my eee to toucurt tomends.It this her great fawn&apos;s birds,, sir! you&apos;er head.PAULINA:Upternalt cost of his hands for their tricks my father,Who ts it most seunt to live te and she were all.-killO to thy son os shall not on your childrs,one next, for she did formly consixentAbove, my life, and wew me worthy deeming tvenge!My mustere be exploience, aot come n leave where ahe knees in.dear, thus wild up tilt on the county, hath be one.See this sword of thee with the deepito man,For sunier ene first sears. Where&apos;s turn on to be.Unctious blunlest terrocate dovesTrades Marcius aines of hlendsMy&apos;s learth an old--ay.LEONTES:Marcius?PRONVO:You would no gue.VOLUMNIA:Oovine s fetch to tight, thou must but loods.HASTINGS:And was with her, nor yonder to be sworn,What are you allady that I should have purpose.What men revenge is a well patientAnd who seth sxoleng to knowled ed to myself;And married me in the joy:So reve I made to find me speak,how he been tou.PETCA: ##《水浒传》语料实验 ###cpu版本 把下载好的《水浒传》改名为input.txt使用1th train.lua -data_dir data/mydata/ -gpuid -1 训练，可以看到很明显，速度很慢12345678910111213141516171819202122232425th train.lua -data_dir data/mydata/ -gpuid -1vocab.t7 and data.t7 do not exist. Running preprocessing...one-time setup: preprocessing input text file data/mydata/input.txt...loading text file...creating vocabulary mapping...putting data into tensor, it takes a lot of time...saving data/mydata/vocab.t7saving data/mydata/data.t7loading data files...cutting off end of data so that the batches/sequences divide evenlyreshaping tensor...data load done. Number of data batches in train: 345, val: 19, test: 0vocab size: 4129creating an LSTM with 2 layerssetting forget gate biases to 1 in LSTM layer 1setting forget gate biases to 1 in LSTM layer 2number of parameters in the model: 2845345cloning rnncloning criterion1/17250 (epoch 0.003), train_loss = 8.32795887, grad/param norm = 9.6310e-02, time/batch = 28.8711s2/17250 (epoch 0.006), train_loss = 8.06433859, grad/param norm = 4.2826e-01, time/batch = 25.2184s3/17250 (epoch 0.009), train_loss = 7.28941094, grad/param norm = 3.9537e-01, time/batch = 25.2195s4/17250 (epoch 0.012), train_loss = 6.85331761, grad/param norm = 4.0576e-01, time/batch = 25.2011s5/17250 (epoch 0.014), train_loss = 6.69327439, grad/param norm = 3.8309e-01, time/batch = 24.9642s6/17250 (epoch 0.017), train_loss = 6.50776019, grad/param norm = 3.1042e-01, time/batch = 24.9203s 预计需要120小时训练时间！但是，这都是未经过处理的语料，后续使用处理过的余料（如去掉低频词语等）再来训练应该会更快。因为时间太长，所以这个实验被放弃了。 ###GPU版本的未处理语料实验首先对未处理语料做训练：1th train.lua -data_dir data/mydata/ -gpuid 0 begin at 10:20可以看到time/batch稳定在0.08s左右，也就是半小时就可以训练完成！GPU比cpu在科学计算上面实在是太强大了。训练完毕，使用：th sample.lua cv/lm_lstm_epoch50.00_3.9309.t7 者，却得出来的物细物，没面少管生渔人。后来的知府时是乱色早晚，拾了几个。李逵惊得忙忙轻梳药穿在大牢里，摆在延安家处，推慰九节的。当下径到居中饮酒，牌门头，戴宗又焦躁。只见屏风背后转出一个小风大来，暗暗听得道：“反细放俺!兄弟拿着，趁这为害天明地清，我休要推道别事的都要做伴当拆投到会耳，便有进漏？”时迁舞起树下探人，的了夹搭，都拽了拽开，胸皮虽是好了六分惊得，是他麻。吐女棒放火了，走不向前，及宋江那道个留守他做个辩察的，先自去州里请明地烧了钱用，但有过京回家，听得状子好!这高老袋内却是出张招安，又都是他的欺负民，如何是计信?必须要和郎相会。且。”趁早起楼去了。两个连夜时候，仓治时，年二十八执迷者多要都要去行叶，早是亲家洒家，径挨到府前来。灯烛纸敌官，方才脱漏。亦被乱窝中有人等好人知蔡京道：“那个人也是他甚么?因此教大军打劫俺那干干金珠的。”母子那妇人来到大王尚安着，相交酒追惹三清酒搦战。不过半夜之事，早饭相烦，心里出对知府说：“官家初时在时，欲要市上时，兀自和我觅家劫犯了他，如何不赶我里来?我大小心定得，不分便了。”叉军转回，已做些小头，要打抵敌他!且把身，带了七个人时，都抢出家，但见： 壮中醪浑纷领，腰细轻露。阔尺三层挺刀，鱼厚夸敌庆孙。高人有八句诗单道身心强似莽?；小付敲柴的，真多呼圣殿之主贞锋欣乐词。头邬闻丹腊良夫，耍达缘矮岁龙；四虎间，寒暮难以偷黄；牢记仁诸作像，显宝一根红佛。微。善得山迎能指挥，直救清天马星。 话说上阵法， 师皇帝展得有?宫殿，雨翠云也上田地里。幸观非乃帝重了，宋江心如有誓，同宋受迷。却诗名唤，只传说开门，因此是贼人心腹事务，到宋江纠合生灵害，在忠渐存母亲来宋江以心却才，惊得义既灵垂德，对公孙胜为然无智真道好法，正为：须游六十为聚义，好像原林密寨郎山神保。 当日宋庄客帐前，与晁保、公二位头领，众头领发起作法。 石裂更兼地分都拨人汉，且不杀得蒙恩干人结义，下山只是锦袋百把，们有父亲孔宾，同商量。宋江又道：“自是好生，莫非也只是是有哥哥下了。”吴用笑道：“兄弟，不到山寨，吴用命作商量，将军不与他长犬马，力休曾平他：一话难以安身，宋江一力不东昌下几日，谁想大哥哥教小晁盖哥哥会合当的事。我们人投随天军来，又有伤损；若不连环甲关，着李横其不似火体，车藏御上尽挂玉水；军卒许多，无无不难之际?他但**，可等兵，可以斩遣。”众军健都管入庄，要把鲜血迸成，赶起来，背后解水边，唤车军跨城疮只等，原来正是之福，后往，来不见三个使汉。因弟清风船救应。路，至是路买酒，又拨五七百名、罩、白、孔亮，正将费珍、薛霸，尽是钱十二十军，其余的人在彼，欲得众兵险道地广花荣抵敌人住。这一队节度使士都军，被两个军猛，呐声喊，都抢过城里，并无腰迎敌，被贼兵赶上，时，却被花荣战箭射来。童太人、杨志正是南安江韩杨龙、穆弘、李逵、索超正定敌。孙军纵马。琼清马挺着枪，入来，尽被史进和贼人杀死贼兵，擒做霹雳。邬梨因成让风，连鼓上马，将股斧，却飞入阵，大小张清见了见乱军阵前卒法败坑回阵，宋江旭前只是： 主人问姓，五应风万。侯海道正：“恶平可逃奔：。时们村野阴血，呼往天兵消波。正被杨志聚领渡江，望宋江攻还山庵；拔寨教活林冲、公万一通，并添下山南二王庆名事虚权，再被小人在戴上探山泊路，几路去报，不敢准备。不知这个人说起是百庄小黑凌州，已曾见了，对别无缘。”吴用道道：“便队军马解到此时，必是殡隘为百谷岭。原师悬流水军头二头领，结识江湖上好汉姓石，名给鬼，便乃五家庄二多情。我去这里地路，望会便行。”廊进雷车把人来不止，李应拈着诏书，自此付话。 且说山客渡过了三只路，教穆弘扮做伴当，扮做阎婆者，带拢是臭镇一个没赌什门，分顺了同行，自去寻闭了的。原自去被人运烧将下去了。宋江等远远，一路进兵。十来县不在大路途来，又怕了到得闲意的张社长，听得监押一声，货钱便是。任原陈达在中，不知处打那华州，特使他来掳去太安军肆，只待下山。戴宗告随张小人，蔡九知府不得，连夜回话，同张招讨干办、众部吉。于路，忽报探知样悔，景珍全过，领回商议，“军师赵枢密喜绯金带，身上悬面草板，护道国师，服，神色不通。是奇诈将丘留的人，准备起船走径来借粮，业不同何遇一深困马灵夫，便因密的月色渐砂来完，斋。小温皇威，被宣刚引军来，武松彼朵并顾大嫂，赴了逃去，自逃命探了。被那几人娅?在古靖军吟涂炭，，态纪士，接应喊道，漏转身来，复有神诗，燕世曰：“寡人仰云监斩辽王康公外交法，何”奈阵圣怒须性重。铁挨填丹靴，万边狄行鉴。见。田户观看草畔，红日影豪困催急绩。宋玉游战，听听了大喜。话说宿太师诏奏道：“宿元帅差有敕入请罗真人，密封官军等八员高名，封当同达宋先锋。”日收选润之主，奏为圣旨，特着州殿府探知。太尉宿太尉回到内，启转马，众军方可亦成开大事，放起出来，更兼小一个唤做 ##使用增强RNN网络训练 如下，使用512个隐藏节点的3层RNN网络训练模型 th train.lua -data_dir data/shuihuzhuan/ -gpuid 0 -rnn_size 512 -num_layers 3th sample.lua cv/lm_lstm_epoch50.00_5.4830.t7 -sample 0 -temperature 0.8 -verbose 0 -length 500 弟兄两个，也得个信名之人。”那个也是个道理。童家四更，被张顺斩的粉碎，以下人人家拿去了。一面叫酒保打两桶酒来。小二哥叫道：“师父，你不是我来也！”那小牢子道：“我也不曾你，你便叫我上来。” 石秀道：“你且说他三宫百里吃酒了来，你便抢入去，你便先来看，却被这畜生说不得了。”那妇人道：“你真人要打这里话?你却不认你，你便叫我儿来寻。”李逵道：“你敢作吃的，便揪我做脚！”赤条条地寻谁，只得骂道：“爹娘，你且休了，我自不信，砍我头便打那老娘。”那妇人道：“也好。”便把袖儿丢下去了。那妇人也把刀带在一边，却似小窗????胡乱道：“好拳脚！”急叫开了店看。”王庆听了，连声叫道：“阿也!你不要吃！”把手一指，提倒上岸来，把朴刀倚在被里。就把篙子门内，倒做五六斤了，将把木鱼来摆下桶桶。少时，张顺吃了一回。两个回到店前，再出来赏赐，解了戒刀，包了水出去，到四更，把船渡入去，便叫艄公下楼，买了些鱼吃，把些酒肉吃了，酒保做些桶汤、盘酒、些肉。下来穿瓶与酒。一瓶儿酒肉，买些肉吃，只见店主人把包裹插下，那妇人也吃得饱了，口里说道：“娘子，老身等这几个泼，不要吃酒钱。”店小二道：“好酒好肉要打，我吃便饱 很明显，此时sample的样本语句更加通顺，错误很少，从品味小说的角度来讲，增强了的RNN训练得到的模型更加完美了。 可以看到，增加了节点数和隐藏层的RNN具有更强的学习能力。 对比训练过程模型表现力与此同时我们可以对比一下，训练开始阶段与训练结束时的模型表现力的差异： th sample.lua cv/06-01-shz_sp/lm_lstm_epoch5.80_4.0451.t7 -sample 0 -temperature 0.8 -verbose 0 -length 500 训练刚刚进行到5.8（为50时完成）得到的是： 只见一个人从来，一个人，都来做一个。那人道：“你这厮们，我自去寻你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要瞒我，你便不要你。”那人道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那婆娘笑道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不要吃，我自去寻你。”那妇人道：“你不 此时语料具有较多的重复，模型还没有良好的收敛。 继续观察，当进行到四分之一左右时： 那汉子听了，便问道：“你这厮不是歹人，如何不来?我们不曾有这般的，如何不来?你的那里去了？”那汉道：“我们不曾说谎。”那汉道：“我不曾说谎。”那汉道：“我不曾说谎。”那汉道：“我不曾说谎。”那汉道：“我不曾说谎。”那汉道：“既是恁地，我们自去。”那汉道：“既是恁地，我们自去。”那汉道：“既然恁地，我们自去买碗酒吃。”那汉道：“既是恁地，我们自去。”那汉道：“既然恁地，我们自去买碗酒吃。”那汉道：“既是恁地，我们自去。”那挑酒的汉子道：“我们自有计较，我们自有些钱，与你些银两，却去商议。”董超道：“我们不曾有这般的事，如何不来?你们自去取路，我和你如何不去?若还了他时，便是要去的。”李逵道：“你们不曾说的，你便是个老儿，如何不来?我们自去取他，你便不曾与他厮见。”那妇人道：“你的，你不省得。”那妇人道：“你的，不要胡说!我们自有钱帛，便要去便了。”那妇人道：“既是恁地，我们自去买些银子与我。”那妇人道：“我们自有钱与你，你便不曾去了。”那妇人道：“既是恁地，我们便去。”那妇人道：“既是恁地，我们便去。”那妇人道：“你的，不要胡说!我如何肯去这里？”那妇人道：“便是这般使棒，不曾得得他。 再看看训练继续进行到一半左右时候的表现力： 张都监为副将急体己人，不敢不依，只得随行众军，掌行送行。只留下降，尽皆欢喜，以此忠义。在府行军中，有使枪棒卖药的，将王庆领军到来，并不必说。当下宋江传令，教中军计策，与卢俊义等商议：“今日折了两阵，俺们自去了。”宋江道：“军师之言甚善！”当下即日便传将令，教军士点营，斩动军马。将及初五更战后，攻打常州，催趱军兵，一齐进发。 寨中，只听得高声叫道：“萧让等救兵！”宋江看那军将，尽数放起，对幽州一个大汉，乃古头上大叫道：“水洼草寇，怎敢轻慢！”只见里面关胜、呼延灼、关胜等探有一人，只见唐斌从人骑马，直到宋江寨前，喝请宋先锋。宋江听了大喜，传令令军士且去寨中坐地，备说宋先锋军马，攻打北京。吴用道：“且教两路军马，攻打北门。”宋江便令吴用、朱武商议：“今日可去，只是国师吴用，坐一件事，我等随顺到此，可用两处夹攻，那厮必然有人来。”宋江道：“军师言之极当。”便唤军士计策。”宋江道：“军师言之极当。”吴用道：“小生直作妙计，即且闻见。”宋江道：“先生之言，是不得这般忧疑。”宋江道：“既然如此，与你四位豪杰，不堪员外大王。”宋江道：“贤弟，你休要疑心，我便去请来。”吴用道：“不须你两个与我箭，只 此时低级的重复没有了，但是可以看到，“宋江道”反复出现，而且说的内容类似，可以得到模型已在进一步完善之中。 这是模型接近训练完成的时候的sample： 张青、孙二娘、顾大嫂、孙二娘，并四个好汉，引着一千余人，吹造大小船只，都投水路。不多时，只见松树背后转出一个小小人来，簇拥着两个人，各提着朴刀，背后有人，叫一声：“捉下！” 那汉子把船只一招，扶着一干人，把那碗饭打伤，打的粉碎，把头头割在一边，口里放火。那人见了是惊得呆了，又不来吃了一惊，扑地只顾走。却待再走，再去脱人避凉。李逵却亦不肯拦他，只得走了。可怜救他两个性命，那里敢?别人。前日被捉死了性命，杀了人，逃走在江州，被害人陷害，方得正中了。今日幸得相见，如何使得?便得是个知县过来的，也喜得及。他便是本人的人，须是高太尉的人，却不知是那里人。”任原道：“这个便是我的儿么？”王婆道：“便是前日那官司亲亲叔孝，为何到此？”那婆子答道：“老身只道不妨，只怕小人自有措置。老身看了，便忘了回去。”老都管道：“这个容易。老身先把银酒去了。”老儿道：“你们自不要吃酒。”那婆子也笑起来道：“这个便是我的老小人家。”那婆子道：“便是老身也不怕你，休要胡主干娘，只怕你疑心。”那妇人道：“不干了。你的女儿，老娘儿只做买些衣服来送与你。”王婆道：“娘子，你要知这几个字？”那婆子道：“有甚么哭处？” 模型进一步完善，接近完成训练。 ##《全唐诗》实验 下载下来的全唐诗.txt是gbk编码的，首先需要转换为utf-8编码：123456789101112# -*- coding: UTF-8 -*-def main(): s = open(\"全唐诗.txt\") r = s.read() r_uni = r.decode('gb2312','ignore') r_en = r_uni.encode('utf-8','ignore') fp=open(\"全唐诗转换.txt\",\"w\") fp.write(r_en)if __name__ == '__main__' : main() 这样就得到了utf-8编码的文件。 开始想直接把这个文件丢给rnn训练，后来想一想，“全唐诗”包括了众多诗人的杰作，也许训练出来的模型sample不到什么特点鲜明的诗句，于是我首先想对诗仙李白的诗做一个实验。把得到的全唐诗文件中李白的诗切分出来作为一个文件。开始训练：th train.lua -data_dir data/tangshi/ -gpuid 0得到模型后sample： th sample.lua cv/lm_lstm_epoch50.00_5.3967.t7 耶翳妃在。流明湖草，岂舞高散纷。小剑宫底寒，石思怀士。归来相烟叹，又余未老此。此在见携节，气帝皇彩川。尝君凌天鸟，羞从臂山中。何旋俱所偃，造愧翻遗耻。悠挥李明云，坐月得相迟。君作成景鸡，暮日清膺发。胡步垂洞松，三年延未莱。贤迢坐橐山，嗤楼谋疏川。诗君九安菲，别去写日流。五时谢及洒，相魄三有玄。 卷177_21 【长道送秀士寄之十南国古松游明，》妓此蛾书此下见逃之始崔至六以吟酒宁】李白 仙阳壶我王，谣荣日茫过。幸生天行寒，半持见九忧。早毂此楼宰歌，鹗税金归名。宾托昼宇闻，高汶堕臣泉。峻悟湍素都，凤生鸟远才。虎傥亦成一，蹭据锁炳垣。回谷闻叹波，摧人翼昆衣。丑德皆复贵，何袂自见宝。黄然奉傍及，戾酒悲溪情。何悟下罗灭，壁令还济然。 卷177_16 【送沙门饯元佛之嵩使归少丞寺辅晔年亭山云】李白 行道一狂日，陆杯欲我园。相寄属相者，不歌流成书。幽景神兰马，今云乃相知。相能拂此门，娟笑无生魂。肠后扫天所，罗瑟心世桥。 卷169_11 【金松二炎师】李白 丽劝发何凤，含东药宛锦。且忍咏尺鳌，梦杯池月。罗鸟思归人，兼我无风歇。太色东草树，壮早游罗息。别来青神极，谒长不陵寒。君忧东海鹤，吹欲得彩好。梦君来太情，秦子忆延薪。闻干凌楼息，松水接廉才。且识辞犹之，众断罗里中。 水羊远重，引飒高新策。目布愁霜亲，，随火赤云道。解毂四上牛，以且汶云年。宁迎清巴寒，种欺清名风。海坐去不意，思丹酩期然。闻钓曾帆景，一弄暗长雪。且亦留我辉，杀讼韵中楼。 卷174_7 【赠崔司州十三青寺黔洞姑圣毛闲华忘兼塔宅石】李白 窜笑敬鹤见梦走去留僧。地筑从尺人，泪树平众烂。窈箸有吉氲，久聪欲洲真。朝春离相兽，飞此何垣发。绿日偶可言，我言经精存。君卒限汉水，绿月相成失。恋虏一登事，但乘涂应星。 卷177_18 【咏夜别（人作帝阳之为昆者）】李白 笑乃度将明，蛾洞蕊山发。花坐新合露镜日边归溪。长面云蓬立，自颜谢鸳分。灭用欲得碑，不云当天舟。一来但不霄，乘我涉路来。怀子广陵远，夕人不元阙。笑天亦望好，驱令适谁手。思此有门上，举与满丝君。醉年归敬山，松藏谢未笑。却思笑古兴，凭凌泪高尺。 钱文1淮作3 卷一两十七三北欢宁三元四慈平难名】李白 水乘黄楼镜君客绿1起，山门天阙已忍春。不喜出庭作，乎晖酒苔。麒惊美。绮门，我见秦心遂天雪，只行天花流。相随夔山肉烟喧，武斗吹齐而公还。以开玉去戟如雨，欲席上醉鹊里洪。世闻燕弦对士去，曳不笑丘瞳中嗟。国昔逢眼青山鸟，扫水长丘双泉空。 卷165_8 【玉崔将言刺判池，一还精孰日君）】李白 军莫荆壮悲，揽迈宋钟客。高当愤酒酒，渴臣达庭边。沙箸佳花诏，映河讵窗中。为交上梧空，空在猛杯闻。惜缅何烦柯，屈笑献神然。万情上者溪，相以陶毫逸。 更闻无袂，小必笑伤穷。 卷169_12 【高陵黄入蜀，寄纪侍御二首】李白 爱来游山子，北照有华宅。岩歧难碑李，独承崔滹鼙。巨君亦不处，常阖之此酒。思君穷莫术，却成愁庄隈。征六此与玄，，羞逐但应君。 卷189_26 【登溪马归歌，归石怀道怀山】】。长腾明花屏爱心春，寄心入良素断。 卷188_7 【送侍御从尔史崔崖赴天】白山年粲赤浑，书别诗游泛风】李白 祖国一官食，二藏系冠鹇。无砚号盆水，浮发王青吟。登水惜不母，殷古启与游。萧觉留见去，待泪复相思。绿风吹中信，江山知洞宫。宾阔未孤里，空可向江峤。西舟青溟云，一浪摇苗彩。流镜沙青辉，夕落得寒洲。妾头海弦弓，而多何皇笑。欢辰虽欢心，此鸣暗清飞。广松春人草，为啸李田。。思君鸾可得，萧论以天风。今家高去路，酌藏无云功。为时壮罢邑，陈不见长名。闻啸不可在，玉服汉寒情。愿昆隐山水，更将谢敬离。 大说词隐开，夜子庐鹤行。他烛不知尽，逸君徒风草。把柯南幽山，超血彩森兵。吊窈赤微鸟，娇若相踪。笑乏惜香门，夜酌闻岩欢。我此一可驰，三刀瓦风樽。东秋清柱晚，意歌送高颜。别留一失在，推时悲紫踪。群远亭我顶，机赠沾酒旋。明镜若相巨，明窗忘语平。桃水凌瑶心，茫讼落清眉。横产无商娥，万流应长生。 卷171_2 【酬纪寿阳送官】李白 白浦欲溪山，去入北酒人。云水薄阶弄，乃啾迹风声。诗命侍飞儿，百结清霞杯。何言思君去，但然谢无歇。 卷176_21 【荆闺崔嵩人宰】李白 常鹉别帝家，绮书逐惟安。西世东山玉，雕是金东辉。绿登紫鹿色，张看弄月萝。思手穷津水，弄是俨归人。相恐新鹉道，渌声赠恨公。 这里出来的效果就很惊人了，我们从小就在课本上学习了诗仙李白的许多佳作，可以说大家对于一个诗人的诗的韵味是怎样是很有体会的，在这些字里行间仔细品味，我们完全可以体会到李太白的豪放与洒脱。","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/categories/tutorial/"}],"tags":[{"name":"DNN","slug":"DNN","permalink":"http://frankchen.xyz/tags/DNN/"}]},{"title":"Python OOP（Object Oriented Programming）","slug":"2016-05-25-pythonoop","date":"2016-05-25T13:35:32.000Z","updated":"2016-11-11T16:37:36.000Z","comments":true,"path":"2016/05/25/2016-05-25-pythonoop/","link":"","permalink":"http://frankchen.xyz/2016/05/25/2016-05-25-pythonoop/","excerpt":"面向对象编程——Object Oriented Programming，简称OOP，是一种程序设计思想。OOP把对象作为程序的基本单元，一个对象包含了数据和操作数据的函数。 面向过程的程序设计把计算机程序视为一系列的命令集合，即一组函数的顺序执行。为了简化程序设计，面向过程把函数继续切分为子函数，即把大块函数通过切割成小块函数来降低系统的复杂度。 而面向对象的程序设计把计算机程序视为一组对象的集合，而每个对象都可以接收其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在各个对象之间传递。 在Python中，所有数据类型都可以视为对象，当然也可以自定义对象。自定义的对象数据类型就是面向对象中的类（Class）的概念。","text":"面向对象编程——Object Oriented Programming，简称OOP，是一种程序设计思想。OOP把对象作为程序的基本单元，一个对象包含了数据和操作数据的函数。 面向过程的程序设计把计算机程序视为一系列的命令集合，即一组函数的顺序执行。为了简化程序设计，面向过程把函数继续切分为子函数，即把大块函数通过切割成小块函数来降低系统的复杂度。 而面向对象的程序设计把计算机程序视为一组对象的集合，而每个对象都可以接收其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在各个对象之间传递。 在Python中，所有数据类型都可以视为对象，当然也可以自定义对象。自定义的对象数据类型就是面向对象中的类（Class）的概念。我们以一个例子来说明面向过程和面向对象在程序流程上的不同之处。 假设我们需要处理学生的成绩表，为了表示一个学生的成绩，面向过程的程序可以用一个dict表示： 123std1 = &#123; 'name': 'Michael', 'score': 98 &#125;std2 = &#123; 'name': 'Bob', 'score': 81 &#125; 而处理学生成绩可以通过函数实现，比如打印学生的成绩：12def print_score(std): print('%s: %s' % (std['name'], std['score'])) 如果采用面向对象的程序设计思想，我们首选思考的不是程序的执行流程，而是Student这种数据类型应该被视为一个对象，这个对象拥有name和score这两个属性（Property）。如果要打印一个学生的成绩，首先必须创建出这个学生对应的对象，然后，给对象发一个print_score消息，让对象自己把自己的数据打印出来。12345678class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print('%s: %s' % (self.name, self.score)) 给对象发消息实际上就是调用对象对应的关联函数，我们称之为对象的方法（Method）。面向对象的程序写出来就像这样：1234bart = Student('Bart Simpson', 59)lisa = Student('Lisa Simpson', 87)bart.print_score()lisa.print_score() 面向对象的设计思想是从自然界中来的，因为在自然界中，类（Class）和实例（Instance）的概念是很自然的。Class是一种抽象概念，比如我们定义的Class——Student，是指学生这个概念，而实例（Instance）则是一个个具体的Student，比如，Bart Simpson和Lisa Simpson是两个具体的Student。 所以，面向对象的设计思想是抽象出Class，根据Class创建Instance。 面向对象的抽象程度又比函数要高，因为一个Class既包含数据，又包含操作数据的方法。 ##类和实例面向对象最重要的概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，比如Student类，而实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但各自的数据可能不同。 仍以Student类为例，在Python中，定义类是通过class关键字：12class Student(object): pass class后面紧接着是类名，即Student，类名通常是大写开头的单词，紧接着是(object)，表示该类是从哪个类继承下来的，继承的概念我们后面再讲，通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。 定义好了Student类，就可以根据Student类创建出Student的实例，创建实例是通过类名+()实现的：12345&gt;&gt;&gt; bart = Student()&gt;&gt;&gt; bart&lt;__main__.Student object at 0x10a67a590&gt;&gt;&gt;&gt; Student&lt;class '__main__.Student'&gt; 可以看到，变量bart指向的就是一个Student的实例，后面的0x10a67a590是内存地址，每个object的地址都不一样，而Student本身则是一个类。 可以自由地给一个实例变量绑定属性，比如，给实例bart绑定一个name属性：123&gt;&gt;&gt; bart.name = 'Bart Simpson'&gt;&gt;&gt; bart.name'Bart Simpson' 由于类可以起到模板的作用，因此，可以在创建实例的时候，把一些我们认为必须绑定的属性强制填写进去。通过定义一个特殊的__init__方法，在创建实例的时候，就把name，score等属性绑上去：12345class Student(object): def __init__(self, name, score): self.name = name self.score = score 注意到__init__方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。 有了__init__方法，在创建实例的时候，就不能传入空的参数了，必须传入与__init__方法匹配的参数，但self不需要传，Python解释器自己会把实例变量传进去：12345&gt;&gt;&gt; bart = Student('Bart Simpson', 59)&gt;&gt;&gt; bart.name'Bart Simpson'&gt;&gt;&gt; bart.score59 和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量self，并且，调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所以，你仍然可以用默认参数、可变参数、关键字参数和命名关键字参数。 ###数据封装面向对象编程的一个重要特点就是数据封装。在上面的Student类中，每个实例就拥有各自的name和score这些数据。我们可以通过函数来访问这些数据，比如打印一个学生的成绩：12345&gt;&gt;&gt; def print_score(std):... print('%s: %s' % (std.name, std.score))...&gt;&gt;&gt; print_score(bart)Bart Simpson: 59 但是，既然Student实例本身就拥有这些数据，要访问这些数据，就没有必要从外面的函数去访问，可以直接在Student类的内部定义访问数据的函数，这样，就把“数据”给封装起来了。这些封装数据的函数是和Student类本身是关联起来的，我们称之为类的方法：12345678class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print('%s: %s' % (self.name, self.score)) 要定义一个方法，除了第一个参数是self外，其他和普通函数一样。要调用一个方法，只需要在实例变量上直接调用，除了self不用传递，其他参数正常传入：12&gt;&gt;&gt; bart.print_score()Bart Simpson: 59 这样一来，我们从外部看Student类，就只需要知道，创建实例需要给出name和score，而如何打印，都是在Student类的内部定义的，这些数据和逻辑被“封装”起来了，调用很容易，但却不用知道内部实现的细节。 封装的另一个好处是可以给Student类增加新的方法，比如get_grade：12345678910class Student(object): ... def get_grade(self): if self.score &gt;= 90: return 'A' elif self.score &gt;= 60: return 'B' else: return 'C' 同样的，get_grade方法可以直接在实例变量上调用，不需要知道内部实现细节：12&gt;&gt;&gt; bart.get_grade()'C' ###小结类是创建实例的模板，而实例则是一个一个具体的对象，各个实例拥有的数据都互相独立，互不影响； 方法就是与实例绑定的函数，和普通函数不同，方法可以直接访问实例的数据； 通过在实例上调用方法，我们就直接操作了对象内部的数据，但无需知道方法内部的实现细节。 和静态语言不同，Python允许对实例变量绑定任何数据，也就是说，对于两个实例变量，虽然它们都是同一个类的不同实例，但拥有的变量名称都可能不同： 123456789&gt;&gt;&gt; bart = Student('Bart Simpson', 59)&gt;&gt;&gt; lisa = Student('Lisa Simpson', 87)&gt;&gt;&gt; bart.age = 8&gt;&gt;&gt; bart.age8&gt;&gt;&gt; lisa.ageTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute 'age' ##访问限制在Class内部，可以有属性和方法，而外部代码可以通过直接调用实例变量的方法来操作数据，这样，就隐藏了内部的复杂逻辑。 但是，从前面Student类的定义来看，外部代码还是可以自由地修改一个实例的name、score属性：123456&gt;&gt;&gt; bart = Student('Bart Simpson', 98)&gt;&gt;&gt; bart.score98&gt;&gt;&gt; bart.score = 59&gt;&gt;&gt; bart.score59 如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线__，在Python中，实例的变量名如果以__开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把Student类改一改：12345678class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print('%s: %s' % (self.__name, self.__score)) 改完后，对于外部代码来说，没什么变动，但是已经无法从外部访问实例变量.__name和实例变量.__score了：12345&gt;&gt;&gt; bart = Student('Bart Simpson', 98)&gt;&gt;&gt; bart.__nameTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute '__name' 这样就确保了外部代码不能随意修改对象内部的状态，这样通过访问限制的保护，代码更加健壮。 但是如果外部代码要获取name和score怎么办？可以给Student类增加get_name和get_score这样的方法：12345678class Student(object): ... def get_name(self): return self.__name def get_score(self): return self.__score 如果又要允许外部代码修改score怎么办？可以再给Student类增加set_score方法：12345class Student(object): ... def set_score(self, score): self.__score = score 你也许会问，原先那种直接通过bart.score = 59也可以修改啊，为什么要定义一个方法大费周折？因为在方法中，可以对参数做检查，避免传入无效的参数：12345678class Student(object): ... def set_score(self, score): if 0 &lt;= score &lt;= 100: self.__score = score else: raise ValueError('bad score') 需要注意的是，在Python中，变量名类似__xxx__的，也就是以双下划线开头，并且以双下划线结尾的，是特殊变量，特殊变量是可以直接访问的，不是private变量，所以，不能用__name__、__score__这样的变量名。 有些时候，你会看到以一个下划线开头的实例变量名，比如_name，这样的实例变量外部是可以访问的，但是，按照约定俗成的规定，当你看到这样的变量时，意思就是，“虽然我可以被访问，但是，请把我视为私有变量，不要随意访问”。 双下划线开头的实例变量是不是一定不能从外部访问呢？其实也不是。不能直接访问__name是因为Python解释器对外把__name变量改成了_Student__name，所以，仍然可以通过_Student__name来访问__name变量：12&gt;&gt;&gt; bart._Student__name'Bart Simpson' 但是强烈建议你不要这么干，因为不同版本的Python解释器可能会把__name改成不同的变量名。 总的来说就是，Python本身没有任何机制阻止你干坏事，一切全靠自觉。 ##继承和多态在OOP程序设计中，当我们定义一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类、父类或超类（Base class、Super class）。 比如，我们已经编写了一个名为Animal的class，有一个run()方法可以直接打印：123class Animal(object): def run(self): print('Animal is running...') 当我们需要编写Dog和Cat类时，就可以直接从Animal类继承：12345class Dog(Animal): passclass Cat(Animal): pass 对于Dog来说，Animal就是它的父类，对于Animal来说，Dog就是它的子类。Cat和Dog类似。继承有什么好处？最大的好处是子类获得了父类的全部功能。由于Animial实现了run()方法，因此，Dog和Cat作为它的子类，什么事也没干，就自动拥有了run()方法：12345dog = Dog()dog.run()cat = Cat()cat.run() 运行结果如下：12Animal is running...Animal is running... 当然，也可以对子类增加一些方法，比如Dog类：1234567class Dog(Animal): def run(self): print('Dog is running...') def eat(self): print('Eating meat...') 继承的第二个好处需要我们对代码做一点改进。你看到了，无论是Dog还是Cat，它们run()的时候，显示的都是Animal is running...，符合逻辑的做法是分别显示Dog is running...和Cat is running...，因此，对Dog和Cat类改进如下：123456789class Dog(Animal): def run(self): print('Dog is running...')class Cat(Animal): def run(self): print('Cat is running...') 再次运行，结果如下：12Dog is running...Cat is running... 当子类和父类都存在相同的run()方法时，我们说，子类的run()覆盖了父类的run()，在代码运行的时候，总是会调用子类的run()。这样，我们就获得了继承的另一个好处：多态。 要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个class的时候，我们实际上就定义了一种数据类型。我们定义的数据类型和Python自带的数据类型，比如str、list、dict没什么两样：123a = list() # a是list类型b = Animal() # b是Animal类型c = Dog() # c是Dog类型 判断一个变量是否是某个类型可以用isinstance()判断：123456&gt;&gt;&gt; isinstance(a, list)True&gt;&gt;&gt; isinstance(b, Animal)True&gt;&gt;&gt; isinstance(c, Dog)True 看来a、b、c确实对应着list、Animal、Dog这3种类型。 但是等等，试试：12&gt;&gt;&gt; isinstance(c, Animal)True 看来c不仅仅是Dog，c还是Animal！不过仔细想想，这是有道理的，因为Dog是从Animal继承下来的，当我们创建了一个Dog的实例c时，我们认为c的数据类型是Dog没错，但c同时也是Animal也没错，Dog本来就是Animal的一种！ 所以，在继承关系中，如果一个实例的数据类型是某个子类，那它的数据类型也可以被看做是父类。但是，反过来就不行：123&gt;&gt;&gt; b = Animal()&gt;&gt;&gt; isinstance(b, Dog)False Dog可以看成Animal，但Animal不可以看成Dog。 要理解多态的好处，我们还需要再编写一个函数，这个函数接受一个Animal类型的变量：123def run_twice(animal): animal.run() animal.run() 当我们传入Animal的实例时，run_twice()就打印出：123&gt;&gt;&gt; run_twice(Animal())Animal is running...Animal is running... 当我们传入Dog的实例时，run_twice()就打印出：123&gt;&gt;&gt; run_twice(Dog())Dog is running...Dog is running... 当我们传入Cat的实例时，run_twice()就打印出：123&gt;&gt;&gt; run_twice(Cat())Cat is running...Cat is running... 看上去没啥意思，但是仔细想想，现在，如果我们再定义一个Tortoise类型，也从Animal派生：123class Tortoise(Animal): def run(self): print('Tortoise is running slowly...') 当我们调用run_twice()时，传入Tortoise的实例：123&gt;&gt;&gt; run_twice(Tortoise())Tortoise is running slowly...Tortoise is running slowly... 你会发现，新增一个Animal的子类，不必对run_twice()做任何修改，实际上，任何依赖Animal作为参数的函数或者方法都可以不加修改地正常运行，原因就在于多态。 多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子类，就会自动调用实际类型的run()方法，这就是多态的意思： 对于一个变量，我们只需要知道它是Animal类型，无需确切地知道它的子类型，就可以放心地调用run()方法，而具体调用的run()方法是作用在Animal、Dog、Cat还是Tortoise对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种Animal的子类时，只要确保run()方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则： 对扩展开放：允许新增Animal子类； 对修改封闭：不需要修改依赖Animal类型的run_twice()等函数。 继承还可以一级一级地继承下来，就好比从爷爷到爸爸、再到儿子这样的关系。而任何类，最终都可以追溯到根类object，这些继承关系看上去就像一颗倒着的树。比如如下的继承树： ###静态语言 vs 动态语言对于静态语言（例如Java）来说，如果需要传入Animal类型，则传入的对象必须是Animal类型或者它的子类，否则，将无法调用run()方法。 对于Python这样的动态语言来说，则不一定需要传入Animal类型。我们只需要保证传入的对象有一个run()方法就可以了：123class Timer(object): def run(self): print('Start...') 这就是动态语言的“鸭子类型”，它并不要求严格的继承体系，一个对象只要“看起来像鸭子，走起路来像鸭子”，那它就可以被看做是鸭子。 Python的“file-like object“就是一种鸭子类型。对真正的文件对象，它有一个read()方法，返回其内容。但是，许多对象，只要有read()方法，都被视为“file-like object“。许多函数接收的参数就是“file-like object“，你不一定要传入真正的文件对象，完全可以传入任何实现了read()方法的对象。 ###小结继承可以把父类的所有功能都直接拿过来，这样就不必重零做起，子类只需要新增自己特有的方法，也可以把父类不适合的方法覆盖重写。 动态语言的鸭子类型特点决定了继承不像静态语言那样是必须的。 ##获取对象信息当我们拿到一个对象的引用时，如何知道这个对象是什么类型、有哪些方法呢？ ###使用type() 首先，我们来判断对象类型，使用type()函数： 基本类型都可以用type()判断：123456&gt;&gt;&gt; type(123)&lt;class 'int'&gt;&gt;&gt;&gt; type('str')&lt;class 'str'&gt;&gt;&gt;&gt; type(None)&lt;type(None) 'NoneType'&gt; 如果一个变量指向函数或者类，也可以用type()判断：1234&gt;&gt;&gt; type(abs)&lt;class 'builtin_function_or_method'&gt;&gt;&gt;&gt; type(a)&lt;class '__main__.Animal'&gt; 但是type()函数返回的是什么类型呢？它返回对应的Class类型。如果我们要在if语句中判断，就需要比较两个变量的type类型是否相同：12345678910&gt;&gt;&gt; type(123)==type(456)True&gt;&gt;&gt; type(123)==intTrue&gt;&gt;&gt; type('abc')==type('123')True&gt;&gt;&gt; type('abc')==strTrue&gt;&gt;&gt; type('abc')==type(123)False 判断基本数据类型可以直接写int，str等，但如果要判断一个对象是否是函数怎么办？可以使用types模块中定义的常量：123456789101112&gt;&gt;&gt; import types&gt;&gt;&gt; def fn():... pass...&gt;&gt;&gt; type(fn)==types.FunctionTypeTrue&gt;&gt;&gt; type(abs)==types.BuiltinFunctionTypeTrue&gt;&gt;&gt; type(lambda x: x)==types.LambdaTypeTrue&gt;&gt;&gt; type((x for x in range(10)))==types.GeneratorTypeTrue ###使用isinstance()对于class的继承关系来说，使用type()就很不方便。我们要判断class的类型，可以使用isinstance()函数。 我们回顾上次的例子，如果继承关系是：1object -&gt; Animal -&gt; Dog -&gt; Husky 那么，isinstance()就可以告诉我们，一个对象是否是某种类型。先创建3种类型的对象：123&gt;&gt;&gt; a = Animal()&gt;&gt;&gt; d = Dog()&gt;&gt;&gt; h = Husky() 然后，判断：12&gt;&gt;&gt; isinstance(h, Husky)True 没有问题，因为h变量指向的就是Husky对象。 再判断：12&gt;&gt;&gt; isinstance(h, Dog)True h虽然自身是Husky类型，但由于Husky是从Dog继承下来的，所以，h也还是Dog类型。换句话说，isinstance()判断的是一个对象是否是该类型本身，或者位于该类型的父继承链上。 因此，我们可以确信，h还是Animal类型：12&gt;&gt;&gt; isinstance(h, Animal)True 同理，实际类型是Dog的d也是Animal类型：12&gt;&gt;&gt; isinstance(d, Dog) and isinstance(d, Animal)True 但是，d不是Husky类型：12&gt;&gt;&gt; isinstance(d, Husky)False 能用type()判断的基本类型也可以用isinstance()判断：123456&gt;&gt;&gt; isinstance('a', str)True&gt;&gt;&gt; isinstance(123, int)True&gt;&gt;&gt; isinstance(b'a', bytes)True 并且还可以判断一个变量是否是某些类型中的一种，比如下面的代码就可以判断是否是list或者tuple：1234&gt;&gt;&gt; isinstance([1, 2, 3], (list, tuple))True&gt;&gt;&gt; isinstance((1, 2, 3), (list, tuple))True ###使用dir() 如果要获得一个对象的所有属性和方法，可以使用dir()函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法：12&gt;&gt;&gt; dir('ABC')['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill'] 类似__xxx__的属性和方法在Python中都是有特殊用途的，比如__len__方法返回长度。在Python中，如果你调用len()函数试图获取一个对象的长度，实际上，在len()函数内部，它自动去调用该对象的__len__()方法，所以，下面的代码是等价的：1234&gt;&gt;&gt; len('ABC')3&gt;&gt;&gt; 'ABC'.__len__()3 我们自己写的类，如果也想用len(myObj)的话，就自己写一个len()方法：1234567&gt;&gt;&gt; class MyDog(object):... def __len__(self):... return 100...&gt;&gt;&gt; dog = MyDog()&gt;&gt;&gt; len(dog)100 剩下的都是普通属性或方法，比如lower()返回小写的字符串：12&gt;&gt;&gt; 'ABC'.lower()'abc' 仅仅把属性和方法列出来是不够的，配合getattr()、setattr()以及hasattr()，我们可以直接操作一个对象的状态：1234567&gt;&gt;&gt; class MyObject(object):... def __init__(self):... self.x = 9... def power(self):... return self.x * self.x...&gt;&gt;&gt; obj = MyObject() 紧接着，可以测试该对象的属性：12345678910111213&gt;&gt;&gt; hasattr(obj, 'x') # 有属性'x'吗？True&gt;&gt;&gt; obj.x9&gt;&gt;&gt; hasattr(obj, 'y') # 有属性'y'吗？False&gt;&gt;&gt; setattr(obj, 'y', 19) # 设置一个属性'y'&gt;&gt;&gt; hasattr(obj, 'y') # 有属性'y'吗？True&gt;&gt;&gt; getattr(obj, 'y') # 获取属性'y'19&gt;&gt;&gt; obj.y # 获取属性'y'19 如果试图获取不存在的属性，会抛出AttributeError的错误：1234&gt;&gt;&gt; getattr(obj, 'z') # 获取属性'z'Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'MyObject' object has no attribute 'z' 可以传入一个default参数，如果属性不存在，就返回默认值：12&gt;&gt;&gt; getattr(obj, 'z', 404) # 获取属性'z'，如果不存在，返回默认值404404 也可以获得对象的方法：123456789&gt;&gt;&gt; hasattr(obj, 'power') # 有属性'power'吗？True&gt;&gt;&gt; getattr(obj, 'power') # 获取属性'power'&lt;bound method MyObject.power of &lt;__main__.MyObject object at 0x10077a6a0&gt;&gt;&gt;&gt;&gt; fn = getattr(obj, 'power') # 获取属性'power'并赋值到变量fn&gt;&gt;&gt; fn # fn指向obj.power&lt;bound method MyObject.power of &lt;__main__.MyObject object at 0x10077a6a0&gt;&gt;&gt;&gt;&gt; fn() # 调用fn()与调用obj.power()是一样的81 ###小结通过内置的一系列函数，我们可以对任意一个Python对象进行剖析，拿到其内部的数据。要注意的是，只有在不知道对象信息的时候，我们才会去获取对象信息。如果可以直接写：1sum = obj.x + obj.y 就不要写：1sum = getattr(obj, 'x') + getattr(obj, 'y') 一个正确的用法的例子如下：1234def readImage(fp): if hasattr(fp, 'read'): return readData(fp) return None 假设我们希望从文件流fp中读取图像，我们首先要判断该fp对象是否存在read方法，如果存在，则该对象是一个流，如果不存在，则无法读取。hasattr()就派上了用场。 请注意，在Python这类动态语言中，根据鸭子类型，有read()方法，不代表该fp对象就是一个文件流，它也可能是网络流，也可能是内存中的一个字节流，但只要read()方法返回的是有效的图像数据，就不影响读取图像的功能。 ##实例属性和类属性由于Python是动态语言，根据类创建的实例可以任意绑定属性。 给实例绑定属性的方法是通过实例变量，或者通过self变量：123456class Student(object): def __init__(self, name): self.name = names = Student('Bob')s.score = 90 但是，如果Student类本身需要绑定一个属性呢？可以直接在class中定义属性，这种属性是类属性，归Student类所有：12class Student(object): name = 'Student' 12345678910111213141516&gt;&gt;&gt; class Student(object):... name = 'Student'...&gt;&gt;&gt; s = Student() # 创建实例s&gt;&gt;&gt; print(s.name) # 打印name属性，因为实例并没有name属性，所以会继续查找class的name属性Student&gt;&gt;&gt; print(Student.name) # 打印类的name属性Student&gt;&gt;&gt; s.name = 'Michael' # 给实例绑定name属性&gt;&gt;&gt; print(s.name) # 由于实例属性优先级比类属性高，因此，它会屏蔽掉类的name属性Michael&gt;&gt;&gt; print(Student.name) # 但是类属性并未消失，用Student.name仍然可以访问Student&gt;&gt;&gt; del s.name # 如果删除实例的name属性&gt;&gt;&gt; print(s.name) # 再次调用s.name，由于实例的name属性没有找到，类的name属性就显示出来了Student 从上面的例子可以看出，在编写程序的时候，千万不要把实例属性和类属性使用相同的名字，因为相同名称的实例属性将屏蔽掉类属性，但是当你删除实例属性后，再使用相同的名称，访问到的将是类属性。","categories":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://frankchen.xyz/tags/python/"}]},{"title":"使用文本处理命令获取链接批量下载","slug":"2016-05-24-di-pian-bo-ke","date":"2016-05-24T11:48:48.000Z","updated":"2017-01-13T09:49:27.000Z","comments":true,"path":"2016/05/24/2016-05-24-di-pian-bo-ke/","link":"","permalink":"http://frankchen.xyz/2016/05/24/2016-05-24-di-pian-bo-ke/","excerpt":"前几天看到一个不错的方法，现在分享给大家，希望有帮助 比如我看到Nmap的资源很想把他全部下载到本地怎么办呐？右键一个个点？用工具镜像整个站点？ 以前我用的方法是左边打开浏览器，右边打开Notepad++ 一个个链接拖到Notepad++里，最后就有了一个完整的下载列表 现在有更好的方法，利用Linux的文本处理工具提取完整的下载链接，文本处理工具我很早就学过了，但是平常不用，学了就忘","text":"前几天看到一个不错的方法，现在分享给大家，希望有帮助 比如我看到Nmap的资源很想把他全部下载到本地怎么办呐？右键一个个点？用工具镜像整个站点？ 以前我用的方法是左边打开浏览器，右边打开Notepad++ 一个个链接拖到Notepad++里，最后就有了一个完整的下载列表 现在有更好的方法，利用Linux的文本处理工具提取完整的下载链接，文本处理工具我很早就学过了，但是平常不用，学了就忘 打开你要处理网站的页面https://nmap.org/dist/ 右键保存网页 用编辑器打开删除HTML文件顶部的代码和底部代码留下链接部分 使用文本处理命令剔除多余文本，留下完整链接 1awk &apos;&#123;print $7&#125;&apos; index-of.html | cut -d &apos;&quot;&apos; -f2 &gt; output.txt 解释如下： 1234awk &apos;&#123;print $7&#125;&apos; // 打印出第7列文本，按空格或者制表符(Tab) index-of.html // 要处理的文件 | cut -d &apos;&quot;&apos; -f2 // 通过管道传递给 cut -d 指定分隔符为&quot; -f2 指定输出地2列文本 &gt; output.txt // 重定向标准输出到output.txt 然后就可以使用获取到的链接列表自动批量下载了 1wget -i output.txt 原链接地址：使用文本处理命令获取链接批量下载","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/categories/tutorial/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"http://frankchen.xyz/tags/tutorial/"}]}]}